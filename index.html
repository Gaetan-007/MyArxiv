<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-24T00:00:00Z">2024-10-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGim: An Efficient Graph Neural Network Library for Real
  Processing-In-Memory Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML library that accelerates GNNs on
real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively. We
extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using
emerging GNN models, and demonstrate that it outperforms its state-of-the-art
CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource
utilization than CPU and GPU systems. Our work provides useful recommendations
for software, system and hardware designers. PyGim is publicly available at
https://github.com/CMU-SAFARI/PyGim.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Scheduling of Vehicular Tasks on Edge Systems with Green
  Energy and Battery Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvarthi Sarkar, Abinash Kumar Ray, Aryabartta Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autonomous vehicle industry is rapidly expanding, requiring significant
computational resources for tasks like perception and decision-making.
Vehicular edge computing has emerged to meet this need, utilizing roadside
computational units (roadside edge servers) to support autonomous vehicles.
Aligning with the trend of green cloud computing, these roadside edge servers
often get energy from solar power. Additionally, each roadside computational
unit is equipped with a battery for storing solar power, ensuring continuous
computational operation during periods of low solar energy availability.
  In our research, we address the scheduling of computational tasks generated
by autonomous vehicles to roadside units with power consumption proportional to
the cube of the computational load of the server. Each computational task is
associated with a revenue, dependent on its computational needs and deadline.
Our objective is to maximize the total revenue of the system of roadside
computational units.
  We propose an offline heuristics approach based on predicted solar energy and
incoming task patterns for different time slots. Additionally, we present
heuristics for real-time adaptation to varying solar energy and task patterns
from predicted values for different time slots. Our comparative analysis shows
that our methods outperform state-of-the-art approaches upto 40\% for real-life
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGim: An Efficient Graph Neural Network Library for Real
  Processing-In-Memory Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML library that accelerates GNNs on
real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively. We
extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using
emerging GNN models, and demonstrate that it outperforms its state-of-the-art
CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource
utilization than CPU and GPU systems. Our work provides useful recommendations
for software, system and hardware designers. PyGim is publicly available at
https://github.com/CMU-SAFARI/PyGim.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance Profiling <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGim: An Efficient Graph Neural Network Library for Real
  Processing-In-Memory Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML library that accelerates GNNs on
real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively. We
extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using
emerging GNN models, and demonstrate that it outperforms its state-of-the-art
CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource
utilization than CPU and GPU systems. Our work provides useful recommendations
for software, system and hardware designers. PyGim is publicly available at
https://github.com/CMU-SAFARI/PyGim.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-23T00:00:00Z">2024-10-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster <span class="highlight-title">LLM</span>
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya K Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, Ashish Panwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Each request in LLM inference goes through two phases: compute-bound prefill
and memory-bandwidth-bound decode. To improve GPU utilization, recent systems
use hybrid batching that combines the prefill and decode phases of different
requests into the same batch. Hybrid batching works well for linear operations
as it amortizes the cost of loading model weights from HBM. However, attention
computation in hybrid batches remains inefficient because existing attention
kernels are optimized for either prefill or decode.
  In this paper, we present POD-Attention -- the first GPU kernel that
efficiently computes attention for hybrid batches. POD-Attention aims to
maximize the utilization of both compute and memory bandwidth by carefully
allocating the GPU's resources such that prefill and decode operations happen
concurrently on the same multiprocessor. We integrate POD-Attention in a
state-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up
attention computation by up to 75% (mean 28%) and increases LLM serving
throughput by up to 22% in offline inference. In online inference,
POD-Attention enables lower time-to-first-token (TTFT), time-between-tokens
(TBT), and request execution latency versus Sarathi-Serve.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Fault-Tolerant Dispersion on Oriented Grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rik Banerjee, Manish Kumar, Anisur Rahaman Molla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dispersion of mobile robots over the nodes of an anonymous graph is an
important problem and turns out to be a crucial subroutine for designing
efficient algorithms for many fundamental graph problems via mobile robots. In
this problem, starting from an arbitrary initial distribution of $n$ robots
across the $n$ nodes, the goal is to achieve a final configuration where each
node holds at most one robot. This paper investigates the dispersion problem on
an oriented grid, considering the possibility of robot failures (crashes) at
any time during the algorithm's execution. We present a crash-tolerant
dispersion algorithm that solves the dispersion problem on an anonymous
oriented grid in $O(\sqrt{n})$ time and using $O(\log n)$ bits of memory per
robot. The algorithm is optimal in terms of both time and memory per robot. We
further extend this algorithm to deal with weak Byzantine robots. The weak
Byzantine fault dispersion algorithm takes optimal $O(\sqrt{n})$ rounds but
requires $O(n\log n)$ bits of memory per robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2405.02002</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in
  Large-scale Cloud Infrastructure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyun Zhang, Randolph Yao, Si Qin, Ze Li, Shekhar Agrawal, Binit R. Mishra, Tri Tran, Minghua Ma, Qingwei Lin, Murali Chintalapati, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of unhealthy nodes in cloud infrastructure signals the potential
failure of machines, which can significantly impact the availability and
reliability of cloud services, resulting in negative customer experiences.
Effectively addressing unhealthy node mitigation is therefore vital for
sustaining cloud system performance. This paper introduces Deoxys, a causal
inference engine tailored to recommending mitigation actions for unhealthy node
in cloud systems to minimize virtual machine downtime and interruptions during
unhealthy events. It employs double machine learning combined with causal
forest to produce precise and reliable mitigation recommendations based solely
on limited observational data collected from the historical unhealthy events.
To enhance the causal inference model, Deoxys further incorporates a policy
fallback mechanism based on model uncertainty and action overriding mechanisms
to (i) improve the reliability of the system, and (ii) strike a good tradeoff
between downtime reduction and resource utilization, thereby enhancing the
overall system performance.
  After deploying Deoxys in a large-scale cloud infrastructure at Microsoft,
our observations demonstrate that Deoxys significantly reduces average VM
downtime by 53% compared to a legacy policy, while leading to 49.5% lower VM
interruption rate. This substantial improvement enhances the reliability and
stability of cloud platforms, resulting in a seamless customer experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Signature-based IaaS Performance Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheik Mohammad Mostakim Fattah, Athman Bouguettaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel change detection framework to identify changes in the
long-term performance behavior of an IaaS service. An IaaS service's long-term
performance behavior is represented by an IaaS performance signature. The
proposed framework leverages time series similarity measures and a sliding
window technique to detect changes in IaaS performance signatures. We introduce
a new IaaS performance noise model that enables the proposed framework to
distinguish between performance noise and actual changes in performance. The
proposed framework utilizes a novel Signal-to-Noise Ratio (SNR) based approach
to detect changes when prior knowledge about performance noise is available. A
set of experiments is conducted using real-world datasets to demonstrate the
effectiveness of the proposed change detection framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in ACM transaction on Internet Technology in October 2024.
  arXiv admin note: text overlap with arXiv:2007.11705</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficiently Scheduling Parallel DAG Tasks on Identical Multiprocessors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shardul Lendve, Konstantinos Bletsas, Pedro F. Souto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel real-time embedded applications can be modelled as directed acyclic
graphs (DAGs) whose nodes model subtasks and whose edges model precedence
constraints among subtasks. Efficiently scheduling such parallel tasks can be
challenging in itself, particularly in hard real-time systems where it must be
ensured offline that the deadlines of the parallel applications will be met at
run time. In this paper, we tackle the problem of scheduling DAG tasks on
identical multiprocessor systems efficiently, in terms of processor
utilisation. We propose a new algorithm that attempts to use dedicated
processor clusters for high-utilisation tasks, as in federated scheduling, but
is also capable of reclaiming the processing capacity lost to fragmentation, by
splitting the execution of parallel tasks over different existing clusters, in
a manner inspired by semi-partitioned C=D scheduling (originally devised for
non-parallel tasks). In the experiments with synthetic DAG task sets, our
Segmented-Flattened-and-Split scheduling approach achieves a significantly
higher scheduling success ratio than federated scheduling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version submitted to RTNS 2024, on 16/08/2024 (with some typos fixed)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TiMePReSt: Time and Memory Efficient Pipeline Parallel DNN Training with
  Removed Staleness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankita Dutta, Nabendu Chaki, Rajat K. De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNN training is time-consuming and requires efficient multi-accelerator
parallelization, where a single training iteration is split over available
accelerators. Current approaches often parallelize training using intra-batch
parallelization. Combining inter-batch and intra-batch pipeline parallelism is
common to further improve training throughput. In this article, we develop a
system, called TiMePReSt, that combines them in a novel way which helps to
better overlap computation and communication, and limits the amount of
communication. The traditional pipeline-parallel training of DNNs maintains
similar working principle as sequential or conventional training of DNNs by
maintaining consistent weight versions in forward and backward passes of a
mini-batch. Thus, it suffers from high GPU memory footprint during training. In
this paper, experimental study demonstrates that compromising weight
consistency doesn't decrease prediction capability of a parallelly trained DNN.
Moreover, TiMePReSt overcomes GPU memory overhead and achieves zero weight
staleness. State-of-the-art techniques often become costly in terms of training
time. In order to address this issue, TiMePReSt introduces a variant of
intra-batch parallelism that parallelizes the forward pass of each mini-batch
by decomposing it into smaller micro-batches. A novel synchronization method
between forward and backward passes reduces training time in TiMePReSt. The
occurrence of multiple sequence problem and its relation with version
difference have been observed in TiMePReSt. This paper presents a mathematical
relationship between the number of micro-batches and worker machines,
highlighting the variation in version difference. A mathematical expression has
been developed to calculate version differences for various combinations of
these two without creating diagrams for all combinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Your DRAM and SSD for Sustainable and Accessible <span class="highlight-title">LLM</span>
  Inference with Mixed-Precision and Multi-level Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Peng, Zhang Cao, Huaizhi Qu, Zhengyu Zhang, Chang Guo, Yanyong Zhang, Zhichao Cao, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated remarkable
capabilities, their massive parameter counts and associated extensive computing
make LLMs' deployment the main part of carbon emission from nowadays AI
applications. Compared to modern GPUs like H$100$, it would be significantly
carbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as
shown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for
LLM servings. However, the limited High Bandwidth Memory (HBM) available on
such GPU often cannot support the loading of LLMs due to the gigantic model
size and intermediate activation data, making their serving challenging. For
instance, a LLaMA2 model with $70$B parameters typically requires $128$GB for
inference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains
infeasible even considering the additional $64$GB DRAM. To address this
challenge, this paper proposes a mixed-precision with a model modularization
algorithm to enable LLM inference on outdated hardware with resource
constraints. (The precision denotes the numerical precision like FP16, INT8,
INT4) and multi-level caching (M2Cache).)
  Specifically, our M2Cache first modulizes neurons in LLM and creates their
importance ranking. Then, it adopts a dynamic sparse mixed-precision
quantization mechanism in weight space to reduce computational demands and
communication overhead at each decoding step. It collectively lowers the
operational carbon emissions associated with LLM inference. Moreover, M2Cache
introduces a three-level cache management system with HBM, DRAM, and SSDs that
complements the dynamic sparse mixed-precision inference. To enhance
communication efficiency, M2Cache maintains a neuron-level mixed-precision LRU
cache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Scale Multi-GPU Based Parallel Traffic Simulation for Accelerated
  Traffic Assignment and Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Jiang, Raja Sengupta, James Demmel, Samuel Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic propagation simulation is crucial for urban planning, enabling
congestion analysis, travel time estimation, and route optimization.
Traditional micro-simulation frameworks are limited to main roads due to the
complexity of urban mobility and large-scale data. We introduce the Large Scale
Multi-GPU Parallel Computing based Regional Scale Traffic Simulation Framework
(LPSim), a scalable tool that leverages GPU parallel computing to simulate
extensive traffic networks with high fidelity and reduced computation time.
LPSim performs millions of vehicle dynamics simulations simultaneously,
outperforming CPU-based methods. It can complete simulations of 2.82 million
trips in 6.28 minutes using a single GPU, and 9.01 million trips in 21.16
minutes on dual GPUs. LPSim is also tested on dual NVIDIA A100 GPUs, achieving
simulations about 113 times faster than traditional CPU methods. This
demonstrates its scalability and efficiency for large-scale applications,
making LPSim a valuable resource for researchers and planners. Code:
https://github.com/Xuan-1998/LPSim
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cold Start Latency in Serverless Computing: A Systematic <span class="highlight-title">Review</span>,
  Taxonomy, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Golec, Guneet Kaur Walia, Mohit Kumar, Felix Cuadrado, Sukhpal Singh Gill, Steve Uhlig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, academics and the corporate sector have paid attention to
serverless computing, which enables dynamic scalability and an economic model.
In serverless computing, users only pay for the time they actually use
resources, enabling zero scaling to optimise cost and resource utilisation.
However, this approach also introduces the serverless cold start problem.
Researchers have developed various solutions to address the cold start problem,
yet it remains an unresolved research area. In this article, we propose a
systematic literature review on clod start latency in serverless computing.
Furthermore, we create a detailed taxonomy of approaches to cold start latency,
which we use to investigate existing techniques for reducing the cold start
time and frequency. We have classified the current studies on cold start
latency into several categories such as caching and application-level
optimisation-based solutions, as well as Artificial Intelligence (AI)/Machine
Learning (ML)-based solutions. Moreover, we have analyzed the impact of cold
start latency on quality of service, explored current cold start latency
mitigation methods, datasets, and implementation platforms, and classified them
into categories based on their common characteristics and features. Finally, we
outline the open challenges and highlight the possible future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version Accepted for Publication in ACM Computing Survey,
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I've Got 99 Problems But FLOPS Ain't One 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandru M. Gherghescu, Vlad-Andrei Bădoiu, Alexandru Agache, Mihai-Valentin Dumitru, Iuliu Vasilescu, Radu Mantu, Costin Raiciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperscalers dominate the landscape of large network deployments, yet they
rarely share data or insights about the challenges they face. In light of this
supremacy, what problems can we find to solve in this space? We take an
unconventional approach to find relevant research directions, starting from
public plans to build a $100 billion datacenter for machine learning
applications. Leveraging the language models scaling laws, we discover what
workloads such a datacenter might carry and explore the challenges one may
encounter in doing so, with a focus on networking research. We conclude that
building the datacenter and training such models is technically possible, but
this requires novel wide-area transports for inter-DC communication, a
multipath transport and novel datacenter topologies for intra-datacenter
communication, high speed scale-up networks and transports, outlining a rich
research agenda for the networking community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts
  Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Cai, Le Qin, Jiayi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models continue to scale up, distributed training systems
have expanded beyond 10k nodes, intensifying the importance of fault tolerance.
Checkpoint has emerged as the predominant fault tolerance strategy, with
extensive studies dedicated to optimizing its efficiency. However, the advent
of the sparse Mixture-of-Experts (MoE) model presents new challenges due to the
substantial increase in model size, despite comparable computational demands to
dense models.
  In this work, we propose the Mixture-of-Checkpoint System (MoC-System) to
orchestrate the vast array of checkpoint shards produced in distributed
training systems. MoC-System features a novel Partial Experts Checkpointing
(PEC) mechanism, an algorithm-system co-design that strategically saves a
selected subset of experts, effectively reducing the MoE checkpoint size to
levels comparable with dense models. Incorporating hybrid parallel strategies,
MoC-System involves fully sharded checkpointing strategies to evenly distribute
the workload across distributed ranks. Furthermore, MoC-System introduces a
two-level checkpointing management method that asynchronously handles in-memory
snapshots and persistence processes.
  We build MoC-System upon the Megatron-DeepSpeed framework, achieving up to a
98.9% reduction in overhead for each checkpointing process compared to the
original method, during MoE model training with ZeRO-2 data parallelism and
expert parallelism. Additionally, extensive empirical analyses substantiate
that our methods enhance efficiency while maintaining comparable model
accuracy, even achieving an average accuracy increase of 1.08% on downstream
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Charon: An Analysis Framework for Rust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Son Ho, Guillaume Boisseau, Lucas Franceschino, Yoann Prak, Aymeric Fromherz, Jonathan Protzenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosion in popularity of the Rust programming language, a wealth
of tools have recently been developed to analyze, verify, and test Rust
programs. Alas, the Rust ecosystem remains relatively young, meaning that every
one of these tools has had to re-implement difficult, time-consuming machinery
to interface with the Rust compiler and its cargo build system, to hook into
the Rust compiler's internal representation, and to expose an abstract syntax
tree (AST) that is suitable for analysis rather than optimized for efficiency.
We address this missing building block of the Rust ecosystem, and propose
Charon, an analysis framework for Rust. Charon acts as a swiss-army knife for
analyzing Rust programs, and deals with all of the tedium above, providing
clients with a clean, stable AST that can serve as the foundation of many
analyses. We demonstrate the usefulness of Charon through a series of case
studies, ranging from a Rust verification framework (Aeneas), a compiler from
Rust to C (Eurydice), and a novel taint-checker for cryptographic code. To
drive the point home, we also re-implement a popular existing analysis (Rudra),
and show that it can be replicated by leveraging the Charon framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-lisp and Flexible Macro Programming with S-expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedanth Padmaraman, Sasank Chilamkurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Llama$.$lisp is a compiler framework intended to target offload processor
backends such as GPUs, using intermediate representation languages (IRs) that
are device-agnostic. The Llama$.$lisp IRs are formulated as S-expressions. This
makes them easy to generate using higher level programming languages, which is
one of the primary goals for Llama$.$lisp. The highest IR layer currently
implemented in Llama$.$lisp is C-Lisp. In this paper, we describe the macro
system developed for the Llama$.$lisp compiler framework. We show how we
implemented FFI bindings as an example of this system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Innovations in Compiler Technology, Bengaluru, 2024. See
  https://compilertech.org. Full project documentation at
  https://outline.von-neumann.ai/s/d0cd5eb9-2e15-4fa4-bd17-c3911f305008</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dependency-Aware Compilation for Surface Code Quantum Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abtin Molavi, Amanda Xu, Swamit Tannu, Aws Albarghouthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical applications of quantum computing depend on fault-tolerant devices
with error correction. Today, the most promising approach is a class of
error-correcting codes called surface codes. We study the problem of compiling
quantum circuits for quantum computers implementing surface codes. Optimal or
near-optimal compilation is critical for both efficiency and correctness. The
compilation problem requires (1) mapping circuit qubits to the device qubits
and (2) routing execution paths between interacting qubits. We solve this
problem efficiently and near-optimally with a novel algorithm that exploits the
dependency structure of circuit operations to formulate discrete optimization
problems that can be approximated via simulated annealing, a classic and simple
algorithm. Our extensive evaluation shows that our approach is powerful and
flexible for compiling realistic workloads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Version: (De/Re)-Composition of Data-Parallel Computations via
  Multi-Dimensional Homomorphisms <span class="chip">PLDI'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ari Rasch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formally introduce a systematic (de/re)-composition approach, based on the
algebraic formalism of "Multi-Dimensional Homomorphisms (MDHs)". Our approach
is designed as general enough to be applicable to a wide range of data-parallel
computations and for various kinds of target parallel architectures. To
efficiently target the deep and complex memory and core hierarchies of
contemporary architectures, we exploit our introduced (de/re)-composition
approach for a correct-by-construction, parametrized cache blocking and
parallelization strategy. We show that our approach is powerful enough to
express, in the same formalism, the (de/re)-composition strategies of different
classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and
we demonstrate that the parameters of our strategies enable systematically
generating code that can be fully automatically optimized (auto-tuned) for the
particular target architecture and characteristics of the input and output data
(e.g., their sizes and memory layouts). Particularly, our experiments confirm
that via auto-tuning, we achieve higher performance than state-of-the-art
approaches, including hand-optimized solutions provided by vendors (such as
NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a
variety of data-parallel computations, including: linear algebra routines,
stencil and quantum chemistry computations, data mining algorithms, and
computations that recently gained high attention due to their relevance for
deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A short version of this paper is published at ACM TOPLAS and
  presented at PLDI'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Effectiveness of Large Language Models in Detecting
  Security Vulnerabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, Mayur Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While automated vulnerability detection techniques have made promising
progress in detecting security vulnerabilities, their scalability and
applicability remain challenging. The remarkable performance of Large Language
Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted
recent works to explore if LLMs can be used to detect vulnerabilities. In this
paper, we perform a more comprehensive study by concurrently examining a higher
number of datasets, languages and LLMs, and qualitatively evaluating
performance across prompts and vulnerability classes while addressing the
shortcomings of existing tools. Concretely, we evaluate the effectiveness of 16
pre-trained LLMs on 5,000 code samples from five diverse security datasets.
These balanced datasets encompass both synthetic and real-world projects in
Java and C/C++ and cover 25 distinct vulnerability classes.
  Overall, LLMs across all scales and families show modest effectiveness in
detecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score
of 0.71 across datasets. They are significantly better at detecting
vulnerabilities only requiring intra-procedural analysis, such as OS Command
Injection and NULL Pointer Dereference. Moreover, they report higher accuracies
on these vulnerabilities than popular static analysis tools, such as CodeQL.
  We find that advanced prompting strategies that involve step-by-step analysis
significantly improve performance of LLMs on real-world datasets in terms of F1
score (by upto 0.18 on average). Interestingly, we observe that LLMs show
promising abilities at performing parts of the analysis correctly, such as
identifying vulnerability-related specifications and leveraging natural
language information to understand code behavior (e.g., to check if code is
sanitized). We expect our insights to guide future work on LLM-augmented
vulnerability detection systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARAS: An Adaptive Low-Cost ReRAM-Based Accelerator for DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sabri, Marc Riera, Antonio González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing Using Memory (PUM) accelerators have the potential to perform Deep
Neural Network (DNN) inference by using arrays of memory cells as computation
engines. Among various memory technologies, ReRAM crossbars show promising
performance in computing dot-product operations in the analog domain.
Nevertheless, the expensive writing procedure of ReRAM cells has led
researchers to design accelerators whose crossbars have enough capacity to
store the full DNN. Given the tremendous and continuous increase in DNN model
sizes, this approach is unfeasible for some networks, or inefficient due to the
huge hardware requirements. Those accelerators lack the flexibility to adapt to
any given DNN model, facing an challenge.
  To address this issue we introduce ARAS, a cost-effective ReRAM-based
accelerator that employs a smart scheduler to adapt different DNNs to the
resource-limited hardware. ARAS also overlaps the computation of a layer with
the weight writing of several layers to mitigate the high writing latency of
ReRAM. Furthermore, ARAS introduces three optimizations aimed at reducing the
energy overheads of writing in ReRAM. Our key optimization capitalizes on the
observation that DNN weights can be re-encoded to augment their similarity
between layers, increasing the amount of bitwise values that are equal or
similar when overwriting ReRAM cells and, hence, reducing the amount of energy
required to update the cells. Overall, ARAS greatly reduces the ReRAM writing
activity. We evaluate ARAS on a popular set of DNNs. ARAS provides up to 2.2x
speedup and 45% energy savings over a baseline PUM accelerator without any
optimization. Compared to a TPU-like accelerator, ARAS provides up to 1.5x
speedup and 61% energy savings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FirePower: Towards a Foundation with Generalizable Knowledge for
  Architecture-Level Power Modeling <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijun Zhang, Mengming Li, Yao lu, Zhiyao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power efficiency is a critical design objective in modern processor design. A
high-fidelity architecture-level power modeling method is greatly needed by CPU
architects for guiding early optimizations. However, traditional
architecture-level power models can not meet the accuracy requirement, largely
due to the discrepancy between the power model and actual design
implementation. While some machine learning (ML)-based architecture-level power
modeling methods have been proposed in recent years, the data-hungry ML model
training process requires sufficient similar known designs, which are
unrealistic in many development scenarios.
  This work proposes a new power modeling solution FirePower that targets
few-shot learning scenario for new target architectures. FirePower proposes
multiple new policies to utilize cross-architecture knowledge. First, it
develops power models at component level, and components are defined in a
power-friendly manner. Second, it supports different generalization strategies
for models of different components. Third, it formulates generalizable and
architecture-specific design knowledge into two separate models. FirePower also
supports the evaluation of the generalization quality. In our experiments,
FirePower can achieve a low error percentage of 5.8% and a high correlation R
of 0.98 on average only using two configurations of target architecture. This
is 8.8% lower in error percentage and 0.03 higher in R compared with directly
training McPAT-Calib baseline on configurations of target architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ASPDAC'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition
  Accelerator with Inter-layer and Intra-layer Optimizations <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijun Zhang, Zhiyao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud is an important data structure for a wide range of applications,
including robotics, AR/VR, and autonomous driving. To process the point cloud,
many deep-learning-based point cloud recognition algorithms have been proposed.
However, to meet the requirement of applications like autonomous driving, the
algorithm must be fast enough, rendering accelerators necessary at the
inference stage. But existing point cloud accelerators are still inefficient
due to two challenges. First, the multi-layer perceptron (MLP) during feature
computation is the performance bottleneck. Second, the feature vector fetching
operation incurs heavy DRAM access.
  In this paper, we propose Pointer, an efficient Resistive Random Access
Memory (ReRAM)-based point cloud recognition accelerator with inter- and
intra-layer optimizations. It proposes three techniques for point cloud
acceleration. First, Pointer adopts ReRAM-based architecture to significantly
accelerate the MLP in feature computation. Second, to reduce DRAM access,
Pointer proposes inter-layer coordination. It schedules the next layer to fetch
the results of the previous layer as soon as they are available, which allows
on-chip fetching thus reduces DRAM access. Third, Pointer proposes
topology-aware intra-layer reordering, which improves the execution order for
better data locality. Pointer proves to achieve 40x to 393x speedup and 22x to
163x energy efficiency over prior accelerators without any accuracy loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ASPDAC'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FuzzWiz -- Fuzzing Framework for Efficient Hardware Coverage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Narayan Gadde, Aman Kumar, Djones Lettnin, Sebastian Simon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ever-increasing design complexity of System-on-Chips (SoCs) led to
significant verification challenges. Unlike software, bugs in hardware design
are vigorous and eternal i.e., once the hardware is fabricated, it cannot be
repaired with any patch. Despite being one of the powerful techniques used in
verification, the dynamic random approach cannot give confidence to complex
Register Transfer Leve (RTL) designs during the pre-silicon design phase. In
particular, achieving coverage targets and exposing bugs is a complicated task
with random simulations. In this paper, we leverage an existing testing
solution available in the software world known as fuzzing and apply it to
hardware verification in order to achieve coverage targets in quick time. We
created an automated hardware fuzzing framework FuzzWiz using metamodeling and
Python to achieve coverage goals faster. It includes parsing the RTL design
module, converting it into C/C++ models, creating generic testbench with
assertions, fuzzer-specific compilation, linking, and fuzzing. Furthermore, it
is configurable and provides the debug flow if any crash is detected during the
fuzzing process. The proposed framework is applied on four IP blocks from
Google's OpenTitan chip with various fuzzing engines to show its scalability
and compatibility. Our benchmarking results show that we could achieve around
90% of the coverage 10 times faster than traditional simulation regression
based approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arcus: SLO Management for Accelerators in the Cloud with Traffic Shaping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiechen Zhao, Ran Shu, Katie Lim, Zewen Fan, Thomas Anderson, Mingyu Gao, Natalie Enright Jerger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud servers use accelerators for common tasks (e.g., encryption,
compression, hashing) to improve CPU/GPU efficiency and overall performance.
However, users' Service-level Objectives (SLOs) can be violated due to
accelerator-related contention. The root cause is that existing solutions for
accelerators only focus on isolation or fair allocation of compute and memory
resources; they overlook the contention for communication-related resources.
Specifically, three communication-induced challenges drive us to re-think the
problem: (1) Accelerator traffic patterns are diverse, hard to predict, and
mixed across users, (2) communication-related components lack effective
low-level isolation mechanism to configure, and (3) computational heterogeneity
of accelerators lead to unique relationships between the traffic mixture and
the corresponding accelerator performance. The focus of this work is meeting
SLOs in accelerator-rich systems. We present \design{}, treating accelerator
SLO management as traffic management with proactive traffic shaping. We develop
an SLO-aware protocol coupled with an offloaded interface on an architecture
that supports precise and scalable traffic shaping. We guarantee accelerator
SLO for various circumstances, with up to 45% tail latency reduction and less
than 1% throughput variance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EN-T: Optimizing Tensor Computing Engines Performance via Encoder-Based
  Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11887v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11887v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhe Wu, Yuchen Gui, Zhichen Zeng, Xiaotian Wang, Huawen Liang, Xi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor computations, with matrix multiplication being the primary operation,
serve as the fundamental basis for data analysis, physics, machine learning,
and deep learning. As the scale and complexity of data continue to grow
rapidly, the demand for tensor computations has also increased significantly.
To meet this demand, several research institutions have started developing
dedicated hardware for tensor computations. To further improve the
computational performance of tensor process units, we have reexamined the issue
of computation reuse that was previously overlooked in existing architectures.
As a result, we propose a novel EN-T architecture that can reduce chip area and
power consumption. Furthermore, our method is compatible with existing tensor
processing units. We evaluated our method on prevalent microarchitectures, the
results demonstrate an average improvement in area efficiency of 8.7\%, 12.2\%,
and 11.0\% for tensor computing units at computational scales of 256 GOPS, 1
TOPS, and 4 TOPS, respectively. Similarly, there were energy efficiency
enhancements of 13.0\%, 17.5\%, and 15.5\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at IEEE International
  Conference on Computer Design, 2024(ICCD 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assert<span class="highlight-title">LLM</span>: Generating and Evaluating Hardware Verification Assertions
  from Design Specifications via Multi-<span class="highlight-title">LLM</span>s <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Yan, Wenji Fang, Mengming Li, Min Li, Shang Liu, Zhiyao Xie, Hongce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assertion-based verification (ABV) is a critical method to ensure logic
designs comply with their architectural specifications. ABV requires
assertions, which are generally converted from specifications through human
interpretation by verification engineers. Existing methods for generating
assertions from specification documents are limited to sentences extracted by
engineers, discouraging their practical applications. In this work, we present
AssertLLM, an automatic assertion generation framework that processes complete
specification documents. AssertLLM can generate assertions from both natural
language and waveform diagrams in specification files. It first converts
unstructured specification sentences and waveforms into structured descriptions
using natural language templates. Then, a customized Large Language Model (LLM)
generates the final assertions based on these descriptions. Our evaluation
demonstrates that AssertLLM can generate more accurate and higher-quality
assertions compared to GPT-4o and GPT-3.5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ASPDAC'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operation Systems <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ B-Side: Binary-Level Static System Call Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaspard Thévenon, Kevin Nguetchouang, Kahina Lazri, Alain Tchana, Pierre Olivier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System call filtering is widely used to secure programs in multi-tenant
environments, and to sandbox applications in modern desktop software deployment
and package management systems. Filtering rules are hard to write and maintain
manually, hence generating them automatically is essential. To that aim,
analysis tools able to identify every system call that can legitimately be
invoked by a program are needed. Existing static analysis works lack precision
because of a high number of false positives, and/or assume the availability of
program/libraries source code -- something unrealistic in many scenarios such
as cloud production environments.
  We present B-Side, a static binary analysis tool able to identify a superset
of the system calls that an x86-64 static/dynamic executable may invoke at
runtime. B-Side assumes no access to program/libraries sources, and shows a
good degree of precision by leveraging symbolic execution, combined with a
heuristic to detect system call wrappers, which represent an important source
of precision loss in existing works. B-Side also allows to statically detect
phases of execution in a program in which different filtering rules can be
applied. We validate B-Side and demonstrate its higher precision compared to
state-of-the-art works: over a set of popular applications, B-Side's average
$F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and
dynamically-compiled binaries taken from the Debian repositories, B-Side
identifies an average of 43 system calls, vs. 271 and 95 for two state-of-the
art competitors. We further evaluate the strictness of the phase-based
filtering policies that can be obtained with B-Side.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in the 25th ACM/IFIP International Middleware
  Conference (Middleware'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SJMalloc: the security-conscious, fast, thread-safe and memory-efficient
  heap allocator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Bauroth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Heap-based exploits that leverage memory management errors continue to pose a
significant threat to application security. The root cause of these
vulnerabilities are the memory management errors within the applications,
however various hardened allocator designs have been proposed as mitigation. A
common feature of these designs is the strategic decision to store heap
metadata separately from the application data in use, thereby reducing the risk
of metadata corruption leading to security breaches. Despite their potential
benefits, hardened allocators have not been widely adopted in real-world
applications. The primary barrier to their adoption is the performance
overheads they introduce. These overheads can negatively impact the efficiency
and speed of applications, which is a critical consideration for developers and
system administrators. Having learned from previous implementations, we
developed SJMalloc, a general-purpose, high-performance allocator that
addresses these concerns. SJMalloc stores its metadata out-of-band, away from
the application's data on the heap. This design choice not only enhances
security but also improves performance. Across a variety of real-world
workloads, SJMalloc demonstrates a ~6% performance improvement compared to
GLibcs allocator, while using only ~5% more memory. Furthermore, SJMalloc
successfully passes the generic elements of the GLibc malloc testsuite and can
thus be used as a drop-in replacement for the standard allocator, offering an
easy upgrade path for enhanced security and performance without requiring
changes to existing applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arcus: SLO Management for Accelerators in the Cloud with Traffic Shaping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiechen Zhao, Ran Shu, Katie Lim, Zewen Fan, Thomas Anderson, Mingyu Gao, Natalie Enright Jerger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud servers use accelerators for common tasks (e.g., encryption,
compression, hashing) to improve CPU/GPU efficiency and overall performance.
However, users' Service-level Objectives (SLOs) can be violated due to
accelerator-related contention. The root cause is that existing solutions for
accelerators only focus on isolation or fair allocation of compute and memory
resources; they overlook the contention for communication-related resources.
Specifically, three communication-induced challenges drive us to re-think the
problem: (1) Accelerator traffic patterns are diverse, hard to predict, and
mixed across users, (2) communication-related components lack effective
low-level isolation mechanism to configure, and (3) computational heterogeneity
of accelerators lead to unique relationships between the traffic mixture and
the corresponding accelerator performance. The focus of this work is meeting
SLOs in accelerator-rich systems. We present \design{}, treating accelerator
SLO management as traffic management with proactive traffic shaping. We develop
an SLO-aware protocol coupled with an offloaded interface on an architecture
that supports precise and scalable traffic shaping. We guarantee accelerator
SLO for various circumstances, with up to 45% tail latency reduction and less
than 1% throughput variance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lower Bounds for Convexity Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Anindya De, Shivam Nadimpalli, Rocco A. Servedio, Erik Waingarten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of testing whether an unknown and arbitrary set $S
\subseteq \mathbb{R}^n$ (given as a black-box membership oracle) is convex,
versus $\varepsilon$-far from every convex set, under the standard Gaussian
distribution. The current state-of-the-art testing algorithms for this problem
make $2^{\tilde{O}(\sqrt{n})\cdot \mathrm{poly}(1/\varepsilon)}$ non-adaptive
queries, both for the standard testing problem and for tolerant testing.
  We give the first lower bounds for convexity testing in the black-box query
model:
  - We show that any one-sided tester (which may be adaptive) must use at least
$n^{\Omega(1)}$ queries in order to test to some constant accuracy
$\varepsilon>0$.
  - We show that any non-adaptive tolerant tester (which may make two-sided
errors) must use at least $2^{\Omega(n^{1/4})}$ queries to distinguish sets
that are $\varepsilon_1$-close to convex versus $\varepsilon_2$-far from
convex, for some absolute constants $0<\varepsilon_1<\varepsilon_2$.
  Finally, we also show that for any constant $c>0$, any non-adaptive tester
(which may make two-sided errors) must use at least $n^{1/4 - c}$ queries in
order to test to some constant accuracy $\varepsilon>0$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, to appear in SODA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the formalization of the notion of a concurrent algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. A. Middelburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous papers give accounts of quests for satisfactory formalizations of
the classical informal notion of an algorithm and the contemporary informal
notion of an interactive algoritm. In this paper, an attempt is made to
generalize the results of the former quest to the contemporary informal notion
of a concurrent algorithm. The notion of a concurrent proto-algorithm is
introduced. The thought is that concurrent algorithms are equivalence classes
of concurrent proto-algorithms under an appropriate equivalence relation. Three
equivalence relations are defined. Two of them are deemed to be bounds for an
appropriate equivalence relation and the third is likely an appropriate one.
The connection between concurrency and non-determinism in the presented setting
is also addressed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, in Sections 2, 3, and 4 there is some text overlap with
  arXiv:2401.08366 and substantial overlap with arXiv:2405.19037</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twisted conjugacy in dihedral Artin groups I: Torus Knot groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gemma Crowe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we provide an alternative solution to a result by Juh\'{a}sz
that the twisted conjugacy problem for odd dihedral Artin groups is solvable,
that is, groups with presentation $G(m) = \langle a,b \; | \; _{m}(a,b) =
{}_{m}(b,a) \rangle$, where $m\geq 3$ is odd, and $_{m}(a,b)$ is the word $abab
\dots$ of length $m$, is solvable. Our solution provides an implementable
linear time algorithm, by considering an alternative group presentation to that
of a torus knot group, and working with geodesic normal forms. An application
of this result is that the conjugacy problem is solvable in extensions of odd
dihedral Artin groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments welcome!</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Formal Languages and Automata Theory <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A formal characterization of discrete condensed objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dagur Asgeirsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Condensed mathematics, developed by Clausen and Scholze over the last few
years, proposes a generalization of topology with better categorical
properties. It replaces the concept of a topological space by that of a
condensed set, which can be defined as a sheaf on a certain site of compact
Hausdorff spaces. Since condensed sets are supposed to be a generalization of
topological spaces, one would like to be able to study the notion of
discreteness. There are various ways to define what it means for a condensed
set to be discrete. In this paper we describe them, and prove that they are
equivalent. The results have been fully formalized in the Lean proof assistant.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Bayesian Networks in Natural Language using Factor Arguments.
  Evaluation in the medical domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaime Sevilla, Nikolay Babakov, Ehud Reiter, Alberto Bugarin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a model for building natural language explanations
for Bayesian Network Reasoning in terms of factor arguments, which are
argumentation graphs of flowing evidence, relating the observed evidence to a
target variable we want to learn about. We introduce the notion of factor
argument independence to address the outstanding question of defining when
arguments should be presented jointly or separately and present an algorithm
that, starting from the evidence nodes and a target node, produces a list of
all independent factor arguments ordered by their strength. Finally, we
implemented a scheme to build natural language explanations of Bayesian
Reasoning using this approach. Our proposal has been validated in the medical
domain through a human-driven evaluation study where we compare the Bayesian
Network Reasoning explanations obtained using factor arguments with an
alternative explanation method. Evaluation results indicate that our proposed
explanation approach is deemed by users as significantly more useful for
understanding Bayesian Network Reasoning than another existing explanation
method it is compared to.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First Workshop on Explainable Artificial Intelligence for the medical
  domain - EXPLIMED. THE 27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A formal characterization of discrete condensed objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dagur Asgeirsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Condensed mathematics, developed by Clausen and Scholze over the last few
years, proposes a generalization of topology with better categorical
properties. It replaces the concept of a topological space by that of a
condensed set, which can be defined as a sheaf on a certain site of compact
Hausdorff spaces. Since condensed sets are supposed to be a generalization of
topological spaces, one would like to be able to study the notion of
discreteness. There are various ways to define what it means for a condensed
set to be discrete. In this paper we describe them, and prove that they are
equivalent. The results have been fully formalized in the Lean proof assistant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the formalization of the notion of a concurrent algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. A. Middelburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous papers give accounts of quests for satisfactory formalizations of
the classical informal notion of an algorithm and the contemporary informal
notion of an interactive algoritm. In this paper, an attempt is made to
generalize the results of the former quest to the contemporary informal notion
of a concurrent algorithm. The notion of a concurrent proto-algorithm is
introduced. The thought is that concurrent algorithms are equivalence classes
of concurrent proto-algorithms under an appropriate equivalence relation. Three
equivalence relations are defined. Two of them are deemed to be bounds for an
appropriate equivalence relation and the third is likely an appropriate one.
The connection between concurrency and non-determinism in the presented setting
is also addressed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, in Sections 2, 3, and 4 there is some text overlap with
  arXiv:2401.08366 and substantial overlap with arXiv:2405.19037</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Internship report: Coherent differentiation in models of Linear Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymeric Walch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coherent differentiation was introduced by Ehrhard in order to generalize
differential categories to a setting in which the sum is only partially
defined, in order to account for the deterministic nature of most models of
computation. This internship report proves that the deriving transformation of
a differential category with a categorical product always induces a coherent
differentiation. This ensures that coherent differentiation is indeed a
generalization of differential categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Higher inductive types in $(\infty,1)$-categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Uemura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a definition of higher inductive types in $(\infty,1)$-categories
with finite limits. We show that the $(\infty,1)$-category of
$(\infty,1)$-categories with higher inductive types is finitarily presentable.
In particular, the initial $(\infty,1)$-category with higher inductive types
exists. We prove a form of canonicity: the global section functor for the
initial $(\infty,1)$-category with higher inductive types preserves higher
inductive types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proof of Thought : Neurosymbolic Program Synthesis allows Robust and
  Interpretable Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debargha Ganguly, Srinivasan Iyengar, Vipin Chaudhary, Shivkumar Kalyanaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing,
yet they struggle with inconsistent reasoning, particularly in novel domains
and complex logical sequences. This research introduces Proof of Thought, a
framework that enhances the reliability and transparency of LLM outputs. Our
approach bridges LLM-generated ideas with formal logic verification, employing
a custom interpreter to convert LLM outputs into First Order Logic constructs
for theorem prover scrutiny. Central to our method is an intermediary
JSON-based Domain-Specific Language, which by design balances precise logical
structures with intuitive human concepts. This hybrid representation enables
both rigorous validation and accessible human comprehension of LLM reasoning
processes. Key contributions include a robust type system with sort management
for enhanced logical integrity, explicit representation of rules for clear
distinction between factual and inferential knowledge, and a flexible
architecture that allows for easy extension to various domain-specific
applications. We demonstrate Proof of Thought's effectiveness through
benchmarking on StrategyQA and a novel multimodal reasoning task, showing
improved performance in open-ended scenarios. By providing verifiable and
interpretable results, our technique addresses critical needs for AI system
accountability and sets a foundation for human-in-the-loop oversight in
high-stakes domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024) System 2 Reasoning At Scale Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transient Evaluation of Non-Markovian Models by Stochastic State Classes
  and Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Dengler, Laura Carnevali, Carlos E. Budde, Enrico Vicario
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-Markovian models have great expressive power, at the cost of complex
analysis of the stochastic process. The method of Stochastic State Classes
(SSCs) derives closed-form analytical expressions for the joint Probability
Density Functions (PDFs) of the active timers with marginal expolynomial PDF,
though being hindered by the number of concurrent non-exponential timers and of
discrete events between regenerations. Simulation is an alternative capable of
handling the large class of PDFs samplable via inverse transform, which however
suffers from rare events. We combine these approaches to analyze time-bounded
transient properties of non-Markovian models. We enumerate SSCs near the root
of the state-space tree and then rely on simulation to reach the target,
affording transient evaluation of models for which the method of SSCs is not
viable while reducing computational time and variance of the estimator of
transient probabilities with respect to simulation. Promising results are
observed in the estimation of rare event probabilities.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-22T00:00:00Z">2024-10-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient IMC Accelerator Design Through Joint Hardware-Workload
  Co-optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing generalized in-memory computing (IMC) hardware that efficiently
supports a variety of workloads requires extensive design space exploration,
which is infeasible to perform manually. Optimizing hardware individually for
each workload or solely for the largest workload often fails to yield the most
efficient generalized solutions. To address this, we propose a joint
hardware-workload optimization framework that identifies optimised IMC chip
architecture parameters, enabling more efficient, workload-flexible hardware.
We show that joint optimization achieves 36%, 36%, 20%, and 69% better
energy-latency-area scores for VGG16, ResNet18, AlexNet, and MobileNetV3,
respectively, compared to the separate architecture parameters search
optimizing for a single largest workload. Additionally, we quantify the
performance trade-offs and losses of the resulting generalized IMC hardware
compared to workload-specific IMC designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BETA: Automated Black-box Exploration for Timing Attacks in Processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congcong Chen, Jinhua Cui, Jiliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern processor advancements have introduced security risks, particularly in
the form of microarchitectural timing attacks. High-profile attacks such as
Meltdown and Spectre have revealed critical flaws, compromising the entire
system's security. Recent black-box automated methods have demonstrated their
advantages in identifying these vulnerabilities on various commercial
processors. However, they often focus on specific attack types or incorporate
numerous ineffective test cases, which severely limits the detection scope and
efficiency.
  In this paper, we present BETA, a novel black-box framework that harnesses
fuzzing to efficiently uncover multifaceted timing vulnerabilities in
processors. Our framework employs a two-pronged approach, enhancing both
mutation space and exploration efficiency: 1) we introduce an innovative fuzzer
that precisely constrains mutation direction for diverse instruction
combinations, including opcode, data, address, and execution level; 2) we
develop a coverage feedback mechanism based on our instruction classification
to discard potentially trivial or redundant test cases. This mechanism
significantly expands coverage across a broader spectrum of instruction types.
We evaluate the performance and effectiveness of BETA on four processors from
Intel and AMD, each featuring distinct microarchitectures. BETA has
successfully detected all x86 processor vulnerabilities previously identified
by recent black-box methods, as well as 8 previously undiscovered timing
vulnerabilities. BETA outperforms the existing state-of-the-art black-box
methods, achieving at least 3x faster detection speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript was first submitted to the ACM International
  Conference on Architectural Support for Programming Languages and Operating
  Systems on October 18, 2024 (Fall Cycle)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 10.60 $μ$W 150 GOPS Mixed-Bit-Width Sparse CNN Accelerator for
  Life-Threatening Ventricular Arrhythmia Detection <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Qin, Zhenge Jia, Zheyu Yan, Jay Mok, Manto Yung, Yu Liu, Xuejiao Liu, Wujie Wen, Luhong Liang, Kwang-Ting Tim Cheng, X. Sharon Hu, Yiyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an ultra-low power, mixed-bit-width sparse convolutional
neural network (CNN) accelerator to accelerate ventricular arrhythmia (VA)
detection. The chip achieves 50% sparsity in a quantized 1D CNN using a sparse
processing element (SPE) architecture. Measurement on the prototype chip TSMC
40nm CMOS low-power (LP) process for the VA classification task demonstrates
that it consumes 10.60 $\mu$W of power while achieving a performance of 150
GOPS and a diagnostic accuracy of 99.95%. The computation power density is only
0.57 $\mu$W/mm$^2$, which is 14.23X smaller than state-of-the-art works, making
it highly suitable for implantable and wearable medical devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, accepted to The 30th Asia and South Pacific Design
  Automation Conference (ASP-DAC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating PoT Quantization on Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rappy Saha, Jude Haris, José Cano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-uniform quantization, such as power-of-two (PoT) quantization, matches
data distributions better than uniform quantization, which reduces the
quantization error of Deep Neural Networks (DNNs). PoT quantization also allows
bit-shift operations to replace multiplications, but there are limited studies
on the efficiency of shift-based accelerators for PoT quantization.
Furthermore, existing pipelines for accelerating PoT-quantized DNNs on edge
devices are not open-source. In this paper, we first design shift-based
processing elements (shift-PE) for different PoT quantization methods and
evaluate their efficiency using synthetic benchmarks. Then we design a
shift-based accelerator using our most efficient shift-PE and propose PoTAcc,
an open-source pipeline for end-to-end acceleration of PoT-quantized DNNs on
resource-constrained edge devices. Using PoTAcc, we evaluate the performance of
our shift-based accelerator across three DNNs. On average, it achieves a 1.23x
speedup and 1.24x energy reduction compared to a multiplier-based accelerator,
and a 2.46x speedup and 1.83x energy reduction compared to CPU-only execution.
Our code is available at https://github.com/gicLAB/PoTAcc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 31st IEEE International Conference on Electronics,
  Circuits and Systems (ICECS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEDUSA: Scalable Biometric Sensing in the Wild through Distributed MIMO
  Radars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Li, Ramanujan K Sheshadri, Karthik Sundaresan, Eugene Chai, Suman Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar-based techniques for detecting vital signs have shown promise for
continuous contactless vital sign sensing and healthcare applications. However,
real-world indoor environments face significant challenges for existing vital
sign monitoring systems. These include signal blockage in non-line-of-sight
(NLOS) situations, movement of human subjects, and alterations in location and
orientation. Additionally, these existing systems failed to address the
challenge of tracking multiple targets simultaneously. To overcome these
challenges, we present MEDUSA, a novel coherent ultra-wideband (UWB) based
distributed multiple-input multiple-output (MIMO) radar system, especially it
allows users to customize and disperse the $16 \times 16$ into sub-arrays.
MEDUSA takes advantage of the diversity benefits of distributed yet wirelessly
synchronized MIMO arrays to enable robust vital sign monitoring in real-world
and daily living environments where human targets are moving and surrounded by
obstacles. We've developed a scalable, self-supervised contrastive learning
model which integrates seamlessly with our hardware platform. Each attention
weight within the model corresponds to a specific antenna pair of Tx and Rx.
The model proficiently recovers accurate vital sign waveforms by decomposing
and correlating the mixed received signals, including comprising human motion,
mobility, noise, and vital signs. Through extensive evaluations involving 21
participants and over 200 hours of collected data (3.75 TB in total, with 1.89
TB for static subjects and 1.86 TB for moving subjects), MEDUSA's performance
has been validated, showing an average gain of 20% compared to existing systems
employing COTS radar sensors. This demonstrates MEDUSA's spatial diversity gain
for real-world vital sign monitoring, encompassing target and environmental
dynamics in familiar and unfamiliar indoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language to Verilog: Design of a Recurrent Spiking Neural
  Network using Large Language Models and ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paola Vitolo, George Psaltakis, Michael Tomlinson, Gian Domenico Licciardo, Andreas G. Andreou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the use of Large Language Models (LLMs) and natural
language prompts to generate hardware description code, namely Verilog.
Building on our prior work, we employ OpenAI's ChatGPT4 and natural language
prompts to synthesize an RTL Verilog module of a programmable recurrent spiking
neural network, while also generating test benches to assess the system's
correctness. The resultant design was validated in three simple machine
learning tasks, the exclusive OR, the IRIS flower classification and the MNIST
hand-written digit classification. Furthermore, the design was validated on a
Field-Programmable Gate Array (FPGA) and subsequently synthesized in the
SkyWater 130 nm technology by using an open-source electronic design automation
flow. The design was submitted to Efabless Tiny Tapeout 6.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was presented at the IEEE/ACM International Conference on
  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Cluster-BFS and Applications to Shortest Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letong Wang, Guy Blelloch, Yan Gu, Yihan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breadth-first Search (BFS) is one of the most important graph processing
subroutines, especially to compute the unweighted distance. Many applications
may require running BFS from multiple sources. Sequentially, when running BFS
on a cluster of nearby vertices, a known optimization is to use
bit-parallelism. Given a subset of vertices with size $k$ and the distance
between any pair of them is no more than $d$, BFS can be applied to all of them
in a total work of $O(dk/w+1)$, where $w$ is the length of a word in bits. We
will refer to this approach as cluster-BFS (C-BFS). Such an approach has been
studied and shown effective both in theory and in practice in the sequential
setting. However, it remains unknown how this can be combined with thread-level
parallelism for C-BFS.
  In this paper, we focus on designing efficient parallel C-BFS based on BFS to
answer unweighted distance queries. Our solution combines the strengths of
bit-level parallelism and thread-level parallelism, and achieves significant
speedup over the plain sequential solution. We also apply our algorithm to
real-world applications. In particular, we identified another application
(landmark-labeling for the approximate distance oracle) that can take advantage
of parallel C-BFS. Under the same memory budget, our new solution improves
accuracy and/or time on all the 18 tested graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Security and RAS in the Computing Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martí Alonso, David Andreu, Ramon Canal, Stefano Di Carlo, Odysseas Chatzopoulos, Cristiano Chenet, Juanjo Costa, Andreu Girones, Dimitris Gizopoulos, George Papadimitriou, Enric Morancho, Beatriz Otero, Alessandro Savino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security and RAS are two non-functional requirements under focus for current
systems developed for the computing continuum. Due to the increased number of
interconnected computer systems across the continuum, security becomes
especially pervasive at all levels, from the smallest edge device to the
high-performance cloud at the other end. Similarly, RAS (Reliability,
Availability, and Serviceability) ensures the robustness of a system towards
hardware defects. Namely, making them reliable, with high availability and
design for easy service. In this paper and as a result of the Vitamin-V EU
project, the authors detail the comprehensive approach to malware and hardware
attack detection; as well as, the RAS features envisioned for future systems
across the computing continuum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI
  Training Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasibul Jamil, Abdul Alim, Laurent Schares, Pavlos Maniotis, Liran Schour, Ali Sydney, Abdullah Kayi, Tevfik Kosar, Bengi Karacali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of AI workloads, especially distributed Large
Language Model (LLM) training, places significant strain on the networking
infrastructure of parallel data centers and supercomputing systems. While
Equal-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,
hash collisions often lead to imbalanced network resource utilization and
performance bottlenecks. This paper presents FlowTracer, a tool designed to
analyze network path utilization and evaluate different routing strategies.
FlowTracer aids in debugging network inefficiencies by providing detailed
visibility into traffic distribution and helping to identify the root causes of
performance degradation, such as issues caused by hash collisions. By offering
flow-level insights, FlowTracer enables system operators to optimize routing,
reduce congestion, and improve the performance of distributed AI workloads. We
use a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to
demonstrate how FlowTracer can be used to compare the flow imbalances of ECMP
routing against a statically configured network. The example showcases a 30%
reduction in imbalance, as measured by a new metric we introduce.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for peer reviewing in IEEE ICC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRA-C: Parameter-Efficient Fine-Tuning of Robust CNN for IoT Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuntao Ding, Xu Cao, Jianhang Xie, Linlin Fan, Shangguang Wang, Zhichao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning of pre-trained convolutional neural network (CNN)
models using local data is essential for providing high-quality services to
users using ubiquitous and resource-limited Internet of Things (IoT) devices.
Low-Rank Adaptation (LoRA) fine-tuning has attracted widespread attention from
industry and academia because it is simple, efficient, and does not incur any
additional reasoning burden. However, most of the existing advanced methods use
LoRA to fine-tune Transformer, and there are few studies on using LoRA to
fine-tune CNN. The CNN model is widely deployed on IoT devices for application
due to its advantages in comprehensive resource occupancy and performance.
Moreover, IoT devices are widely deployed outdoors and usually process data
affected by the environment (such as fog, snow, rain, etc.). The goal of this
paper is to use LoRA technology to efficiently improve the robustness of the
CNN model. To this end, this paper first proposes a strong, robust CNN
fine-tuning method for IoT devices, LoRA-C, which performs low-rank
decomposition in convolutional layers rather than kernel units to reduce the
number of fine-tuning parameters. Then, this paper analyzes two different rank
settings in detail and observes that the best performance is usually achieved
when ${\alpha}/{r}$ is a constant in either standard data or corrupted data.
This discovery provides experience for the widespread application of LoRA-C.
Finally, this paper conducts many experiments based on pre-trained models.
Experimental results on CIFAR-10, CIFAR-100, CIFAR-10-C, and Icons50 datasets
show that the proposed LoRA-Cs outperforms standard ResNets. Specifically, on
the CIFAR-10-C dataset, the accuracy of LoRA-C-ResNet-101 achieves 83.44%
accuracy, surpassing the standard ResNet-101 result by +9.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Scheduling of Vehicular Tasks on Edge Systems with Green
  Energy and Battery Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvarthi Sarkar, binash Kumar Ray, Aryabartta Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autonomous vehicle industry is rapidly expanding, requiring significant
computational resources for tasks like perception and decision-making.
Vehicular edge computing has emerged to meet this need, utilizing roadside
computational units (roadside edge servers) to support autonomous vehicles.
Aligning with the trend of green cloud computing, these roadside edge servers
often get energy from solar power. Additionally, each roadside computational
unit is equipped with a battery for storing solar power, ensuring continuous
computational operation during periods of low solar energy availability.
  In our research, we address the scheduling of computational tasks generated
by autonomous vehicles to roadside units with power consumption proportional to
the cube of the computational load of the server. Each computational task is
associated with a revenue, dependent on its computational needs and deadline.
Our objective is to maximize the total revenue of the system of roadside
computational units.
  We propose an offline heuristics approach based on predicted solar energy and
incoming task patterns for different time slots. Additionally, we present
heuristics for real-time adaptation to varying solar energy and task patterns
from predicted values for different time slots. Our comparative analysis shows
that our methods outperform state-of-the-art approaches upto 40\% for real-life
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-focused HPC Data Centers Can Provide More Power Grid Flexibility and
  at Lower Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Zhou, Angel Paredes, Chaimaa Essayeh, Thomas Morstyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent growth of Artificial Intelligence (AI), particularly large
language models, requires energy-demanding high-performance computing (HPC)
data centers, which poses a significant burden on power system capacity.
Scheduling data center computing jobs to manage power demand can alleviate
network stress with minimal infrastructure investment and contribute to fast
time-scale power system balancing. This study, for the first time,
comprehensively analyzes the capability and cost of grid flexibility provision
by GPU-heavy AI-focused HPC data centers, along with a comparison with
CPU-heavy general-purpose HPC data centers traditionally used for scientific
computing. Using real-world data from 7 AI-focused HPC data centers, 7
general-purpose HPC data centers, and 3 cloud platforms, we find that
AI-focused HPC data centers can offer greater flexibility at 50% lower cost for
a range of power system services. By comparing the cost to flexibility market
prices, we illustrate the financial profitability of flexibility provision for
AI-focused HPC data centers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (including supplementary materials and references), under
  review for Joule</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMUSD: Asynchronous Multi-Device Speculative Decoding for <span class="highlight-title">LLM</span>
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bradley McDanel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models typically generate tokens autoregressively, using each
token as input for the next. Recent work on Speculative Decoding has sought to
accelerate this process by employing a smaller, faster draft model to more
quickly generate candidate tokens. These candidates are then verified in
parallel by the larger (original) verify model, resulting in overall speedup
compared to using the larger model by itself in an autoregressive fashion. In
this work, we introduce AMUSD (Asynchronous Multi-device Speculative Decoding),
a system that further accelerates generation by decoupling the draft and verify
phases into a continuous, asynchronous approach. Unlike conventional
speculative decoding, where only one model (draft or verify) performs token
generation at a time, AMUSD enables both models to perform predictions
independently on separate devices (e.g., GPUs). We evaluate our approach over
multiple datasets and show that AMUSD achieves an average 29% improvement over
speculative decoding and up to 1.96$\times$ speedup over conventional
autoregressive decoding, while achieving identical output quality. Our system
is open-source and available at https://github.com/BradMcDanel/AMUSD/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Framework for Clustered Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wu, Tales Imbiriba, Pau Closas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the main challenges of federated learning (FL) is handling
non-independent and identically distributed (non-IID) client data, which may
occur in practice due to unbalanced datasets and use of different data sources
across clients. Knowledge sharing and model personalization are key strategies
for addressing this issue. Clustered federated learning is a class of FL
methods that groups clients that observe similarly distributed data into
clusters, such that every client is typically associated with one data
distribution and participates in training a model for that distribution along
their cluster peers. In this paper, we present a unified Bayesian framework for
clustered FL which associates clients to clusters. Then we propose several
practical algorithms to handle the, otherwise growing, data associations in a
way that trades off performance and computational complexity. This work
provides insights on client-cluster associations and enables client knowledge
sharing in new ways. The proposed framework circumvents the need for unique
client-cluster associations, which is seen to increase the performance of the
resulting models in a variety of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fast Parallel Approach for Neighborhood-based Link Prediction by
  Disregarding Large Hubs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11415v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11415v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Link prediction can help rectify inaccuracies in various graph algorithms,
stemming from unaccounted-for or overlooked links within networks. However,
many existing works use a baseline approach, which incurs unnecessary
computational costs due to its high time complexity. Further, many studies
focus on smaller graphs, which can lead to misleading conclusions. Here, we
study the prediction of links using neighborhood-based similarity measures on
large graphs. In particular, we improve upon the baseline approach (IBase), and
propose a heuristic approach that additionally disregards large hubs (DLH),
based on the idea that high-degree nodes contribute little similarity among
their neighbors. On a server equipped with dual 16-core Intel Xeon Gold 6226R
processors, DLH is on average 1019x faster than IBase, especially on web graphs
and social networks, while maintaining similar prediction accuracy. Notably,
DLH achieves a link prediction rate of 38.1M edges/s and improves performance
by 1.6x for every doubling of threads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Transverse-Read-assisted Fast Valid-Bits Collection in Stochastic
  Computing MACs for Energy-Efficient in-RTM DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihe Wang, Zhiying Zhang, Xingwu Dong, Danghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It looks very attractive to coordinate racetrack-memory (RM) and
stochastic-computing (SC) jointly to build an ultra-low power
neuron-architecture.However, the above combination has always been questioned
in a fatal weakness that the heavy valid-bits collection of RM-MTJ, a.k.a.
accumulative parallel counters (APCs), cannot physically match the requirement
for energy-efficient in-memory DNNs.Fortunately, a recently developed
Transverse-Read (TR) provides a lightweight collection of valid-bits by
detecting domain-wall resistance between a couple of MTJs on a single
nanowire.In this work, we first propose a neuron-architecture that utilizes
parallel TRs to build an ultra-fast valid-bits collection for SC, in which, a
vector multiplication is successfully degraded as swift TRs.To solve huge
storage for full stochastic sequences caused by the limited TR banks, a hybrid
coding, pseudo-fractal compression, is designed to generate stochastic
sequences by segments.To overcome the misalignment by the parallel
early-termination, an asynchronous schedule of TR is further designed to
regularize the vectorization, in which, the valid-bits from different lanes are
merged in multiple RM-stacks for vector-level valid-bits collection.However, an
inherent defect of TR, i.e., neighbor parts cannot be accessed simultaneously,
could limit the throughput of the parallel vector multiplication, therefore, an
interleaving data placement is used for full utilization of memory bus among
different vectors.The results show that the SC-MAC assisted with TR achieves
$2.88\times-4.40\times $speedup compared to CORUSCANT, at the same time, energy
consumption is reduced by $1.26\times-1.42\times$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Portable, Massively Parallel Implementation of a Material Point Method
  for Compressible Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Joseph Baioni, Tommaso Benacchio, Luigi Capone, Carlo de Falco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent evolution of software and hardware technologies is leading to a
renewed computational interest in Particle-In-Cell (PIC) methods such as the
Material Point Method (MPM). Indeed, provided some critical aspects are
properly handled, PIC methods can be cast in formulations suitable for the
requirements of data locality and fine-grained parallelism of modern hardware
accelerators such as Graphics Processing Units (GPUs). Such a rapid and
continuous technological development increases also the importance of generic
and portable implementations. While the capabilities of MPM on a wide range
continuum mechanics problem have been already well assessed, the use of the
method in compressible fluid dynamics has received less attention. In this
paper we present a portable, highly parallel, GPU based MPM solver for
compressible gas dynamics. The implementation aims to reach a good compromise
between portability and efficiency in order to provide a first assessment of
the potential of this approach in solving strongly compressible gas flow
problems, also taking into account solid obstacles. The numerical model
considered constitutes a first step towards the development of a monolithic MPM
solver for Fluid-Structure Interaction (FSI) problems at all Mach numbers up to
the supersonic regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autobahn: Seamless high speed BFT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Giridharan, Florian Suri-Payer, Ittai Abraham, Lorenzo Alvisi, Natacha Crooks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's practical, high performance Byzantine Fault Tolerant (BFT) consensus
protocols operate in the partial synchrony model. However, existing protocols
are inefficient when deployments are indeed partially synchronous. They deliver
either low latency during fault-free, synchronous periods (good intervals) or
robust recovery from events that interrupt progress (blips). At one end,
traditional, view-based BFT protocols optimize for latency during good
intervals, but, when blips occur, can suffer from performance degradation
(hangovers) that can last beyond the return of a good interval. At the other
end, modern DAG-based BFT protocols recover more gracefully from blips, but
exhibit lackluster latency during good intervals. To close the gap, this work
presents Autobahn, a novel high-throughput BFT protocol that offers both low
latency and seamless recovery from blips. By combining a highly parallel
asynchronous data dissemination layer with a low-latency, partially synchronous
consensus mechanism, Autobahn (i) avoids the hangovers incurred by traditional
BFT protocols and (ii) matches the throughput of state of the art DAG-based BFT
protocols while cutting their latency in half, matching the latency of
traditional BFT protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrashEvent<span class="highlight-title">LLM</span>: Predicting System Crashes with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Mudgal, Bijan Arbab, Swaathi Sampath Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the dependence on computer systems expands across various domains,
focusing on personal, industrial, and large-scale applications, there arises a
compelling need to enhance their reliability to sustain business operations
seamlessly and ensure optimal user satisfaction. System logs generated by these
devices serve as valuable repositories of historical trends and past failures.
The use of machine learning techniques for failure prediction has become
commonplace, enabling the extraction of insights from past data to anticipate
future behavior patterns. Recently, large language models have demonstrated
remarkable capabilities in tasks including summarization, reasoning, and event
prediction. Therefore, in this paper, we endeavor to investigate the potential
of large language models in predicting system failures, leveraging insights
learned from past failure behavior to inform reasoning and decision-making
processes effectively. Our approach involves leveraging data from the Intel
Computing Improvement Program (ICIP) system crash logs to identify significant
events and develop CrashEventLLM. This model, built upon a large language model
framework, serves as our foundation for crash event prediction. Specifically,
our model utilizes historical data to forecast future crash events, informed by
expert annotations. Additionally, it goes beyond mere prediction, offering
insights into potential causes for each crash event. This work provides the
preliminary insights into prompt-based large language models for the log-based
event prediction task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICITCOM'24. Copyrights will be with IEEE</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Decision Problem for Regular First-Order Theories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umang Mathur, David Mestel, Mahesh Viswanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical `decision problem' asks whether a given formula of first-order
logic is satisfiable. In this work we consider an extension of this problem to
regular first-order theories, i.e. (infinite) regular sets of formulae.
Building on the beautiful classification of syntactic classes as decidable or
undecidable for the classical decision problem, we show that some classes (the
EPR and Gurevich classes) which are decidable in the classical setting are
undecidable for regular theories; on the other hand for each we show a subclass
which remains decidable in our setting, leaving a complete classification as a
challenge for future work. Finally, we observe that our problem generalises
prior work on verification of uninterpreted programs, and give a semantic class
of existential formulae for which the problem is decidable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstract Operational Methods for Call-by-Push-Value 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Goncharov, Stelios Tsampas, Henning Urbat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Levy's call-by-push-value is a comprehensive programming paradigm that
combines elements from functional and imperative programming, supports
computational effects and subsumes both call-by-value and call-by-name
evaluation strategies. In the present work, we develop modular methods to
reason about program equivalence in call-by-push-value, and in fine-grain
call-by-value, which is a popular lightweight call-by-value sublanguage of the
former. Our approach is based on the fundamental observation that presheaf
categories of sorted sets are suitable universes to model call-by-(push)-value
languages, and that natural, coalgebraic notions of program equivalence such as
applicative similarity and logical relations can be developed within. Starting
from this observation, we formalize fine-grain call-by-value and
call-by-push-value in the higher-order abstract GSOS framework, reduce their
key congruence properties to simple syntactic conditions by leveraging existing
theory and argue that introducing changes to either language incurs minimal
proof overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C-lisp and Flexible Macro Programming with S-expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedanth Padmaraman, Sasank Chilamkurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Llama.lisp is a compiler framework intended to target offload processor
backends such as GPUs, using intermediate representation languages (IRs) that
are device-agnostic. The Llama.lisp IRs are formulated as S-expressions. This
makes them easy to generate using higher level programming languages, which is
one of the primary goals for Llama.lisp. The highest IR layer currently
implemented in Llama.lisp is C-Lisp. In this paper, we describe the macro
system developed for the Llama.lisp compiler framework. We show how we
implemented FFI bindings as an example of this system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Innovations in Compiler Technology, Bengaluru, 2024. See
  https://compilertech.org. Full project documentation at
  https://outline.von-neumann.ai/s/d0cd5eb9-2e15-4fa4-bd17-c3911f305008</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Beyond the Phase Ordering Problem: Finding the Globally Optimal Code
  w.r.t. Optimization Phases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03120v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03120v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Yu Wang</span>, Hongyu Chen, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new concept called \textit{semantically
equivalence} \wrt \textit{optimization phases} \textit{(\sep)}, which defines
the set of programs a compiler considers semantically equivalent to the input
using a set of optimization phases. We show both theoretically and empirically
that solving the phase ordering problem does not necessarily result in the most
efficient code among all programs that a compiler deems semantically equivalent
to the input, hereinafter referred to as the global optimal code \wrt
optimization phases.
  To find the global optimal code \wrt optimization phases, we present a
conceptual framework, leveraging the reverse of existing optimization phases.
In theory, we prove that the framework is capable of finding the global optimal
code for any program. We realize this framework into a technique, called
\textit{iterative bi-directional optimization (\tool)}, which performs both the
normal and reverse optimizations to increase and decrease the efficiency of the
generated code, respectively.
  We evaluate \tool on C/C++ files randomly extracted from highly mature and
influential programs (\eg, Linux kernel, OpenSSL, Z3). Results show that \tool
frequently generates more efficient code -- measured by either code size or
runtime performance -- than exhaustive search, which is the solution to the
phase ordering problem. We also find by simply incorporating \tool's reverse
optimization phases, the effectiveness of the optimization of state-of-the-art
compilers (\eg, GCC/LLVM) can be significantly improved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insights from the Usage of the Ansible Lightspeed Code Completion
  Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17442v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17442v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis Mandel, Luca Buratti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of Large Language Models (LLMs) which can generate code, has
made it possible to create tools that improve developer productivity.
Integrated development environments or IDEs which developers use to write
software are often used as an interface to interact with LLMs. Although many
such tools have been released, almost all of them focus on general-purpose
programming languages. Domain-specific languages, such as those crucial for
Information Technology (IT) automation, have not received much attention.
Ansible is one such YAML-based IT automation-specific language. Ansible
Lightspeed is an LLM-based service designed explicitly to generate Ansible
YAML, given natural language prompt.
  In this paper, we present the design and implementation of the Ansible
Lightspeed service. We then evaluate its utility to developers using diverse
indicators, including extended utilization, analysis of user edited
suggestions, as well as user sentiments analysis. The evaluation is based on
data collected for 10,696 real users including 3,910 returning users. The code
for Ansible Lightspeed service and the analysis framework is made available for
others to use.
  To our knowledge, our study is the first to involve thousands of users of
code assistants for domain-specific languages. We are also the first code
completion tool to present N-Day user retention figures, which is 13.66% on Day
30. We propose an improved version of user acceptance rate, called Strong
Acceptance rate, where a suggestion is considered accepted only if less than
50% of it is edited and these edits do not change critical parts of the
suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong
acceptance rate of 49.08% for multi-line Ansible task suggestions. With our
findings we provide insights into the effectiveness of small, dedicated models
in a domain-specific context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published at the 39th IEEE/ACM International
  Conference on Automated Software Engineering (ASE 2024), Industry Showcase
  under the title "Ansible Lightspeed: A Code Generation Service for IT
  Automation"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target-Aware Implementation of Real Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brett Saiki, Jackson Brough, Jonas Regehr, Jesús Ponce, Varun Pradeep, Aditya Akhileshwaran, Zachary Tatlock, Pavel Panchekha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New low-precision accelerators, vector instruction sets, and library
functions make maximizing accuracy and performance of numerical code
increasingly challenging. Two lines of work$\unicode{x2013}$traditional
compilers and numerical compilers$\unicode{x2013}$attack this problem from
opposite directions. Traditional compiler backends optimize for specific target
environments but are limited in their ability to balance performance and
accuracy. Numerical compilers trade off accuracy and performance, or even
improve both, but ignore the target environment. We join aspects of both to
produce Chassis, a target-aware numerical compiler.
  Chassis compiles mathematical expressions to operators from a target
description, which lists the real expressions each operator approximates and
estimates its cost and accuracy. Chassis then uses an iterative improvement
loop to optimize for speed and accuracy. Specifically, a new instruction
selection modulo equivalence algorithm efficiently searches for faster
target-specific programs, while a new cost-opportunity heuristic supports
iterative improvement. We demonstrate Chassis' capabilities on 9 different
targets, including hardware ISAs, math libraries, and programming languages.
Chassis achieves significantly better accuracy and performance trade-offs than
both Clang (by 8.9x) or Herbie (by up to 3.5x) by leveraging low-precision
accelerators, accuracy-optimized numerical helper functions, and library
subcomponents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metamorphic Debugging for Accountable Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeid Tizpaz-Niari, Shiva Darian, Ashutosh Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the laws have become more complicated and enormous, the role of software
systems in navigating and understanding these intricacies has become more
critical. Given their socio-economic and legally critical implications,
ensuring software accountability -- encompassing qualities such as legal
compliance, explainability, perceptions of procedural justice, fairness of
outcomes, and confidentiality/privacy -- is of paramount social importance.
Moreover, software that accurately interprets its requirements, complies with
legal standards and upholds social fairness can serve as a surrogate for legal
and social norms, enabling policymakers to inquire about the law as seamlessly
as a software engineer conducts a test. However, ensuring software
accountability faces three key challenges: i) Translating legalese into formal
specifications, ii) Lack of a definitive 'truth' for queries (the oracle
problem), and iii) Scarcity of trustworthy datasets due to privacy and legal
concerns.
  Drawing from the experiences in debugging U.S. tax preparation software, we
propose that these challenges can be tackled by focusing on relational
specifications. While the exact output for a given input may be unknown, the
relationship between the outputs of two related inputs may be easier to
express. This observation resembles i) the legal doctrine of precedent, meaning
that similar cases must yield similar rulings; and ii) metamorphic relation
(MR) in software engineering that requires a specific relation between software
inputs and outputs. We propose metamorphic debugging as the foundation for
detecting, explaining, and repairing socio-legal software for these relations.
We showcase recent results that leverage metamorphic debugging to detect and
explain accountability bugs in tax prep and poverty management software
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the 3rd International Workshop on Programming Languages and the
  Law (ProLaLa'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modal Effect Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Tang, Leo White, Stephen Dolan, Daniel Hillerström, Sam Lindley, Anton Lorenzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effect handlers are a powerful abstraction for defining, customising, and
composing computational effects. Statically ensuring that all effect operations
are handled requires some form of effect system, but using a traditional effect
system would require adding extensive effect annotations to the millions of
lines of existing code in these languages. Recent proposals seek to address
this problem by removing the need for explicit effect polymorphism. However,
they typically rely on fragile syntactic mechanisms or on introducing a
separate notion of second-class function. We introduce a novel semantic
approach based on modal effect types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>4Decompile: Decompiling Binary Code with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05286v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05286v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decompilation aims to convert binary code to high-level source code, but
traditional tools like Ghidra often produce results that are difficult to read
and execute. Motivated by the advancements in Large Language Models (LLMs), we
propose LLM4Decompile, the first and largest open-source LLM series (1.3B to
33B) trained to decompile binary code. We optimize the LLM training process and
introduce the LLM4Decompile-End models to decompile binary directly. The
resulting models significantly outperform GPT-4o and Ghidra on the HumanEval
and ExeBench benchmarks by over 100% in terms of re-executability rate.
Additionally, we improve the standard refinement approach to fine-tune the
LLM4Decompile-Ref models, enabling them to effectively refine the decompiled
code from Ghidra and achieve a further 16.2% improvement over the
LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to
revolutionize binary code decompilation, delivering remarkable improvements in
readability and executability while complementing conventional tools for
optimal results. Our code, dataset, and models are released at
https://github.com/albertan017/LLM4Decompile
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Explanations in Probabilistic Logic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17045v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17045v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germán Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of tools based on artificial intelligence has also led to the
need of producing explanations which are understandable by a human being. In
most approaches, the system is considered a black box, making it difficult to
generate appropriate explanations. In this work, though, we consider a setting
where models are transparent: probabilistic logic programming (PLP), a paradigm
that combines logic programming for knowledge representation and probability to
model uncertainty. However, given a query, the usual notion of explanation is
associated with a set of choices, one for each random variable of the model.
Unfortunately, such a set does not explain why the query is true and, in fact,
it may contain choices that are actually irrelevant for the considered query.
To improve this situation, we present in this paper an approach to explaining
explanations which is based on defining a new query-driven inference mechanism
for PLP where proofs are labeled with "choice expressions", a compact and easy
to manipulate representation for sets of choices. The combination of proof
trees and choice expressions allows us to produce comprehensible query
justifications with a causal structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Programming Languages and Systems (Proceedings of APLAS 2024),
  Springer LNCS, 2024, and is available online at
  https://doi.org/10.1007/978-981-97-8943-6_7</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural temporal logic for mechanized program verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleftherios Ioannidis, Yannick Zakowski, Steve Zdancewic, Sebastian Angel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanized verification of liveness properties for programs with effects,
nondeterminism, and nontermination is difficult. Existing temporal reasoning
frameworks operate on the level of models (traces, automata) not executable
code, creating a verification gap and losing the benefits of modularity and
composition enjoyed by structural program logics. Reasoning about infinite
traces and automata requires complex (co-)inductive proof techniques and
familiarity with proof assistant mechanics (e.g., guardedness checker). We
propose a structural approach to the verification of temporal properties with a
new temporal logic that we call Ticl. Using Ticl, we internalize complex
(co-)inductive proof techniques to structural lemmas and reasoning about
variants and invariants. We show that it is possible to perform mechanized
proofs of general temporal properties, while working in a high-level of
abstraction. We demonstrate the benefits of ticl by giving short, structural
proofs of safety and liveness properties for programs with queues, secure
memory, and distributed consensus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Computational Indistinguishability and Logical Relations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ugo Dal Lago, Zeinab Galal, Giulia Giusti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A $\lambda$-calculus is introduced in which all programs can be evaluated in
probabilistic polynomial time and in which there is sufficient structure to
represent sequential cryptographic constructions and adversaries for them, even
when the latter are oracle-based. A notion of observational equivalence
capturing computational indistinguishability and a class of approximate logical
relations are then presented, showing that the latter represent a sound proof
technique for the former. The work concludes with the presentation of an
example of a security proof in which the encryption scheme induced by a
pseudorandom function is proven secure against active adversaries in a purely
equational style.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>APLAS 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operation Systems <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bauplan: zero-copy, scale-up FaaS for data pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacopo Tagliabue, Tyler Caraza-Harter, Ciro Greco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chaining functions for longer workloads is a key use case for FaaS platforms
in data applications. However, modern data pipelines differ significantly from
typical serverless use cases (e.g., webhooks and microservices); this makes it
difficult to retrofit existing pipeline frameworks due to structural
constraints. In this paper, we describe these limitations in detail and
introduce bauplan, a novel FaaS programming model and serverless runtime
designed for data practitioners. bauplan enables users to declaratively define
functional Directed Acyclic Graphs (DAGs) along with their runtime
environments, which are then efficiently executed on cloud-based workers. We
show that bauplan achieves both better performance and a superior developer
experience for data workloads by making the trade-off of reducing generality in
favor of data-awareness
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 10th International Workshop on Serverless Computing
  (pre-print)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing over FP/EDF Execution Times: Known Results and Open Problems <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Bini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many use cases the execution time of tasks is unknown and can be chosen by
the designer to increase or decrease the application features depending on the
availability of processing capacity. If the application has real-time
constraints, such as deadlines, then the necessary and sufficient
schedulability test must allow the execution times to be left unspecified. By
doing so, the designer can then perform optimization of the execution times by
picking the schedulable values that minimize any given cost.
  In this paper, we review existing results on the formulation of both the
Fixed Priority and Earliest Deadline First exact schedulability constraints.
The reviewed formulations are expressed by a combination of linear constraints,
which enables then optimization routines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at OPERA 2024 (https://opera24.di.unito.it/) This work is
  partially supported by the project "Trustworthy Cyber-Physical Pipelines",
  funded by the MAECI Italy-Sweden co-operation id. PGR02086, and the spoke
  "FutureHPC and BigData" of the ICSC - Centro Nazionale di Ricerca in
  High-Performance Computing, Big Data and Quantum Computing funded by European
  Union -NextGenerationEU</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards tolerant testing stabilizer states 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinivasan Arunachalam, Arkopal Dutt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the following task: suppose an algorithm is given copies of an
unknown $n$-qubit quantum state $|\psi\rangle$ promised $(i)$ $|\psi\rangle$ is
$\varepsilon_1$-close to a stabilizer state in fidelity or $(ii)$
$|\psi\rangle$ is $\varepsilon_2$-far from all stabilizer states, decide which
is the case. We show two results:
  (i) Assuming $|\psi\rangle$ is a phase state, i.e.,
$|\psi\rangle=\frac{1}{\sqrt{2^n}}\sum \limits_{x \in \{0,1\}^n}
{f(x)}|x\rangle$ where $f:\{0,1\}^n\rightarrow \{-1,1\}$, then we give a
$\textsf{poly}(1/\varepsilon_1)$ sample and $n\cdot
\textsf{poly}(1/\varepsilon_1)$ time algorithm for every $\varepsilon_1 > 0$
and $\varepsilon_2 \leq \textsf{poly}(\varepsilon_1)$, for tolerant testing
stabilizer states.
  (ii) For arbitrary quantum states $|\psi\rangle$, assuming a conjecture in
additive combinatorics, we give a $\textsf{poly}(1/\varepsilon_1)$-sample and
$n\cdot \textsf{poly}(1/\varepsilon_1)$-time algorithm for this task for every
$\varepsilon_1>0$ and $\varepsilon_2\leq 2^{-\textsf{poly}(1/\varepsilon_1)}$
  Our proof includes a new definition of Gowers norm for quantum states, an
inverse theorem for the Gowers-$3$ norm of states and new bounds on stabilizer
covering for structured subsets of Paulis using results in additive
combinatorics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 2 figures; v1 required an unproven result in combinatorics
  which is rephrased as a conjecture here</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Complexity of Optimizing Atomic Congestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius Brand, Robert Ganian, Subrahmanyam Kalyanasundaram, Fionn Mc Inerney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atomic congestion games are a classic topic in network design, routing, and
algorithmic game theory, and are capable of modeling congestion and flow
optimization tasks in various application areas. While both the price of
anarchy for such games as well as the computational complexity of computing
their Nash equilibria are by now well-understood, the computational complexity
of computing a system-optimal set of strategies -- that is, a centrally planned
routing that minimizes the average cost of agents -- is severely understudied
in the literature. We close this gap by identifying the exact boundaries of
tractability for the problem through the lens of the parameterized complexity
paradigm. After showing that the problem remains highly intractable even on
extremely simple networks, we obtain a set of results which demonstrate that
the structural parameters which control the computational (in)tractability of
the problem are not vertex-separator based in nature (such as, e.g.,
treewidth), but rather based on edge separators. We conclude by extending our
analysis towards the (even more challenging) min-max variant of the problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short version appeared at AAAI 2024. Long version accepted in the
  Journal of Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $C_{2k+1}$-coloring of bounded-diameter graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06694v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06694v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Piecyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a fixed graph $H$, in the graph homomorphism problem, denoted by
$Hom(H)$, we are given a graph $G$ and we have to determine whether there
exists an edge-preserving mapping $\varphi: V(G) \to V(H)$. Note that
$Hom(C_3)$, where $C_3$ is the cycle of length $3$, is equivalent to
$3$-Coloring. The question whether $3$-Coloring is polynomial-time solvable on
diameter-$2$ graphs is a well-known open problem. In this paper we study the
$Hom(C_{2k+1})$ problem on bounded-diameter graphs for $k\geq 2$, so we
consider all other odd cycles than $C_3$. We prove that for $k\geq 2$, the
$Hom(C_{2k+1})$ problem is polynomial-time solvable on diameter-$(k+1)$ graphs
-- note that such a result for $k=1$ would be precisely a polynomial-time
algorithm for $3$-Coloring of diameter-$2$ graphs.
  Furthermore, we give subexponential-time algorithms for diameter-$(k+2)$
graphs.
  We complement these results with a lower bound for diameter-$(2k+2)$ graphs
-- in this class of graphs the $Hom(C_{2k+1})$ problem is NP-hard and cannot be
solved in subexponential-time, unless the ETH fails.
  Finally, we consider another direction of generalizing $3$-Coloring on
diameter-$2$ graphs. We consider other target graphs $H$ than odd cycles but we
restrict ourselves to diameter $2$. We show that if $H$ is triangle-free, then
$Hom(H)$ is polynomial-time solvable on diameter-$2$ graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Wrong statement about diameter-(k+3) graphs removed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximum Bipartite vs. Triangle-Free Subgraph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamio-Vesa Nakajima, Stanislav Živný
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a (multi)graph $G$ which contains a bipartite subgraph with $\rho$
edges, what is the largest triangle-free subgraph of $G$ that can be found
efficiently? We present an SDP-based algorithm that finds one with at least
$0.8823 \rho$ edges, thus improving on the subgraph with $0.878 \rho$ edges
obtained by the classic Max-Cut algorithm of Goemans and Williamson. On the
other hand, by a reduction from Hastad's 3-bit PCP we show that it is NP-hard
to find a triangle-free subgraph with $(25 / 26 + \epsilon) \rho \approx (0.961
+ \epsilon) \rho$ edges.
  As an application, we classify the Maximum Promise Constraint Satisfaction
Problem MaxPCSP($G$,$H$) for all bipartite $G$: Given an input (multi)graph $X$
which admits a $G$-colouring satisfying $\rho$ edges, find an $H$-colouring of
$X$ that satisfies $\rho$ edges. This problem is solvable in polynomial time,
apart from trivial cases, if $H$ contains a triangle, and is NP-hard otherwise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Subsumes a part of arXiv:2311.00440</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Formal Languages and Automata Theory <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Decision Problem for Regular First-Order Theories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umang Mathur, David Mestel, Mahesh Viswanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical `decision problem' asks whether a given formula of first-order
logic is satisfiable. In this work we consider an extension of this problem to
regular first-order theories, i.e. (infinite) regular sets of formulae.
Building on the beautiful classification of syntactic classes as decidable or
undecidable for the classical decision problem, we show that some classes (the
EPR and Gurevich classes) which are decidable in the classical setting are
undecidable for regular theories; on the other hand for each we show a subclass
which remains decidable in our setting, leaving a complete classification as a
challenge for future work. Finally, we observe that our problem generalises
prior work on verification of uninterpreted programs, and give a semantic class
of existential formulae for which the problem is decidable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Positive Hennessy-Milner Logic for Branching Bisimulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herman Geuvers, Komi Golov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labelled transitions systems can be studied in terms of modal logic and in
terms of bisimulation. These two notions are connected by Hennessy-Milner
theorems, that show that two states are bisimilar precisely when they satisfy
the same modal logic formulas. Recently, apartness has been studied as a dual
to bisimulation, which also gives rise to a dual version of the Hennessy-Milner
theorem: two states are apart precisely when there is a modal formula that
distinguishes them.
  In this paper, we introduce "directed" versions of Hennessy-Milner theorems
that characterize when the theory of one state is included in the other. For
this we introduce "positive modal logics" that only allow a limited use of
negation. Furthermore, we introduce directed notions of bisimulation and
apartness, and then show that, for this positive modal logic, the theory of $s$
is included in the theory of $t$ precisely when $s$ is directed bisimilar to
$t$. Or, in terms of apartness, we show that $s$ is directed apart from $t$
precisely when the theory of $s$ is not included in the theory of $t$. From the
directed version of the Hennessy-Milner theorem, the original result follows.
  In particular, we study the case of branching bisimulation and
Hennessy-Milner Logic with Until (HMLU) as a modal logic. We introduce
"directed branching bisimulation" (and directed branching apartness) and
"Positive Hennessy-Milner Logic with Until" (PHMLU) and we show the directed
version of the Hennessy-Milner theorems. In the process, we show that every
HMLU formula is equivalent to a Boolean combination of Positive HMLU formulas,
which is a very non-trivial result. This gives rise to a sublogic of HMLU that
is equally expressive but easier to reason about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages + appendices (22 pages total)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Decision Problem for Regular First-Order Theories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umang Mathur, David Mestel, Mahesh Viswanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical `decision problem' asks whether a given formula of first-order
logic is satisfiable. In this work we consider an extension of this problem to
regular first-order theories, i.e. (infinite) regular sets of formulae.
Building on the beautiful classification of syntactic classes as decidable or
undecidable for the classical decision problem, we show that some classes (the
EPR and Gurevich classes) which are decidable in the classical setting are
undecidable for regular theories; on the other hand for each we show a subclass
which remains decidable in our setting, leaving a complete classification as a
challenge for future work. Finally, we observe that our problem generalises
prior work on verification of uninterpreted programs, and give a semantic class
of existential formulae for which the problem is decidable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interchangeable Token Embeddings for Extendable Vocabulary and
  Alpha-Equivalence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        İlker Işık, Ramazan Gokberk Cinbis, Ebru Aydin Gol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach for learning interchangeable tokens in language
models to obtain an extendable vocabulary that can generalize to new tokens.
Our method is designed to address alpha-equivalence, the principle that
renaming bound variables in a syntactic expression preserves semantics. This
property arises in many formal languages such as temporal logics, in which all
proposition symbols represent the same concept but are distinguishable from
each other. To handle such tokens, we develop a dual-part embedding approach.
The first part is shared across all interchangeable tokens, thereby enforcing
that they represent the same core concept. The second part is randomly
generated for each token, which enables distinguishability. We evaluate our
method in a Transformer encoder-decoder model on two tasks: solving linear
temporal logic formulae and copying with extendable vocabulary. Our method
demonstrates promising generalization capabilities in addition to introducing a
favorable inductive bias for alpha-equivalence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abstract Operational Methods for Call-by-Push-Value 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Goncharov, Stelios Tsampas, Henning Urbat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Levy's call-by-push-value is a comprehensive programming paradigm that
combines elements from functional and imperative programming, supports
computational effects and subsumes both call-by-value and call-by-name
evaluation strategies. In the present work, we develop modular methods to
reason about program equivalence in call-by-push-value, and in fine-grain
call-by-value, which is a popular lightweight call-by-value sublanguage of the
former. Our approach is based on the fundamental observation that presheaf
categories of sorted sets are suitable universes to model call-by-(push)-value
languages, and that natural, coalgebraic notions of program equivalence such as
applicative similarity and logical relations can be developed within. Starting
from this observation, we formalize fine-grain call-by-value and
call-by-push-value in the higher-order abstract GSOS framework, reduce their
key congruence properties to simple syntactic conditions by leveraging existing
theory and argue that introducing changes to either language incurs minimal
proof overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simply-typed constant-domain modal lambda calculus I: distanced beta
  reduction and combinatory logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Walsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A system $\boldsymbol\lambda_{\upsilon}$ is developed that combines modal
logic and simply-typed lambda calculus, and that generalizes the system studied
by Montague and Gallin. Whereas Montague and Gallin worked with Church's simple
theory of types, the system $\boldsymbol\lambda_{\upsilon}$ is developed in the
typed base theory most commonly used today, namely the simply-typed lambda
calculus. Further, the system $\boldsymbol\lambda_{\upsilon}$ is controlled by
a parameter $\upsilon$ which allows more options for state types and state
variables than is present in Montague and Gallin. A main goal of the paper is
to establish the basic metatheory of $\boldsymbol\lambda_{\upsilon}$: (i) a
completeness theorem is proven for $\beta\eta$-reduction, and (ii) an
Andrews-like characterization of Henkin models in terms of combinatory logic is
given; and this involves a distanced version of $\beta$-reduction and a
$\mathsf{BCKW}$-like basis rather than $\mathsf{SKI}$-like basis. Further,
conservation of the maximal system $\boldsymbol\lambda_{\omega}$ over
$\boldsymbol\lambda_{\upsilon}$ is proven, and expressibility of
$\boldsymbol\lambda_{\omega}$ in $\boldsymbol\lambda_{\upsilon}$ is proven;
thus these modal logics are highly expressive. Similar results are proven for
the relation between $\boldsymbol\lambda_{\omega}$ and $\boldsymbol\lambda$,
the corresponding ordinary simply-typed lambda calculus. This answers a
question of Zimmerman in the simply-typed setting. In a companion paper this is
extended to Church's simple theory of types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Complex Queries on Knowledge Graphs with Neural Link
  Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07063v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07063v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yin, Zihao Wang, Yangqiu Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning on knowledge graphs is a challenging task because it utilizes
observed information to predict the missing one. Particularly, answering
complex queries based on first-order logic is one of the crucial tasks to
verify learning to reason abilities for generalization and composition.
Recently, the prevailing method is query embedding which learns the embedding
of a set of entities and treats logic operations as set operations and has
shown great empirical success. Though there has been much research following
the same formulation, many of its claims lack a formal and systematic
inspection. In this paper, we rethink this formulation and justify many of the
previous claims by characterizing the scope of queries investigated previously
and precisely identifying the gap between its formulation and its goal, as well
as providing complexity analysis for the currently investigated queries.
Moreover, we develop a new dataset containing ten new types of queries with
features that have never been considered and therefore can provide a thorough
investigation of complex queries. Finally, we propose a new neural-symbolic
method, Fuzzy Inference with Truth value (FIT), where we equip the neural link
predictors with fuzzy logic theory to support end-to-end learning using complex
queries with provable reasoning capability. Empirical results show that our
method outperforms previous methods significantly in the new dataset and also
surpasses previous methods in the existing dataset at the same time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Received in ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Positive Hennessy-Milner Logic for Branching Bisimulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herman Geuvers, Komi Golov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labelled transitions systems can be studied in terms of modal logic and in
terms of bisimulation. These two notions are connected by Hennessy-Milner
theorems, that show that two states are bisimilar precisely when they satisfy
the same modal logic formulas. Recently, apartness has been studied as a dual
to bisimulation, which also gives rise to a dual version of the Hennessy-Milner
theorem: two states are apart precisely when there is a modal formula that
distinguishes them.
  In this paper, we introduce "directed" versions of Hennessy-Milner theorems
that characterize when the theory of one state is included in the other. For
this we introduce "positive modal logics" that only allow a limited use of
negation. Furthermore, we introduce directed notions of bisimulation and
apartness, and then show that, for this positive modal logic, the theory of $s$
is included in the theory of $t$ precisely when $s$ is directed bisimilar to
$t$. Or, in terms of apartness, we show that $s$ is directed apart from $t$
precisely when the theory of $s$ is not included in the theory of $t$. From the
directed version of the Hennessy-Milner theorem, the original result follows.
  In particular, we study the case of branching bisimulation and
Hennessy-Milner Logic with Until (HMLU) as a modal logic. We introduce
"directed branching bisimulation" (and directed branching apartness) and
"Positive Hennessy-Milner Logic with Until" (PHMLU) and we show the directed
version of the Hennessy-Milner theorems. In the process, we show that every
HMLU formula is equivalent to a Boolean combination of Positive HMLU formulas,
which is a very non-trivial result. This gives rise to a sublogic of HMLU that
is equally expressive but easier to reason about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages + appendices (22 pages total)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural temporal logic for mechanized program verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleftherios Ioannidis, Yannick Zakowski, Steve Zdancewic, Sebastian Angel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanized verification of liveness properties for programs with effects,
nondeterminism, and nontermination is difficult. Existing temporal reasoning
frameworks operate on the level of models (traces, automata) not executable
code, creating a verification gap and losing the benefits of modularity and
composition enjoyed by structural program logics. Reasoning about infinite
traces and automata requires complex (co-)inductive proof techniques and
familiarity with proof assistant mechanics (e.g., guardedness checker). We
propose a structural approach to the verification of temporal properties with a
new temporal logic that we call Ticl. Using Ticl, we internalize complex
(co-)inductive proof techniques to structural lemmas and reasoning about
variants and invariants. We show that it is possible to perform mechanized
proofs of general temporal properties, while working in a high-level of
abstraction. We demonstrate the benefits of ticl by giving short, structural
proofs of safety and liveness properties for programs with queues, secure
memory, and distributed consensus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-21T00:00:00Z">2024-10-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAIM: Scalable Analog Ising Machine for Solving Quadratic Binary
  Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasan Razmkhah, Jui-Yu Huang, Mehdi Kamal, Massoud Pedram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a CMOS-compatible Lechner-Hauke-Zoller (LHZ)--based
analog tile structure as a fundamental unit for developing scalable analog
Ising machines (IMs). In the designed LHZ tile, the voltage-controlled
oscillators are employed as the physical Ising spins, while for the ancillary
spins, we introduce an oscillator-based circuit to emulate the constraint
needed to ensure the correct functionality of the tile. We implement the
proposed LHZ tile in 12nm FinFET technology using the Cadence Virtuoso.
Simulation results show the proposed tile could converge to the results in
about 31~ns. Also, the designed spins could operate at approximately 13~GHz.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 8 figures, prepared in IEEE format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ControlPULPlet: A Flexible Real-time Multi-core RISC-V Controller for
  2.5D Systems-in-package 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Ottaviano, Robert Balas, Tim Fischer, Thomas Benz, Andrea Bartolini, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of real-time control algorithms and the trend
toward 2.5D technology necessitate the development of scalable controllers for
managing the complex, integrated operation of chiplets within 2.5D
systems-in-package. These controllers must provide real-time computing
capabilities and have chiplet-compatible IO interfaces for communication with
the controlled components. This work introduces ControlPULPlet, a
chiplet-compatible, real-time multi-core RISC-V controller, which is available
as an open-source release. It includes a 32-bit CV32RT core for efficient
interrupt handling and a specialized direct memory access (DMA) engine to
automate periodic sensor readouts. A tightly-coupled programmable multi-core
accelerator is integrated via a dedicated AXI4 port. A flexible AXI4-compatible
die-to-die (D2D) link supports inter-chiplet communication in 2.5D systems and
enables high-bandwidth transfers in traditional 2D monolithic setups. We
designed and fabricated ControlPULPlet as a silicon prototype called Kairos
using TSMC's 65nm CMOS technology. Kairos executes predictive control
algorithms at up to 290 MHz while consuming just 30 mW of power. The D2D link
requires only 16.5 kGE in physical area per channel, adding just 2.9% to the
total system area. It supports off-die access with an energy efficiency of 1.3
pJ/b and achieves a peak duplex transfer rate of 51 Gb/s per second at 200 MHz.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4.5 pages, 11 figures, submitted to Transactions on Circuits and
  Systems Part II - Express Briefs (TCAS-II)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formalising CXL Cache Coherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengsong Tan, Alastair F. Donaldson, John Wickerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report our experience formally modelling and verifying CXL.cache, the
inter-device cache coherence protocol of the Compute Express Link standard. We
have used the Isabelle proof assistant to create a formal model for CXL.cache
based on the prose English specification. This led to us identifying and
proposing fixes to several problems we identified as unclear, ambiguous or
inaccurate, some of which could lead to incoherence if left unfixed. Nearly all
our issues and proposed fixes have been confirmed and tentatively accepted by
the CXL consortium for adoption, save for one which is still under discussion.
To validate the faithfulness of our model we performed scenario verification of
essential restrictions such as "Snoop-pushes-GO", and produced a fully
mechanised proof of a coherence property of the model. The considerable size of
this proof, comprising tens of thousands of lemmas, prompted us to develop new
proof automation tools, which we have made available for other Isabelle users
working with similarly cumbersome proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TEXEL: A neuromorphic processor with on-chip learning for beyond-CMOS
  device integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugh Greatorex, Ole Richter, Michele Mastella, Madison Cotteret, Philipp Klein, Maxime Fabre, Arianna Rubino, Willian Soares Girão, Junren Chen, Martin Ziegler, Laura Bégon-Lours, Giacomo Indiveri, Elisabetta Chicca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in memory technologies, devices and materials have shown
great potential for integration into neuromorphic electronic systems. However,
a significant gap remains between the development of these materials and the
realization of large-scale, fully functional systems. One key challenge is
determining which devices and materials are best suited for specific functions
and how they can be paired with CMOS circuitry. To address this, we introduce
TEXEL, a mixed-signal neuromorphic architecture designed to explore the
integration of on-chip learning circuits and novel two- and three-terminal
devices. TEXEL serves as an accessible platform to bridge the gap between
CMOS-based neuromorphic computation and the latest advancements in emerging
devices. In this paper, we demonstrate the readiness of TEXEL for device
integration through comprehensive chip measurements and simulations. TEXEL
provides a practical system for testing bio-inspired learning algorithms
alongside emerging devices, establishing a tangible link between brain-inspired
computation and cutting-edge device research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures. Supplementary material: 8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepVigor+: Scalable and Accurate Semi-Analytical Fault Resilience
  Analysis for Deep Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hasan Ahmadilivani, Jaan Raik, Masoud Daneshtalab, Maksim Jenihhin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing exploitation of Machine Learning (ML) in safety-critical applications
necessitates rigorous safety analysis. Hardware reliability assessment is a
major concern with respect to measuring the level of safety. Quantifying the
reliability of emerging ML models, including Deep Neural Networks (DNNs), is
highly complex due to their enormous size in terms of the number of parameters
and computations. Conventionally, Fault Injection (FI) is applied to perform a
reliability measurement. However, performing FI on modern-day DNNs is
prohibitively time-consuming if an acceptable confidence level is to be
achieved. In order to speed up FI for large DNNs, statistical FI has been
proposed. However, the run-time for the large DNN models is still considerably
long.
  In this work, we introduce DeepVigor+, a scalable, fast and accurate
semi-analytical method as an efficient alternative for reliability measurement
in DNNs. DeepVigor+ implements a fault propagation analysis model and attempts
to acquire Vulnerability Factors (VFs) as reliability metrics in an optimal
way. The results indicate that DeepVigor+ obtains VFs for DNN models with an
error less than 1\% and 14.9 up to 26.9 times fewer simulations than the
best-known state-of-the-art statistical FI enabling an accurate reliability
analysis for emerging DNNs within a few minutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 8 tables, 16 equations. The source code is
  accessible via: https://github.com/mhahmadilivany/DeepVigor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of a 64-bit SQRT-CSLA with Reduced Area and High-Speed
  Applications in Low Power VLSI Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        CH. Pallavi, C. Padma, R. Kiran Kumar, T. Suguna, C. Nalini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main areas of research in VLSI system design include area, high speed,
and power-efficient data route logic systems. The amount of time needed to send
a carry through the adder limits the pace at which addition can occur in
digital adders. One of the quickest adders, the Carry Select Adder (CSLA), is
utilized by various data processing processors to carry out quick arithmetic
operations. It is evident from the CSLA's structure that there is room to cut
back on both the area and the delay. This work employs a straightforward and
effective gate-level adjustment (in a regular structure) that significantly
lowers the CSLA's area and delay. In light of this adjustment Square-Root Carry
Select Adder (SQRT CSLA) designs with bit lengths of 8, 16, 32, and 64. When
compared to the standard SQRT CSLA, the suggested design significantly reduces
both area and latency. Xilinx ISE tool is used for Simulation and synthesis.
The performance of the recommended designs in terms of delay is estimated in
this study using the standard designs. The study of the findings indicates that
the suggested CSLA structure outperforms the standard SQRT CSLA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hiding in Plain Sight: Reframing Hardware Trojan Benchmarking as a
  Hide&Seek Modification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Sarihi, Ahmad Patooghy, Peter Jamieson, Abdel-Hameed A. Badawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work focuses on advancing security research in the hardware design space
by formally defining the realistic problem of Hardware Trojan (HT) detection.
The goal is to model HT detection more closely to the real world, i.e.,
describing the problem as The Seeker's Dilemma where a detecting agent is
unaware of whether circuits are infected by HTs or not. Using this theoretical
problem formulation, we create a benchmark that consists of a mixture of
HT-free and HT-infected restructured circuits while preserving their original
functionalities. The restructured circuits are randomly infected by HTs,
causing a situation where the defender is uncertain if a circuit is infected or
not. We believe that our innovative benchmark and methodology of creating
benchmarks will help the community judge the detection quality of different
methods by comparing their success rates in circuit classification. We use our
developed benchmark to evaluate three state-of-the-art HT detection tools to
show baseline results for this approach. We use Principal Component Analysis to
assess the strength of our benchmark, where we observe that some restructured
HT-infected circuits are mapped closely to HT-free circuits, leading to
significant label misclassification by detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2402.17918</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAMOUS: Flexible Accelerator for the Attention Mechanism of <span class="highlight-title">Transformer</span>
  on UltraScale+ <span class="highlight-title">FPGA</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Kabir, Md. Arafat Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer neural networks (TNNs) are being applied across a widening range
of application domains, including natural language processing (NLP), machine
translation, and computer vision (CV). Their popularity is largely attributed
to the exceptional performance of their multi-head self-attention blocks when
analyzing sequential data and extracting features. To date, there are limited
hardware accelerators tailored for this mechanism, which is the first step
before designing an accelerator for a complete model. This paper proposes
\textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention
(MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is
optimized for high utilization of processing elements and on-chip memories to
improve parallelism and reduce latency. An efficient tiling of large matrices
has been employed to distribute memory and computing resources across different
modules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C
and U200 data center cards containing Ultrascale+ FPGAs. Experimental results
are presented that show that it can attain a maximum throughput, number of
parallel attention heads, embedding dimension and tile size of 328 (giga
operations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore,
it is 3.28$\times$ and 2.6$\times$ faster than the Intel Xeon Gold 5220R CPU
and NVIDIA V100 GPU respectively. It is also 1.3$\times$ faster than the
fastest state-of-the-art FPGA-based accelerator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2409.13975</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrojanForge: Generating Adversarial Hardware Trojan Examples with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Sarihi, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Hardware Trojan (HT) problem can be thought of as a continuous game
between attackers and defenders, each striving to outsmart the other by
leveraging any available means for an advantage. Machine Learning (ML) has
recently played a key role in advancing HT research. Various novel techniques,
such as Reinforcement Learning (RL) and Graph Neural Networks (GNNs), have
shown HT insertion and detection capabilities. HT insertion with ML techniques,
specifically, has seen a spike in research activity due to the shortcomings of
conventional HT benchmarks and the inherent human design bias that occurs when
we create them. This work continues this innovation by presenting a tool called
TrojanForge, capable of generating HT adversarial examples that defeat HT
detectors; demonstrating the capabilities of GAN-like adversarial tools for
automatic HT insertion. We introduce an RL environment where the RL insertion
agent interacts with HT detectors in an insertion-detection loop where the
agent collects rewards based on its success in bypassing HT detectors. Our
results show that this process helps inserted HTs evade various HT detectors,
achieving high attack success percentages. This tool provides insight into why
HT insertion fails in some instances and how we can leverage this knowledge in
defense.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdChain: Decentralized Header Bidding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behkish Nassirzadeh, Albert Heinle, Stefanos Leonardos, Anwar Hasan, Vijay Ganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the involvement of multiple intermediaries without trusted parties,
lack of proper regulations, and a complicated supply chain, ad impression
discrepancy affects online advertising. This issue causes up to $82 billion
annual revenue loss for honest parties. The loss can be significantly reduced
with a precise and trusted decentralized mechanism. This paper presents
AdChain, a decentralized, distributed, and verifiable solution that detects and
minimizes online advertisement impression discrepancies. AdChain establishes
trust by employing multiple independent agents to receive and record log-level
data, along with a consensus protocol to validate each ad data. AdChain is
scalable, efficient, and compatible with the current infrastructure. Our
experimental evaluation, using over half a million ad data points, identifies
system parameters that achieve 98% accuracy, reducing the ad discrepancy rate
from 20% to 2%. Our cost analysis shows that active nodes on AdChain can
generate profits comparable to miners on major blockchain networks like
Bitcoin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Being published at MARBLE 2024 (The 5th International Conference on
  Mathematical Research for Blockchain Economy)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUMBO: Making durable read-only transactions fly on hardware
  transactional memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Barreto, Daniel Castro, Paolo Romano, Alexandro Baldassin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent improvements in supporting Persistent Hardware
Transactions (PHTs) on emerging persistent memories (PM), the poor performance
of Read-Only (RO) transactions remains largely overlooked. We propose DUMBO, a
new design for PHT that eliminates the two most crucial bottlenecks that hinder
RO transactions in state-of-the-art PHT. At its core, DUMBO exploits advanced
instructions that some contemporary HTMs provide to suspend (and resume)
transactional access tracking. Our experimental evaluation with an IBM POWER9
system using the TPC-C benchmark shows that DUMBO can outperform the state of
the art designs for persistent hardware (SPHT) and software memory transactions
(Pisces), by up to 4.0x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Final Report for CHESS: Cloud, High-Performance Computing, and Edge for
  Science and Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Tallent, Jan Strube, Luanzheng Guo, Hyungro Lee, Jesun Firoz, Sayan Ghosh, Bo Fang, Oceane Bel, Steven Spurgeon, Sarah Akers, Christina Doty, Erol Cromwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the theory-experiment cycle requires effective distributed
workflows that utilize a computing continuum spanning lab instruments, edge
sensors, computing resources at multiple facilities, data sets distributed
across multiple information sources, and potentially cloud. Unfortunately, the
obvious methods for constructing continuum platforms, orchestrating workflow
tasks, and curating datasets over time fail to achieve scientific requirements
for performance, energy, security, and reliability. Furthermore, achieving the
best use of continuum resources depends upon the efficient composition and
execution of workflow tasks, i.e., combinations of numerical solvers, data
analytics, and machine learning. Pacific Northwest National Laboratory's LDRD
"Cloud, High-Performance Computing (HPC), and Edge for Science and Security"
(CHESS) has developed a set of interrelated capabilities for enabling
distributed scientific workflows and curating datasets. This report describes
the results and successes of CHESS from the perspective of open science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperDrive: Scheduling Serverless Functions in the Edge-Cloud-Space 3D
  Continuum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Pusztai, Cynthia Marcelino, Stefan Nastic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of Low Earth Orbit~(LEO) satellites has grown enormously in the
past years. Their abundance and low orbits allow for low latency communication
with a satellite almost anywhere on Earth, and high-speed inter-satellite laser
links~(ISLs) enable a quick exchange of large amounts of data among satellites.
As the computational capabilities of LEO satellites grow, they are becoming
eligible as general-purpose compute nodes. In the 3D continuum, which combines
Cloud and Edge nodes on Earth and satellites in space into a seamless computing
fabric, workloads can be executed on any of the aforementioned compute nodes,
depending on where it is most beneficial. However, scheduling on LEO satellites
moving at approx. 27,000 km/h requires picking the satellite with the lowest
latency to all data sources (ground and, possibly, earth observation
satellites). Dissipating heat from onboard hardware is challenging when facing
the sun and workloads must not drain the satellite's batteries. These factors
make meeting SLOs more challenging than in the Edge-Cloud continuum, i.e., on
Earth alone. We present HyperDrive, an SLO-aware scheduler for serverless
functions specifically designed for the 3D continuum. It places functions on
Cloud, Edge, or Space compute nodes, based on their availability and ability to
meet the SLO requirements of the workflow. We evaluate HyperDrive using a
wildfire disaster response use case with high Earth Observation data processing
requirements and stringent SLOs, showing that it enables the design and
execution of such next-generation 3D scenarios with 71% lower network latency
than the best baseline scheduler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE/ACM Symposium on Edge Computing(SEC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Product Passport Management with Decentralised Identifiers and
  Verifiable Credentials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ismael Illán García, Francesc D. Muñoz-Escoí, Jordi Arjona Aroca, F. Javier Fernández-Bravo Peñuela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital product passports (DPP) have been proposed in the European Ecodesign
for Sustainable Products Regulation (ESPR) as a means to keep and provide
product information that facilitates product reusage, reparation, and
recycling. Thus, DPPs should provide a positive effect on the environmental
impact of future manufactured products, preventing waste and promoting a
circular economy (CE) model. ESPR settles a set of requirements in collecting
and administering product-related data. Decentralised identifiers (DID) and
verifiable credentials (VC) are two self-sovereign-identity-related elements
that may help in that DPP management since they introduce a decentralised
administration of identity that may enhance the overall scalability of the
resulting system, improving also its reliability. This paper analyses the ESPR
requirements and describes how they may be achieved using DIDs and VCs,
assessing their performance in some scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning with MMD-based Early Stopping for Adaptive GNSS
  Interference Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishant S. Gaikwad, Lucas Heublein, Nisha L. Raichur, Tobias Feigl, Christopher Mutschler, Felix Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables multiple devices to collaboratively train a
global model while maintaining data on local servers. Each device trains the
model on its local server and shares only the model updates (i.e., gradient
weights) during the aggregation step. A significant challenge in FL is managing
the feature distribution of novel, unbalanced data across devices. In this
paper, we propose an FL approach using few-shot learning and aggregation of the
model weights on a global server. We introduce a dynamic early stopping method
to balance out-of-distribution classes based on representation learning,
specifically utilizing the maximum mean discrepancy of feature embeddings
between local and global models. An exemplary application of FL is
orchestrating machine learning models along highways for interference
classification based on snapshots from global navigation satellite system
(GNSS) receivers. Extensive experiments on four GNSS datasets from two
real-world highways and controlled environments demonstrate that our FL method
surpasses state-of-the-art techniques in adapting to both novel interference
classes and multipath scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Quantum-HPC Solutions for Max-Cut: Bridging Classical and Quantum
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishan Patwardhan, Akhil Akkapelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research explores the integration of the Quantum Approximate
Optimization Algorithm (QAOA) into Hybrid Quantum-HPC systems for solving the
Max-Cut problem, comparing its performance with classical algorithms like
brute-force search and greedy heuristics. We develop a theoretical model to
analyze the time complexity, scalability, and communication overhead in hybrid
systems. Using simulations, we evaluate QAOA's performance on small-scale
Max-Cut instances, benchmarking its runtime, solution accuracy, and resource
utilization. The study also investigates the scalability of QAOA with
increasing problem size, offering insights into its potential advantages over
classical methods for large-scale combinatorial optimization problems, with
implications for future Quantum computing applications in HPC environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE PuneCon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Parallel Program Performance Through DSL-Driven Code
  Generation with <span class="highlight-title">LLM</span> Optimizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping computations to processors and assigning data to memory are critical
for maximizing performance in parallel programming. These mapping decisions are
managed through the development of specialized low-level system code, called
mappers, crafted by performance engineers. Each mapper is tailored to a
specific application and optimized for the underlying machine architecture, a
process that requires days of refinement and tuning from an expert. Despite
advances in system research, automating mapper generation remains a challenge
due to the complexity of making millions of decisions to find the optimal
solution and generate the solution as code. We introduce an approach that
leverages recent advances in LLM-based optimizers for mapper design. In under
ten minutes, our method automatically discovers mappers that surpass human
expert designs in scientific applications by up to 1.34X speedup. For parallel
matrix multiplication algorithms, our mapper achieves up to 1.31X of the
expert-designed solution. To achieve this, we simplify the complexity of
low-level code generation by introducing a domain-specific language (DSL) that
abstracts the low-level system programming details and defines a structured
search space for LLMs to explore. To maximize the application performance, we
use an LLM optimizer to improve an agentic system that generates the mapper
code. As a result, this approach significantly reduces the workload for
performance engineers while achieving substantial performance gains across
diverse applications. Finally, our results demonstrate the effectiveness of
LLM-based optimization in system design and suggest its potential for
addressing other complex system challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streamlining Cloud-Native Application Development and Deployment with
  Robust Encapsulation <span class="chip">SoCC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawissanutt Lertpongrujikorn, Hai Duc Nguyen, Mohsen Amini Salehi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Serverless abstractions (e.g., FaaS) poorly support non-functional
requirements (e.g., QoS and constraints), are provider-dependent, and are
incompatible with other cloud abstractions (e.g., databases). As a result,
application developers have to undergo numerous rounds of development and
manual deployment refinements to finally achieve their desired quality and
efficiency. In this paper, we present Object-as-a-Service (OaaS) -- a novel
serverless paradigm that borrows the object-oriented programming concepts to
encapsulate business logic, data, and non-functional requirements into a single
deployment package, thereby streamlining provider-agnostic cloud-native
application development. We also propose a declarative interface for the
non-functional requirements of applications that relieves developers from
daunting refinements to meet their desired QoS and deployment constraint
targets. We realized the OaaS paradigm through a platform called Oparaca and
evaluated it against various real-world applications and scenarios. The
evaluation results demonstrate that Oparaca can enhance application performance
by 60X and improve reliability by 50X through latency, throughput, and
availability enforcement -- all with remarkably less development and deployment
time and effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM Symposium of Cloud Computing (SoCC '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adventures with Grace Hopper AI Super Chip and the National Research
  Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Alex Hurt, Grant J. Scott, Derek Weitzel, Huijun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The National Science Foundation (NSF) funded National Research Platform (NRP)
is a hyper-converged cluster of nationally and globally interconnected
heterogeneous computing resources. The dominant computing environment of the
NRP is the x86 64 instruction set architecture (ISA), often with graphics
processing units (GPUs). Researchers across the nation leverage containers and
Kubernetes to execute high-throughput computing (HTC) workloads across the
heterogeneous cyberinfrastructure with minimal friction and maximum
flexibility. As part of the NSF-funded GP-ENGINE project, we stood up the first
server with an NVIDIA Grace Hopper AI Chip (GH200), an alternative ARM ISA, for
the NRP. This presents challenges, as containers must be specifically built for
ARM versus x86 64. Herein, we describe the challenges encountered, as well as
our resulting solutions and some relevant performance benchmarks. We
specifically compare the GH200 to A100 for computer vision workloads, within
compute nodes in the NRP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Communication-Efficient Multi-Objective Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baris Askin, Pranay Sharma, Gauri Joshi, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a federated version of multi-objective optimization (MOO), where a
single model is trained to optimize multiple objective functions. MOO has been
extensively studied in the centralized setting but is less explored in
federated or distributed settings. We propose FedCMOO, a novel
communication-efficient federated multi-objective optimization (FMOO) algorithm
that improves the error convergence performance of the model compared to
existing approaches. Unlike prior works, the communication cost of FedCMOO does
not scale with the number of objectives, as each client sends a single
aggregated gradient, obtained using randomized SVD (singular value
decomposition), to the central server. We provide a convergence analysis of the
proposed method for smooth non-convex objective functions under milder
assumptions than in prior work. In addition, we introduce a variant of FedCMOO
that allows users to specify a preference over the objectives in terms of a
desired ratio of the final objective values. Through extensive experiments, we
demonstrate the superiority of our proposed method over baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal, Non-pipelined Reduce-scatter and Allreduce Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesper Larsson Träff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reduce-scatter collective operation in which $p$ processors in a network
of processors collectively reduce $p$ input vectors into a result vector that
is partitioned over the processors is important both in its own right and as
building block for other collective operations. We present a surprisingly
simple, but non-trivial algorithm for solving this problem optimally in
$\lceil\log_2 p\rceil$ communication rounds with each process sending,
receiving and reducing exactly $p-1$ blocks of vector elements. We combine this
with a similarly simple allgather algorithm to get a likewise optimal algorithm
for the allreduce collective operation where the result vector is replicated on
all processors. The communication pattern is a simple, $\lceil\log_2
p\rceil$-regular, circulant graph also used elsewhere. The algorithms assume
the binary reduction operator to be commutative and we discuss this assumption.
The algorithms can readily be implemented and used for the collective
operations MPI_Reduce_scatter_block, MPI_Reduce_scatter and MPI_Allreduce as
specified in the MPI standard. The communication pattern can likewise be used
for all-to-all communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Massively Parallel Ruling Set Made Deterministic <span class="chip">SC'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeff Giliberti, Zahra Parsaeian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the deterministic complexity of the $2$-Ruling Set problem in the
model of Massively Parallel Computation (MPC) with linear and strongly
sublinear local memory.
  Linear MPC: We present a constant-round deterministic algorithm for the
$2$-Ruling Set problem that matches the randomized round complexity recently
settled by Cambus, Kuhn, Pai, and Uitto [DISC'23], and improves upon the
deterministic $O(\log \log n)$-round algorithm by Pai and Pemmaraju [PODC'22].
Our main ingredient is a simpler analysis of CKPU's algorithm based solely on
bounded independence, which makes its efficient derandomization possible.
  Sublinear MPC: We present a deterministic algorithm that computes a
$2$-Ruling Set in $\tilde O(\sqrt{\log n})$ rounds deterministically. Notably,
this is the first deterministic ruling set algorithm with sublogarithmic round
complexity, improving on the $O(\log \Delta + \log \log^* n)$-round complexity
that stems from the deterministic MIS algorithm of Czumaj, Davies, and Parter
[TALG'21]. Our result is based on a simple and fast randomness-efficient
construction that achieves the same sparsification as that of the randomized
$\tilde O(\sqrt{\log n})$-round LOCAL algorithm by Kothapalli and Pemmaraju
[FSTTCS'12].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at DISC'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusionize++: Improving Serverless Application Performance Using Dynamic
  Task Inlining and Infrastructure Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trever Schirmer, Joel Scheuner, Tobias Pfandzelter, David Bermbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Function-as-a-Service (FaaS) execution model increases developer
productivity by removing operational concerns such as managing hardware or
software runtimes. Developers, however, still need to partition their
applications into FaaS functions, which is error-prone and complex:
Encapsulating only the smallest logical unit of an application as a FaaS
function maximizes flexibility and reusability. Yet, it also leads to
invocation overheads, additional cold starts, and may increase cost due to
double billing during synchronous invocations. Conversely, deploying an entire
application as a single FaaS function avoids these overheads but decreases
flexibility. In this paper we present Fusionize, a framework that automates
optimizing for this trade-off by automatically fusing application code into an
optimized multi-function composition. Developers only need to write
fine-grained application code following the serverless model, while Fusionize
automatically fuses different parts of the application into FaaS functions,
manages their interactions, and configures the underlying infrastructure. At
runtime, it monitors application performance and adapts it to minimize
request-response latency and costs. Real-world use cases show that Fusionize
can improve the deployment artifacts of the application, reducing both median
request-response latency and cost of an example IoT application by more than
35%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author copy of article accepted in IEEE Transactions on Cloud
  Computing with DOI 10.1109/TCC.2024.3451108</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WarmSwap: Sharing Dependencies for Accelerating Cold Starts in
  Serverless Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Devesh Tiwari, Gene Cooperman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents WarmSwap, a novel provider-side cold-start optimization
for serverless computing. This optimization reduces cold-start time when
booting and loading dependencies at runtime inside a function container.
Previous approaches to the optimization of cold starts tend to fall into two
categories: optimizing the infrastructure of serverless computing to benefit
all serverless functions; or function-specific tuning for individual serverless
functions. In contrast, WarmSwap offers a broad middle ground, which optimizes
entire categories of serverless functions. WarmSwap eliminates the need to
initialize middleware or software dependencies when launching a new serverless
container, by migrating a pre-initialized live dependency image to the new
function instance. WarmSwap respects the provider's cache constraints, as a
single pre-warmed dependency image in the cache is shared among all serverless
functions requiring that software dependency image. WarmSwap has been tested on
seven representative functions from FunctionBench. In those tests, WarmSwap
accelerates dependency loading for serverless functions with large dependency
requirements by a factor ranging from 2.2 to 3.2. Simulation experiments using
Azure traces indicate that WarmSwap can save 88\% of optimization space when
sharing a dependency image among ten different functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effect of Personalization in FedProx: A Fine-grained Analysis on
  Statistical Accuracy and Communication Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FedProx is a simple yet effective federated learning method that enables
model personalization via regularization. Despite remarkable success in
practice, a rigorous analysis of how such a regularization provably improves
the statistical accuracy of each client's local model hasn't been fully
established. Setting the regularization strength heuristically presents a risk,
as an inappropriate choice may even degrade accuracy. This work fills in the
gap by analyzing the effect of regularization on statistical accuracy, thereby
providing a theoretical guideline for setting the regularization strength for
achieving personalization. We prove that by adaptively choosing the
regularization strength under different statistical heterogeneity, FedProx can
consistently outperform pure local training and achieve a nearly
minimax-optimal statistical rate. In addition, to shed light on resource
allocation, we design an algorithm, provably showing that stronger
personalization reduces communication complexity without increasing the
computation cost overhead. Finally, our theory is validated on both synthetic
and real-world datasets and its generalizability is verified in a non-convex
setting.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance Profiling <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Final Report for CHESS: Cloud, High-Performance Computing, and Edge for
  Science and Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Tallent, Jan Strube, Luanzheng Guo, Hyungro Lee, Jesun Firoz, Sayan Ghosh, Bo Fang, Oceane Bel, Steven Spurgeon, Sarah Akers, Christina Doty, Erol Cromwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the theory-experiment cycle requires effective distributed
workflows that utilize a computing continuum spanning lab instruments, edge
sensors, computing resources at multiple facilities, data sets distributed
across multiple information sources, and potentially cloud. Unfortunately, the
obvious methods for constructing continuum platforms, orchestrating workflow
tasks, and curating datasets over time fail to achieve scientific requirements
for performance, energy, security, and reliability. Furthermore, achieving the
best use of continuum resources depends upon the efficient composition and
execution of workflow tasks, i.e., combinations of numerical solvers, data
analytics, and machine learning. Pacific Northwest National Laboratory's LDRD
"Cloud, High-Performance Computing (HPC), and Edge for Science and Security"
(CHESS) has developed a set of interrelated capabilities for enabling
distributed scientific workflows and curating datasets. This report describes
the results and successes of CHESS from the perspective of open science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADS Performance Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Weber, Holger Eichelberger, Jobst Hildebrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time measurements are important for in-depth control of manufacturing
processes, which, for modern AI methods, need integration with high-level
languages. In our last SSP paper we investigated the performance of a Python
and a Java-JNA based approach to integrate the Beckhoff ADS protocol for
real-time edge communication into an Industry 4.0 platform. There, we have
shown that while Java outperforms Python, both solutions do not meet the
desired goal of 1-20kHz depending on the task. However, we are are still
lacking an explanation for this result as well as an analysis of alternatives.
For the first topic, we show in this paper that 1) exchanging Java-JNA with
Java-JNI in this setting does not further improve the performance 2) a C++
program realizing the same behavior in a more direct integration does not
perform better and 3) profiling shows that the majority of the execution is
spend in ADS. For the second topic, we show that alternative uses of the ADS
library allow for better performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Three pages about ADS integration into Java</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Industry 4.0 Connectors -- A Performance Experiment with Modbus/TCP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Nikolajew, Holger Eichelberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For Industry 4.0 applications, communication protocols and data formats even
for legacy devices are fundamental. In this paper, we focus on the Modbus/TCP
protocol, which is, e.g., used in energy metering. Allowing Industry 4.0
applications to include data from such protocols without need for programming
would increase flexibility and, in turn, improve development efficiency. As one
particular approach, we discuss the automated generation of Modbus/TCP
connectors for our Open Source oktoflow platform and compare the performance of
handcrafted as well as generated connectors in different settings, including
industrial energy metering devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on
  Commercial DRAM-PIMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkai Chen, Tianhua Han, Cheng Liu, Shengwen Liang, Kuai Yu, Lei Dai, Ziming Yuan, Ying Wang, Lei Zhang, Huawei Li, Xiaowei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic
similarity search in large datasets, has become a fundamental component of
critical applications such as information retrieval and retrieval-augmented
generation (RAG). However, ANNS is a well-known I/O-intensive algorithm with a
low compute-to-I/O ratio, often requiring massive storage due to the large
volume of high-dimensional data. This leads to I/O bottlenecks on CPUs and
memory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM)
architecture, which offers high bandwidth, large-capacity memory, and the
ability to perform efficient computation in or near the data, presents a
promising solution for ANNS. In this work, we investigate the use of commercial
DRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS
engine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM
exhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup
tables (LUTs) to replace more multiplications with I/O operations. We then
systematically tune ANNS to search optimized configurations with lower
computational load, aligning the compute-to-I/O ratio of ANNS with that of
DRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS
algorithm, we further explore implementation optimizations to fully utilize the
two thousand parallel processing units with private local memory in DRAM-PIMs.
To address the load imbalance caused by ANNS requests distributed across
different clusters of large datasets, we propose a load-balancing strategy that
combines static data layout optimization with dynamic runtime request
scheduling. Experimental results on representative datasets show that DRIM-ANN
achieves an average performance speedup of 2.92x compared to a 32-thread CPU
counterpart.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Optimality of the Speed-Aware Join-the-Shortest-Queue in the
  Halfin-Whitt Regime for <span class="highlight-title">Heterogeneous</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10497v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10497v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanidhay Bhambay, Burak Büke, Arpan Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Join-the-Shortest-Queue (JSQ) load balancing scheme is known to minimise
the average response time of jobs in homogeneous systems with identical
servers. However, for {\em heterogeneous} systems with servers having different
processing speeds, finding an optimal load balancing scheme remains an open
problem for finite system sizes. Recently, for systems with heterogeneous
servers, a variant of the JSQ scheme, called the {\em
Speed-Aware-Join-the-Shortest-Queue (SA-JSQ)} scheme, has been shown to achieve
asymptotic optimality in the fluid-scaling regime where the number of servers
$n$ tends to infinity but the normalised the arrival rate of jobs remains
constant. {In this paper, we show that the SA-JSQ scheme is also asymptotically
optimal for heterogeneous systems in the {\em Halfin-Whitt} traffic regime
where the normalised arrival rate scales as $1-O(1/\sqrt{n})$.} Our analysis
begins by establishing that an appropriately scaled and centered version of the
Markov process describing system dynamics weakly converges to a two-dimensional
reflected {\em Ornstein-Uhlenbeck (OU) process}. We then show using {\em
Stein's method} that the stationary distribution of the underlying Markov
process converges to that of the OU process as the system size increases by
establishing the validity of interchange of limits. {Finally, through coupling
with a suitably constructed system, we show that SA-JSQ asymptotically
minimises the diffusion-scaled total number of jobs and the diffusion-scaled
number of waiting jobs in the steady-state in the Halfin-Whitt regime among all
policies which dispatch jobs based on queue lengths and server speeds.}
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separations in query complexity for total search problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shalev Ben-David, Srijita Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the query complexity analogue of the class TFNP of total search
problems. We give a way to convert partial functions to total search problems
under certain settings; we also give a way to convert search problems back into
partial functions.
  As an application, we give new separations for degree-like measures. We give
an exponential separation between quantum query complexity and approximate
degree for a total search problem. We also give an exponential separation
between approximate degree and the positive quantum adversary for a total
search problem.
  We then strengthen the former separation to upper bound a larger measure: the
two-sided approximate non-negative degree, also called the conical junta
degree. This measure is often larger than quantum query complexity and even a
separation from randomized query complexity was not known. We extend our
results to communication complexity, and obtain an exponential separation
between quantum information complexity and the relaxed partition bound for a
total search problem. Even a weaker separation between randomized communication
complexity and the relaxed partition bound was not known for total search
problems (or even for partial functions).
  Most of our separations for total search problems can be converted to
separations for partial functions. Using this, we reprove the recent
exponential separation between quantum query complexity and approximate degree
for a partial function by Ambainis and Belovs (2023), among other new results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integer Polynomial Factorization by Recombination of Real Factors:
  Re-evaluating an Old Technique in Modern Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Iravanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polynomial factorization over $ZZ$ is of great historical and practical
importance. Currently, the standard technique is to factor the polynomial over
finite fields first and then to lift to integers. Factorization over finite
fields can be done in polynomial time using Berlekamp or Cantor-Zassenhaus
algorithms. Lifting from the finite field to $ZZ$ requires a combinatorial
algorithm. The van Hoeij algorithm casts the combinatorial problem as a
knapsack-equivalent problem, which is then solved using lattice-reduction (the
LLL algorithm) in polynomial time, which is implemented in many computer
algebra systems (CAS).
  In this paper, we revisit the old idea of starting with factorization over
$RR$ instead of a finite field, followed by recombination of the resulting
linear and quadratic factors. We transform the problem into an integer subset
sum problem, which is then solved using the Horowizt-Sinha algorithm. This
algorithm can factor a random integer polynomial of degree $d$ in a time
complexity of $O(2^(d slash 4))$.
  While the resulting algorithm is exponential, consistent with the integer
subset sum problem being in NP, it has a few advantages. First, it is simple
and easy to implement. Second, it is almost embarrassingly parallelizable. We
demonstrate this by implementing the algorithm in a Graphic Processing Unit
(GPU). The resulting code can factor a degree 100 polynomial is a few tenths of
a second, comparable to some standard CAS. This shows that it is possible to
use current hardware, especially massively parallel systems like GPU, to the
benefit of symbolic algebra.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning junta <span class="highlight-title">distribution</span>s and quantum junta states, and QAC$^0$
  circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Escudero-Gutiérrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we consider the problems of learning junta distributions, their
quantum counter-part, quantum junta states, and QAC$^0$ circuits, which we show
to be juntas.
  $\mathbf{Junta\ distributions.\ }$A probability distribution $p:\{-1,1\}^n\to
\mathbb [0,1]$ is a $k$-junta if it only depends on $k$ variables. We show that
they can be learned with to error $\varepsilon$ in total variation distance
from $O(2^k\log(n)/\varepsilon^2)$ samples, which quadratically improves the
upper bound of Aliakbarpour et al. (COLT'16) and matches their lower bound in
every parameter.
  $\mathbf{Junta\ states.\ }$We initiate the study of $n$-qubit states that are
$k$-juntas, those that are the tensor product of a $k$-qubit state and an
$(n-k)$-qubit maximally mixed state. We show that these states can be learned
with error $\varepsilon$ in trace distance with
$O(12^{k}\log(n)/\varepsilon^2)$ single copies. We also prove a lower bound of
$\Omega((4^k+\log (n))/\varepsilon^2)$ copies. Along the way, we give a new
proof of the optimal performance of Classical Shadows based on Pauli analysis.
  $\mathbf{QAC^0\ circuits.\ }$Nadimpalli et al. (STOC'24) recently showed that
the Pauli spectrum of QAC$^0$ circuits (with not too many auxiliary qubits) is
concentrated on low-degree. We remark that they showed something stronger,
namely that the Choi states of those circuits are close to be juntas. As a
consequence, we show that $n$-qubit QAC$^0$ circuits with size $s$, depth $d$
and $a$ auxiliary qubits can be learned from $2^{O(\log(s^22^a)^d)}\log (n)$
copies of the Choi state, improving the $n^{O(\log(s^22^a)^d)}$ by Nadimpalli
et al. In addition, we use this remark to improve on the lower bounds against
QAC$^0$ circuits to compute the address function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A parametric version of the Hilbert Nullstellensatz 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13027v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13027v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rida Ait El Manssour, Nikhil Balaji, Klara Nosan, Mahsa Shirmohammadi, James Worrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hilbert's Nullstellensatz is a fundamental result in algebraic geometry that
gives a necessary and sufficient condition for a finite collection of
multivariate polynomials to have a common zero in an algebraically closed
field. Associated with this result, there is the computational problem HN of
determining whether a system of polynomials with coefficients in the field of
rational numbers has a common zero over the field of algebraic numbers.
  In an influential paper, Koiran showed that HN can be determined in the
polynomial hierarchy assuming the Generalised Riemann Hypothesis (GRH). More
precisely, he showed that HN lies in the complexity class AM under GRH. In a
later work he generalised this result by showing that the problem DIM, which
asks to determine the dimension of the set of solutions of a given polynomial
system, also lies in AM subject to GRH.
  In this paper we study the solvability of polynomial equations over arbitrary
algebraically closed fields of characteristic zero. Up to isomorphism, every
such field is the algebraic closure of a field of rational functions. We thus
formulate a parametric version of HN, called HNP, in which the input is a
system of polynomials with coefficients in a function field
$\mathbb{Q}(\mathbf{x})$ and the task is to determine whether the polynomials
have a common zero in the algebraic closure
$\overline{\mathbb{Q}(\mathbf{x})}$.
  We observe that Koiran's proof that DIM lies in AM can be interpreted as a
randomised polynomial-time reduction of DIM to HNP, followed by an argument
that HNP lies in AM. Our main contribution is a self-contained proof that HNP
lies in AM that follows the same basic idea as Koiran's argument -- namely
random instantiation of the parameters -- but whose justification is purely
algebraic, relying on a parametric version of Hilbert's Nullstellensatz, and
avoiding recourse to semi-algebraic geometry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $Π_{2}^{P}$ vs PSpace Dichotomy for the Quantified Constraint
  Satisfaction Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitriy Zhuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Quantified Constraint Satisfaction Problem is the problem of evaluating a
sentence with both quantifiers, over relations from some constraint language,
with conjunction as the only connective. We show that for any constraint
language on a finite domain the Quantified Constraint Satisfaction Problem is
either in $\Pi_{2}^{P}$, or PSpace-complete. Additionally, we build a
constraint language on a 6-element domain such that the Quantified Constraint
Satisfaction Problem over this language is $\Pi_{2}^{P}$-complete.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some misprints were fixed, acknowledgements were added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A simplified proof of the CSP Dichotomy Conjecture and XY-symmetric
  operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitriy Zhuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a new theory of strong subalgebras and linear congruences that are
defined globally. Using this theory we provide a new proof of the correctness
of Zhuk's algorithm for all tractable CSPs on a finite domain, and therefore a
new simplified proof of the CSP Dichotomy Conjecture. Additionally, using the
new theory we prove that composing a weak near-unanimity operation of an odd
arity $n$ we can derive an $n$-ary operation that is symmetric on all
two-element sets. Thus, CSP over a constraint language $\Gamma$ on a finite
domain is tractable if and only if there exist infinitely many polymorphisms of
$\Gamma$ that are symmetric on all two-element sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A few misprints were fixed and acknowledgements were added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast interpolation and multiplication of unbalanced polynomials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Giorgi, Bruno Grenet, Armelle Perret du Cray, Daniel S. Roche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the classical problems of interpolating a polynomial given a
black box for evaluation, and of multiplying two polynomials, in the setting
where the bit-lengths of the coefficients may vary widely, so-called unbalanced
polynomials. Writing s for the total bit-length and D for the degree, our new
algorithms have expected running time $\tilde{O}(s \log D)$, whereas previous
methods for (resp.) dense or sparse arithmetic have at least $\tilde{O}(sD)$ or
$\tilde{O}(s^2)$ bit complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-degree learning and the metric entropy of polynomials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.09659v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.09659v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Eskenazis, Paata Ivanisvili, Lauritz Streck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Let $\mathscr{F}_{n,d}$ be the class of all functions $f:\{-1,1\}^n\to[-1,1]$
on the $n$-dimensional discrete hypercube of degree at most $d$. In the first
part of this paper, we prove that any (deterministic or randomized) algorithm
which learns $\mathscr{F}_{n,d}$ with $L_2$-accuracy $\varepsilon$ requires at
least $\Omega((1-\sqrt{\varepsilon})2^d\log n)$ queries for large enough $n$,
thus establishing the sharpness as $n\to\infty$ of a recent upper bound of
Eskenazis and Ivanisvili (2021). To do this, we show that the $L_2$-packing
numbers $\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon)$ of the
concept class $\mathscr{F}_{n,d}$ satisfy the two-sided estimate
$$c(1-\varepsilon)2^d\log n \leq \log
\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon) \leq \frac{2^{Cd}\log
n}{\varepsilon^4}$$ for large enough $n$, where $c, C>0$ are universal
constants. In the second part of the paper, we present a logarithmic upper
bound for the randomized query complexity of classes of bounded approximate
polynomials whose Fourier spectra are concentrated on few subsets. As an
application, we prove new estimates for the number of random queries required
to learn approximate juntas of a given degree, functions with rapidly decaying
Fourier tails and constant depth circuits of given size. Finally, we obtain
bounds for the number of queries required to learn the polynomial class
$\mathscr{F}_{n,d}$ without error in the query and random example models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version of Discrete Analysis 2023:17 with typographical error
  in Corollary 7 corrected</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics of Sets of Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwoo Kim, Shaan Nagy, Thomas Reps, Loris D'Antoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications like program synthesis sometimes require proving that a property
holds for all of the infinitely many programs described by a grammar - i.e., an
inductively defined set of programs. Current verification frameworks
overapproximate programs' behavior when sets of programs contain loops,
including two Hoare-style logics that fail to be relatively complete when loops
are allowed. In this work, we prove that compositionally verifying simple
properties for infinite sets of programs requires tracking distinct program
behaviors over unboundedly many executions. Tracking this information is both
necessary and sufficient for verification. We prove this fact in a general,
reusable theory of denotational semantics that can model the expressivity and
compositionality of verification techniques over infinite sets of programs. We
construct the minimal compositional semantics that captures simple properties
of sets of programs and use it to derive the first sound and relatively
complete Hoare-style logic for infinite sets of programs. Thus, our methods can
be used to design minimally complex, compositional verification techniques for
sets of programs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 8 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formalising CXL Cache Coherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengsong Tan, Alastair F. Donaldson, John Wickerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report our experience formally modelling and verifying CXL.cache, the
inter-device cache coherence protocol of the Compute Express Link standard. We
have used the Isabelle proof assistant to create a formal model for CXL.cache
based on the prose English specification. This led to us identifying and
proposing fixes to several problems we identified as unclear, ambiguous or
inaccurate, some of which could lead to incoherence if left unfixed. Nearly all
our issues and proposed fixes have been confirmed and tentatively accepted by
the CXL consortium for adoption, save for one which is still under discussion.
To validate the faithfulness of our model we performed scenario verification of
essential restrictions such as "Snoop-pushes-GO", and produced a fully
mechanised proof of a coherence property of the model. The considerable size of
this proof, comprising tens of thousands of lemmas, prompted us to develop new
proof automation tools, which we have made available for other Isabelle users
working with similarly cumbersome proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streamlining Cloud-Native Application Development and Deployment with
  Robust Encapsulation <span class="chip">SoCC '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawissanutt Lertpongrujikorn, Hai Duc Nguyen, Mohsen Amini Salehi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Serverless abstractions (e.g., FaaS) poorly support non-functional
requirements (e.g., QoS and constraints), are provider-dependent, and are
incompatible with other cloud abstractions (e.g., databases). As a result,
application developers have to undergo numerous rounds of development and
manual deployment refinements to finally achieve their desired quality and
efficiency. In this paper, we present Object-as-a-Service (OaaS) -- a novel
serverless paradigm that borrows the object-oriented programming concepts to
encapsulate business logic, data, and non-functional requirements into a single
deployment package, thereby streamlining provider-agnostic cloud-native
application development. We also propose a declarative interface for the
non-functional requirements of applications that relieves developers from
daunting refinements to meet their desired QoS and deployment constraint
targets. We realized the OaaS paradigm through a platform called Oparaca and
evaluated it against various real-world applications and scenarios. The
evaluation results demonstrate that Oparaca can enhance application performance
by 60X and improve reliability by 50X through latency, throughput, and
availability enforcement -- all with remarkably less development and deployment
time and effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM Symposium of Cloud Computing (SoCC '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Beyond the Phase Ordering Problem: Finding the Globally Optimal Code
  w.r.t. Optimization Phases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03120v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03120v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author">Yu Wang</span>, Hongyu Chen, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phase ordering problem has been a long-standing challenge in compiler
optimizations. Over the past four decades, a significant amount of effort has
been devoted, and indeed, substantial progress has been made. However, in this
paper, we raise questions about the overall significance of solving the phase
ordering problem in the first place, as pursuing a solution to this problem may
not align with the fundamental goal of compiler optimizations, i.e., generating
the globally optimal code among all programs that compilers deem semantically
equivalent to an input program.
  Our findings, supported by both theoretical and empirical evidence, show that
solving the phase ordering problem is not equivalent to generating such
globally optimal code. The fundamental reason that applying the optimal phase
ordering may still result in suboptimal code is the exclusion of programs of
less efficiency during the optimization process. Motivated by this insight, we
propose a theoretical approach, called \textit{infinitive iterative
bi-directional optimizations} (\textit{IIBO}), which is guaranteed to converge
to the globally optimal code for any input program. We realize IIBO into a
practical algorithm and apply it to optimize real-world programs. Results show
that IIBO frequently generates more efficient code than GCC/LLVM, two
state-of-the-art industry compilers, as well as exhaustive search, which can be
deemed the solution to the phasing ordering problem.% input programs.
  Given the significance and impact of our results, we are currently in active
discussions with LLVM engineers on the possible incorporation of our findings
into their next release. In general, we expect our work to inspire new design
principles for compiler development in the pursuit of generating the globally
optimal code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisQ: A Markov Decision Process Based Language for Quantum Distributed
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Chang, Saitej Yavvari, Rance Cleaveland, Samik Basu, Liyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of quantum computers has reached a great milestone, in spite
of restrictions on important quantum resources: the number of qubits being
entangled at a single-location quantum computer. Recently, there has been some
work to combine single-location quantum computing and quantum networking
techniques to develop distributed quantum systems such that large entangled
qubit groups can be established through remote processors, and quantum
algorithms can be executed distributively. We present DisQ as a framework to
facilitate the rewrites of quantum algorithms to their distributed versions.
The core of DisQ is a distributed quantum programming language that combines
the concepts of Chemical Abstract Machine (CHAM) and Markov Decision Processes
(MDP) with the objective of providing a clearly distinguishing quantum
concurrent and distributed behaviors. Based on the DisQ language, we develop a
simulation relation for verifying the equivalence of a quantum algorithm and
its distributed versions. We present several case studies, such as quantum
addition and Shor's algorithm, to demonstrate their equivalent rewrites to
distributed versions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 2</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operation Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transparent and Efficient Live Migration across <span class="highlight-title">Heterogeneous</span> Hosts with
  Wharf 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Yang, Aibo Hu, Yusheng Zheng, Brian Zhao, Xinqi Zhang, Andrew Quinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live migration allows a user to move a running application from one machine
(a source) to another (a destination) without restarting it. The technique has
proven useful for diverse tasks including load balancing, managing system
updates, improving data locality, and improving system resilience.
Unfortunately, current live migration solutions fail to meet today's computing
needs. First, most techniques do not support heterogeneous source and
destination hosts, as they require the two machines to have the same
instruction set architecture (ISA) or use the same operating system (OS), which
hampers numerous live migration usecases. Second, many techniques are not
transparent, as they require that applications be written in a specific
high-level language or call specific library functions, which imposes barriers
to entry for many users. We present a new lightweight abstraction, called a
vessel, that supports transparent heterogeneous live migration. A vessel
maintains a machine-independent encoding of a process's state, using
WebAssembly abstractions, allowing it to be executed on nearly-arbitrary ISAs.
A vessel virtualizes all of its OS state, using the WebAssembly System
Interface (WASI), allowing it to execute on nearly arbitrary OS. We introduce
docks and software systems that execute and migrate vessels. Docks face two key
challenges: First, maintaining a machine-independent encoding at all points in
a process is extremely expensive. So, docks instead ensure that a vessel is
guaranteed to eventually reach a machine-independent point and delay the
initiation of vessel migration until the vessel reaches such a point. Second, a
dock may receive a vessel migration that originates from a dock executing on a
different OS.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Formal Languages and Automata Theory <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-TL: Low-Resource Temporal Knowledge Representation of Planning
  Instructions Using Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Manas, Stefan Zwicklbauer, Adrian Paschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents often face the challenge of interpreting uncertain natural
language instructions for planning tasks. Representing these instructions as
Linear Temporal Logic (LTL) enables planners to synthesize actionable plans. We
introduce CoT-TL, a data-efficient in-context learning framework for
translating natural language specifications into LTL representations. CoT-TL
addresses the limitations of large language models, which typically rely on
extensive fine-tuning data, by extending chain-of-thought reasoning and
semantic roles to align with the requirements of formal logic creation. This
approach enhances the transparency and rationale behind LTL generation,
fostering user trust. CoT-TL achieves state-of-the-art accuracy across three
diverse datasets in low-data scenarios, outperforming existing methods without
fine-tuning or intermediate translations. To improve reliability and minimize
hallucinations, we incorporate model checking to validate the syntax of the
generated LTL output. We further demonstrate CoT-TL's effectiveness through
ablation studies and evaluations on unseen LTL structures and formulas in a new
dataset. Finally, we validate CoT-TL's practicality by integrating it into a
QuadCopter for multi-step drone planning based on natural language
instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Proceedings of the 2024 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2024), Abu
  Dhabi 14-18 October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tokenization as Finite-State Transduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Cognetta, Naoaki Okazaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is the first step in modern neural language model pipelines
where an input text is converted to a sequence of subword tokens. We introduce
from first principles a finite-state transduction framework which can
efficiently encode all possible tokenizations of a regular language. We then
constructively show that Byte-Pair Encoding (BPE) and MaxMatch (WordPiece), two
popular tokenization schemes, fit within this framework. For BPE, this is
particularly surprising given its resemblance to context-free grammar and the
fact that it does not tokenize strings from left to right.
  An application of this is to guided generation, where the outputs of a
language model are constrained to match some pattern. Here, patterns are
encoded at the character level, which creates a mismatch between the
constraints and the model's subword vocabulary. While past work has focused
only on constraining outputs without regard to the underlying tokenization
algorithm, our framework allows for simultaneously constraining the model
outputs to match a specified pattern while also adhering to the underlying
tokenizer's canonical tokenization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + 5 pages in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian scaling laws for in-context learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a powerful technique for getting language models
to perform complex tasks with no training updates. Prior work has established
strong correlations between the number of in-context examples provided and the
accuracy of the model's predictions. In this paper, we seek to explain this
correlation by showing that ICL approximates a Bayesian learner. This
perspective gives rise to a family of novel Bayesian scaling laws for ICL. In
experiments with \mbox{GPT-2} models of different sizes, our scaling laws
exceed or match existing scaling laws in accuracy while also offering
interpretable terms for task priors, learning efficiency, and per-example
probabilities. To illustrate the analytic power that such interpretable scaling
laws provide, we report on controlled synthetic dataset experiments designed to
inform real-world studies of safety alignment. In our experimental protocol, we
use SFT to suppress an unwanted existing model capability and then use ICL to
try to bring that capability back (many-shot jailbreaking). We then experiment
on real-world instruction-tuned LLMs using capabilities benchmarks as well as a
new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws
accurately predict the conditions under which ICL will cause the suppressed
behavior to reemerge, which sheds light on the ineffectiveness of post-training
at increasing LLM safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages main text, 26 pages total</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Invariants and Home Spaces in Transition Systems and Petri Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07623v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07623v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gerard Memmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This lecture note focuses on comparing the notions of invariance and home
spaces in Transition Systems and more particularly, in Petri Nets. We also
describe how linear algebra relates to these basic notions in Computer Science,
how it can be used for extracting invariant properties from a parallel system
described by a Labeled Transition System in general and a Petri Net in
particular. We endeavor to regroup a number of algebraic results dispersed
throughout the Petri Nets literature with the addition of new results around
the notions of semiflows and generating sets. \newline Several extensive
examples are given to illustrate how the notion of invariants and home spaces
can be methodically utilized through basic arithmetic and algebra to prove
behavioral properties of a Petri Net. Some additional thoughts on invariants
and home spaces will conclude this note.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>108 pages with some news results, figures, and examples</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetries of Dependency Quantified Boolean Formulas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemens Hofstadler, Manuel Kauers, Martina Seidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetries have been exploited successfully within the realms of SAT and QBF
to improve solver performance in practical applications and to devise more
powerful proof systems. As a first step towards extending these advancements to
the class of dependency quantified Boolean formulas (DQBFs), which generalize
QBF by allowing more nuanced variable dependencies, this work develops a
comprehensive theory to characterize symmetries for DQBFs. We also introduce
the notion of symmetry breakers of DQBFs, along with a concrete construction,
and discuss how to detect DQBF symmetries algorithmically using a graph-based
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Arithmetical Hierarchy: A Realizability-Theoretic Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takayuki Kihara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we investigate the arithmetical hierarchy from the
perspective of realizability theory. An experimental observation in classical
computability theory is that the notion of degrees of unsolvability for natural
arithmetical decision problems only plays a role in counting the number of
quantifiers, jumps, or mind-changes. In contrast, we reveal that when the
realizability interpretation is combined with many-one reducibility, it becomes
possible to classify natural arithmetical problems in a very nontrivial way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solovay reducibility via translation functions on rationals and on reals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Titov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solovay reducibility $\redsolovay$ was introduced by Robert M. Solovay in
1975 via translation functions on rationals. In 2022, its totalized version
$\redsolovaytotal$ (i.e., Solovay reducibility via a total function on
rationals) has been examined by Merkle and Titov (arXiv:2407.14869).
  In 2020, Kumabe, Miyabe, Mizusawa and Suzuki (arXiv:1903.08625) have
discovered that Solovay reducibility can be characterized on left-c.e.\ reals
using the notion of a translation function on reals. In 2024, Kumabe, Miyabe,
and Suzuki (DOI: 10.3233/COM-230486) have introduced a new reducibility
$\redclopen$ on all reals, that uses the notion of a translation function on
reals, and its totalized version $\redcllocal$. %They have also shown that
$\redcllocal$ implies $\redclopen$, wherein the converse is not true even for
left-c.e. reals.
  In this work, we show that $\redsolovayreal$ implies $\redclopen$ and
$\redsolovaytotal$ implies $\redcllocal$ on all reals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A draft for a further publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pantograph: A Machine-to-Machine Interaction Interface for Advanced
  Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leni Aniva, Chuyue Sun, Brando Miranda, Clark Barrett, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-assisted theorem proving refers to the process of conducting
structured reasoning to automatically generate proofs for mathematical
theorems. Recently, there has been a surge of interest in using machine
learning models in conjunction with proof assistants to perform this task. In
this paper, we introduce Pantograph, a tool that provides a versatile interface
to the Lean 4 proof assistant and enables efficient proof search via powerful
search algorithms such as Monte Carlo Tree Search. In addition, Pantograph
enables high-level reasoning by enabling a more robust handling of Lean 4's
inference steps. We provide an overview of Pantograph's architecture and
features. We also report on an illustrative use case: using machine learning
models and proof sketches to prove Lean 4 theorems. Pantograph's innovative
features pave the way for more advanced machine learning models to perform
complex proof searches and high-level reasoning, equipping future researchers
to design more versatile and powerful theorem provers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $Π_{2}^{P}$ vs PSpace Dichotomy for the Quantified Constraint
  Satisfaction Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitriy Zhuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Quantified Constraint Satisfaction Problem is the problem of evaluating a
sentence with both quantifiers, over relations from some constraint language,
with conjunction as the only connective. We show that for any constraint
language on a finite domain the Quantified Constraint Satisfaction Problem is
either in $\Pi_{2}^{P}$, or PSpace-complete. Additionally, we build a
constraint language on a 6-element domain such that the Quantified Constraint
Satisfaction Problem over this language is $\Pi_{2}^{P}$-complete.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some misprints were fixed, acknowledgements were added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A simplified proof of the CSP Dichotomy Conjecture and XY-symmetric
  operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitriy Zhuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a new theory of strong subalgebras and linear congruences that are
defined globally. Using this theory we provide a new proof of the correctness
of Zhuk's algorithm for all tractable CSPs on a finite domain, and therefore a
new simplified proof of the CSP Dichotomy Conjecture. Additionally, using the
new theory we prove that composing a weak near-unanimity operation of an odd
arity $n$ we can derive an $n$-ary operation that is symmetric on all
two-element sets. Thus, CSP over a constraint language $\Gamma$ on a finite
domain is tractable if and only if there exist infinitely many polymorphisms of
$\Gamma$ that are symmetric on all two-element sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A few misprints were fixed and acknowledgements were added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On The Axioms Of $\mathcal{M},\mathcal{N}$-Adhesive Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Castelnovo, Marino Miculan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adhesive and quasiadhesive categories provide a general framework for the
study of algebraic graph rewriting systems. In a quasiadhesive category any two
regular subobjects have a join which is again a regular subobject. Vice versa,
if regular monos are adhesive, then the existence of a regular join for any
pair of regular subobjects entails quasiadhesivity. It is also known
(quasi)adhesive categories can be embedded in a Grothendieck topos via a
functor preserving pullbacks and pushouts along (regular) monomorphisms. In
this paper we extend these results to $\mathcal{M}, \mathcal{N}$-adhesive
categories, a concept recently introduced to generalize the notion of
(quasi)adhesivity. We introduce the notion of $\mathcal{N}$-adhesive morphism,
which allows us to express $\mathcal{M}, \mathcal{N}$-adhesivity as a condition
on the subobjects's posets. Moreover, $\mathcal{N}$-adhesive morphisms allows
us to show how an $\mathcal{M},\mathcal{N}$-adhesive category can be embedded
into a Grothendieck topos, preserving pullbacks and $\mathcal{M},
\mathcal{N}$-pushouts.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-20T00:00:00Z">2024-10-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Performance Profiling <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Event Joining in Practice With Kafka and Flink 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srijan Saket, Vivek Chandela, Md. Danish Kalim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historically, machine learning training pipelines have predominantly relied
on batch training models, retraining models every few hours. However,
industrial practitioners have proved that real-time training can lead to a more
adaptive and personalized user experience. The transition from batch to
real-time is full of tradeoffs to get the benefits of accuracy and freshness
while keeping the costs low and having a predictable, maintainable system.
  Our work characterizes migrating to a streaming pipeline for a machine
learning model using Apache Kafka and Flink. We demonstrate how to transition
from Google Pub/Sub to Kafka to handle incoming real-time events and leverage
Flink for streaming joins using RocksDB and checkpointing. We also address
challenges such as managing causal dependencies between events, balancing event
time versus processing time, and ensuring exactly-once versus at-least-once
delivery guarantees, among other issues. Furthermore, we showcase how we
improved scalability by using topic partitioning in Kafka, reduced event
throughput by \textbf{85\%} through the use of Avro schema and compression,
decreased costs by \textbf{40\%}, and implemented a separate pipeline to ensure
data correctness. Our findings provide valuable insights into the tradeoffs and
complexities of real-time systems, enabling better-informed decisions tailored
to specific requirements for building effective streaming systems that enhance
user satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPIC: Efficient Position-Independent Context Caching for Serving Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are critical for a wide range of applications,
but serving them efficiently becomes increasingly challenging as inputs become
more complex. Context caching improves serving performance by exploiting
inter-request dependency and reusing key-value (KV) cache across requests, thus
improving time-to-first-token (TTFT). However, existing prefix-based context
caching requires exact token prefix matches, limiting cache reuse in few-shot
learning, multi-document QA, or retrieval-augmented generation, where prefixes
may vary. In this paper, we present EPIC, an LLM serving system that introduces
position-independent context caching (PIC), enabling modular KV cache reuse
regardless of token chunk position (or prefix). EPIC features two key designs:
AttnLink, which leverages static attention sparsity to minimize recomputation
for accuracy recovery, and KVSplit, a customizable chunking method that
preserves semantic coherence. Our experiments demonstrate that Epic delivers up
to 8x improvements in TTFT and 7x throughput over existing systems, with
negligible or no accuracy loss. By addressing the limitations of traditional
caching approaches, Epic enables more scalable and efficient LLM inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QOPS: A <span class="highlight-title">Compiler</span> Framework for Quantum Circuit Simulation Acceleration
  with Profile Guided Optimizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Tsung Wu, Po-Hsuan Huang, Kai-Chieh Chang, Chia-Heng Tu, Shih-Hao Hung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum circuit simulation is important in the evolution of quantum software
and hardware. Novel algorithms can be developed and evaluated by performing
quantum circuit simulations on classical computers before physical quantum
computers are available. Unfortunately, compared with a physical quantum
computer, a prolonged simulation time hampers the rapid development of quantum
algorithms. Inspired by the feedback-directed optimization scheme used by
classical compilers to improve the generated code, this work proposes a quantum
compiler framework QOPS to enable profile-guided optimization (PGO) for quantum
circuit simulation acceleration. The QOPS compiler instruments a quantum
simulator to collect performance data during the circuit simulation and it then
generates the optimized version of the quantum circuit based on the collected
data. Experimental results show the PGO can effectively shorten the simulation
time on our tested benchmark programs. Especially, the simulator-specific PGO
(virtual swap) can be applied to the benchmarks to accelerate the simulation
speed by a factor of 1.19. As for the hardware-independent PGO, compared with
the brute force mechanism (turning on all available compilation flags), which
achieves 21% performance improvement against the non-optimized version, the PGO
can achieve 16% speedup with a factor of 63 less compilation time than the
brute force approach.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operation Systems <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for Dynamic Memory Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arisrei Lim, Abhiram Maddukuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, reinforcement learning (RL) has gained popularity and has
been applied to a wide range of tasks. One such popular domain where RL has
been effective is resource management problems in systems. We look to extend
work on RL for resource management problems by considering the novel domain of
dynamic memory allocation management. We consider dynamic memory allocation to
be a suitable domain for RL since current algorithms like first-fit, best-fit,
and worst-fit can fail to adapt to changing conditions and can lead to
fragmentation and suboptimal efficiency. In this paper, we present a framework
in which an RL agent continuously learns from interactions with the system to
improve memory management tactics. We evaluate our approach through various
experiments using high-level and low-level action spaces and examine different
memory allocation patterns. Our results show that RL can successfully train
agents that can match and surpass traditional allocation strategies,
particularly in environments characterized by adversarial request patterns. We
also explore the potential of history-aware policies that leverage previous
allocation requests to enhance the allocator's ability to handle complex
request patterns. Overall, we find that RL offers a promising avenue for
developing more adaptive and efficient memory allocation strategies,
potentially overcoming limitations of hardcoded allocation algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tide: A Split OS Architecture for Control Plane Offloading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Tigar Humphries, Neel Natu, Kostis Kaffes, Stanko Novaković, Paul Turner, Hank Levy, David Culler, Christos Kozyrakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The end of Moore's Law is driving cloud providers to offload virtualization
and the network data plane to SmartNICs to improve compute efficiency. Even
though individual OS control plane tasks consume up to 5% of cycles across the
fleet, they remain on the host CPU because they are tightly intertwined with OS
mechanisms. Moreover, offloading puts the slow PCIe interconnect in the
critical path of OS decisions.
  We propose Tide, a new split OS architecture that separates OS control plane
policies from mechanisms and offloads the control plane policies onto a
SmartNIC. Tide has a new host-SmartNIC communication API, state synchronization
mechanism, and communication mechanisms that overcome the PCIe bottleneck, even
for $\mu$s-scale workloads. Tide frees up host compute for applications and
unlocks new optimization opportunities, including machine learning-driven
policies, scheduling on the network I/O path, and reducing on-host
interference. We demonstrate that Tide enables OS control planes that are
competitive with on-host performance for the most difficult $\mu$s-scale
workloads. Tide outperforms on-host control planes for memory management
(saving 16 host cores), Stubby network RPCs (saving 8 cores), and GCE virtual
machine management (11.2% performance improvement).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>About 11 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Classical Subspaces, The No Synchronization Law, and More 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18201v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18201v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Epstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper looks at the intersection of algorithmic information theory and
physics, namely quantum mechanics, thermodynamics, and black holes. We discuss
theorems which characterize the barrier between the quantum world and the
classical realm. The notion of a ``semi-classical subspace'' is introduced.
Partial signals and partial information cloning can be obtained on quantum
states in semi-classical subspaces. The No Synchronization Law is detailed,
which says separate and isolated physical systems evolving over time cannot
have algorithmic thermodynamic entropies that are in synch. We look at future
work involving the Kolmogorov complexity of black holes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2402.13049</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Asymmetry Yields Faster Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Alman, Ran Duan, Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, Renfei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new improvement on the laser method for designing fast matrix
multiplication algorithms. The new method further develops the recent advances
by [Duan, Wu, Zhou FOCS 2023] and [Vassilevska Williams, Xu, Xu, Zhou SODA
2024]. Surprisingly the new improvement is achieved by incorporating more
asymmetry in the analysis, circumventing a fundamental tool of prior work that
requires two of the three dimensions to be treated identically. The method
yields a new bound on the square matrix multiplication exponent
$$\omega<2.371339,$$ improved from the previous bound of $\omega<2.371552$. We
also improve the bounds of the exponents for multiplying rectangular matrices
of various shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, in SODA 2025. arXiv admin note: text overlap with
  arXiv:2307.07970</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Formal Verification of a Highly-Configurable Register
  Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhang Zhang, Bryan Olmos, Basavaraj Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Registers in IP blocks of an SoC perform a variety of functions, most of
which are essential to the SoC operation. The complexity of register
implementation is relatively low when compared with other design blocks.
However, the extensive number of registers, combined with the various potential
functions they can perform, necessitates considerable effort during
implementation, especially when using a manual approach. Therefore, an in-house
register generator was proposed by the design team to reduce the manual effort
in the register implementation. This in-house register generator supports not
only the generation of register blocks but also bus-related blocks. Meanwhile,
to support various requirements, 41 generation options are used for this
generator, which is highly-configurable. From the verification perspective, it
is infeasible to achieve complete verification results with a manual approach
for all options combinations. Besides the complexity caused by configurability,
the register verification is still time-consuming due to two widely recognized
issues: the unreliability of specifications and the complexity arising from
diverse access policies. To deal with the highly-configurable feature and both
register verification issues, we propose an automated register verification
framework using formal methods following the Model Driven Architecture (MDA).
Based on our results, the human effort in the register verification can be
reduced significantly, from 20Person-Day (20PD) to 3PD for each configuration,
and 100\% code coverage can be achieved. During the project execution, eleven
new design bugs were found with the proposed verification framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in DVCon US 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLC Intra-set Write Balancing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Krishna, Ayush Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of Non-Volatile Memory (NVM) in computer architecture has
brought about new challenges, one of which is the write endurance problem.
Frequent writes to a particular cache cell in NVM can lead to degradation of
the memory cell and reduce its lifespan. To solve this problem, we propose a
sample-based blocking technique for the Last Level Cache (LLC). Our approach
involves defining a threshold value and sampling a subset of cache sets. If the
number of writes to a way in a sampled set exceeds the threshold, the way is
blocked, and writes are redirected to other ways. We also maintain a history
structure to record the number of writes in a set and a PC-Table to use for
blocking in unsampled sets. Based on blocking on sampled sets, variance of
values stored in history is used to determine whether blocking had a positive
impact or not, and on this basis, value corresponding to instruction pointer is
incremented or decremented. This value is later used for blocking in unsampled
sets. Our results show that our approach significantly balances write traffic
to the cache and improves the overall lifespan of the memory cells while having
better performance to the base-line system. Our approach can also be applied to
other cache hierarchies and NVM technologies to mitigate the problem of write
endurance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fastrack: Fast IO for Secure ML using GPU TEEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqin Wang, Rachit Rajat, Jonghyun Lee, Tingting Tang, Murali Annavaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As cloud-based ML expands, ensuring data security during training and
inference is critical. GPU-based Trusted Execution Environments (TEEs) offer
secure, high-performance solutions, with CPU TEEs managing data movement and
GPU TEEs handling authentication and computation. However, CPU-to-GPU
communication overheads significantly hinder performance, as data must be
encrypted, authenticated, decrypted, and verified, increasing costs by 12.69 to
33.53 times. This results in GPU TEE inference becoming 54.12% to 903.9% slower
and training 10% to 455% slower than non-TEE systems, undermining GPU TEE
advantages in latency-sensitive applications.
  This paper analyzes Nvidia H100 TEE protocols and identifies three key
overheads: 1) redundant CPU re-encryption, 2) limited authentication
parallelism, and 3) unnecessary operation serialization. We propose Fastrack,
optimizing with 1) direct GPU TEE communication, 2) parallelized
authentication, and 3) overlapping decryption with PCI-e transmission. These
optimizations cut communication costs and reduce inference/training runtime by
up to 84.6%, with minimal overhead compared to non-TEE systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDP4Bit: Toward 4-bit Communication Quantization in Sharded Data
  Parallelism for <span class="highlight-title">LLM</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinda Jia, Cong Xie, Hanlin Lu, Daoce Wang, Hao Feng, Chengming Zhang, Baixi Sun, Haibin Lin, Zhi Zhang, Xin Liu, Dingwen Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a clear trend towards language models with an
ever-increasing number of parameters, as well as the growing training overhead
and memory usage. Distributed training, particularly through Sharded Data
Parallelism (ShardedDP) which partitions optimizer states among workers, has
emerged as a crucial technique to mitigate training time and memory usage. Yet,
a major challenge in the scalability of ShardedDP is the intensive
communication of weights and gradients. While compression techniques can
alleviate this issue, they often result in worse accuracy. Driven by this
limitation, we propose SDP4Bit (Toward 4Bit Communication Quantization in
Sharded Data Parallelism for LLM Training), which effectively reduces the
communication of weights and gradients to nearly 4 bits via two novel
techniques: quantization on weight differences, and two-level gradient smooth
quantization. Furthermore, SDP4Bit presents an algorithm-system co-design with
runtime optimization to minimize the computation overhead of compression. In
addition to the theoretical guarantees of convergence, we empirically evaluate
the accuracy of SDP4Bit on the pre-training of GPT models with up to 6.7
billion parameters, and the results demonstrate a negligible impact on training
loss. Furthermore, speed experiments show that SDP4Bit achieves up to
4.08$\times$ speedup in end-to-end throughput on a scale of 128 GPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRA: A Method of Federated MultI-Task Learning for LaRge LAnguage
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Elbakary, Chaouki Ben Issaid, Tamer ElBatt, Karim Seddik, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a method for fine-tuning Large Language Models
(LLMs), inspired by Multi-Task learning in a federated manner. Our approach
leverages the structure of each client's model and enables a learning scheme
that considers other clients' tasks and data distribution. To mitigate the
extensive computational and communication overhead often associated with LLMs,
we utilize a parameter-efficient fine-tuning method, specifically Low-Rank
Adaptation (LoRA), reducing the number of trainable parameters. Experimental
results, with different datasets and models, demonstrate the proposed method's
effectiveness compared to existing frameworks for federated fine-tuning of LLMs
in terms of average and local performances. The proposed scheme outperforms
existing baselines by achieving lower local loss for each client while
maintaining comparable global performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian data fusion for distributed learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wu, Tales Imbiriba, Pau Closas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the main challenges of federated learning (FL) is handling
non-independent and identically distributed (non-IID) client data, which may
occur in practice due to unbalanced datasets and use of different data sources
across clients. Knowledge sharing and model personalization are key strategies
for addressing this issue. Clustered federated learning is a class of FL
methods that groups clients that observe similarly distributed data into
clusters, such that every client is typically associated with one data
distribution and participates in training a model for that distribution along
their cluster peers. In this paper, we present a unified Bayesian framework for
clustered FL which associates clients to clusters. Then we propose several
practical algorithms to handle the, otherwise growing, data associations in a
way that trades off performance and computational complexity. This work
provides insights on client-cluster associations and enables client knowledge
sharing in new ways. The proposed framework circumvents the need for unique
client-cluster associations, which is seen to increase the performance of the
resulting models in a variety of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heuristic-based Dynamic Leiden Algorithm for Efficient Tracking of
  Communities on Evolving Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection, or clustering, identifies groups of nodes in a graph
that are more densely connected to each other than to the rest of the network.
Given the size and dynamic nature of real-world graphs, efficient community
detection is crucial for tracking evolving communities, enhancing our
understanding and management of complex systems. The Leiden algorithm, which
improves upon the Louvain algorithm, efficiently detects communities in large
networks, producing high-quality structures. However, existing multicore
dynamic community detection algorithms based on Leiden are inefficient and lack
support for tracking evolving communities. This technical report introduces the
first implementations of parallel Naive-dynamic (ND), Delta-screening (DS), and
Dynamic Frontier (DF) Leiden algorithms that efficiently track communities over
time. Experiments on a 64-core AMD EPYC-7742 processor demonstrate that ND, DS,
and DF Leiden achieve average speedups of 3.9x, 4.4x, and 6.1x, respectively,
on large graphs with random batch updates compared to the Static Leiden
algorithm, and these approaches scale at 1.4 - 1.5x for every thread doubling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:2405.11658</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPIC: Efficient Position-Independent Context Caching for Serving Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are critical for a wide range of applications,
but serving them efficiently becomes increasingly challenging as inputs become
more complex. Context caching improves serving performance by exploiting
inter-request dependency and reusing key-value (KV) cache across requests, thus
improving time-to-first-token (TTFT). However, existing prefix-based context
caching requires exact token prefix matches, limiting cache reuse in few-shot
learning, multi-document QA, or retrieval-augmented generation, where prefixes
may vary. In this paper, we present EPIC, an LLM serving system that introduces
position-independent context caching (PIC), enabling modular KV cache reuse
regardless of token chunk position (or prefix). EPIC features two key designs:
AttnLink, which leverages static attention sparsity to minimize recomputation
for accuracy recovery, and KVSplit, a customizable chunking method that
preserves semantic coherence. Our experiments demonstrate that Epic delivers up
to 8x improvements in TTFT and 7x throughput over existing systems, with
negligible or no accuracy loss. By addressing the limitations of traditional
caching approaches, Epic enables more scalable and efficient LLM inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edge AI: A Taxonomy, Systematic <span class="highlight-title">Review</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukhpal Singh Gill, Muhammed Golec, Jianmin Hu, Minxian Xu, Junhui Du, Huaming Wu, Guneet Kaur Walia, Subramaniam Subramanian Murugesan, Babar Ali, Mohit Kumar, Kejiang Ye, Prabal Verma, Surendra Kumar, Felix Cuadrado, Steve Uhlig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge Artificial Intelligence (AI) incorporates a network of interconnected
systems and devices that receive, cache, process, and analyze data in close
communication with the location where the data is captured with AI technology.
Recent advancements in AI efficiency, the widespread use of Internet of Things
(IoT) devices, and the emergence of edge computing have unlocked the enormous
scope of Edge AI. Edge AI aims to optimize data processing efficiency and
velocity while ensuring data confidentiality and integrity. Despite being a
relatively new field of research from 2014 to the present, it has shown
significant and rapid development over the last five years. This article
presents a systematic literature review for Edge AI to discuss the existing
research, recent advancements, and future research directions. We created a
collaborative edge AI learning system for cloud and edge computing analysis,
including an in-depth study of the architectures that facilitate this
mechanism. The taxonomy for Edge AI facilitates the classification and
configuration of Edge AI systems while examining its potential influence across
many fields through compassing infrastructure, cloud computing, fog computing,
services, use cases, ML and deep learning, and resource management. This study
highlights the significance of Edge AI in processing real-time data at the edge
of the network. Additionally, it emphasizes the research challenges encountered
by Edge AI systems, including constraints on resources, vulnerabilities to
security threats, and problems with scalability. Finally, this study highlights
the potential future research directions that aim to address the current
limitations of Edge AI by providing innovative solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Version Accepted for Publication in Springer Cluster
  Computing, 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formalization of Differential Privacy in Isabelle/HOL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tetsuya Sato, Yasuhiko Minamide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy is a statistical definition of privacy that has
attracted the interest of both academia and industry. Its formulations are easy
to understand, but the differential privacy of databases is complicated to
determine. One of the reasons for this is that small changes in database
programs can break their differential privacy. Therefore, formal verification
of differential privacy has been studied for over a decade.
  In this paper, we propose an Isabelle/HOL library for formalizing
differential privacy in a general setting. To our knowledge, it is the first
formalization of differential privacy that supports continuous probability
distributions. First, we formalize the standard definition of differential
privacy and its basic properties. Second, we formalize the Laplace mechanism
and its differential privacy. Finally, we formalize the differential privacy of
the report noisy max mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insights from the Usage of the Ansible Lightspeed Code Completion
  Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis Mandel, Luca Buratti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of Large Language Models (LLMs) which can generate code, has
made it possible to create tools that improve developer productivity.
Integrated development environments or IDEs which developers use to write
software are often used as an interface to interact with LLMs. Although many
such tools have been released, almost all of them focus on general-purpose
programming languages. Domain-specific languages, such as those crucial for
Information Technology (IT) automation, have not received much attention.
Ansible is one such YAML-based IT automation-specific language. Ansible
Lightspeed is an LLM-based service designed explicitly to generate Ansible
YAML, given natural language prompt.
  In this paper, we present the design and implementation of the Ansible
Lightspeed service. We then evaluate its utility to developers using diverse
indicators, including extended utilization, analysis of user edited
suggestions, as well as user sentiments analysis. The evaluation is based on
data collected for 10,696 real users including 3,910 returning users. The code
for Ansible Lightspeed service and the analysis framework is made available for
others to use.
  To our knowledge, our study is the first to involve thousands of users of
code assistants for domain-specific languages. We are also the first code
completion tool to present N-Day user retention figures, which is 13.66\% on
Day 30. We propose an improved version of user acceptance rate, called Strong
Acceptance rate, where a suggestion is considered accepted only if less than
$50\%$ of it is edited and these edits do not change critical parts of the
suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong
acceptance rate of 49.08\% for multi-line Ansible task suggestions. With our
findings we provide insights into the effectiveness of small, dedicated models
in a domain-specific context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published at the 39th IEEE/ACM International
  Conference on Automated Software Engineering (ASE 2024), Industry Showcase
  under the title "Ansible Lightspeed: A Code Generation Service for IT
  Automation"</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Formal Languages and Automata Theory <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Formal Verification of a Highly-Configurable Register
  Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhang Zhang, Bryan Olmos, Basavaraj Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Registers in IP blocks of an SoC perform a variety of functions, most of
which are essential to the SoC operation. The complexity of register
implementation is relatively low when compared with other design blocks.
However, the extensive number of registers, combined with the various potential
functions they can perform, necessitates considerable effort during
implementation, especially when using a manual approach. Therefore, an in-house
register generator was proposed by the design team to reduce the manual effort
in the register implementation. This in-house register generator supports not
only the generation of register blocks but also bus-related blocks. Meanwhile,
to support various requirements, 41 generation options are used for this
generator, which is highly-configurable. From the verification perspective, it
is infeasible to achieve complete verification results with a manual approach
for all options combinations. Besides the complexity caused by configurability,
the register verification is still time-consuming due to two widely recognized
issues: the unreliability of specifications and the complexity arising from
diverse access policies. To deal with the highly-configurable feature and both
register verification issues, we propose an automated register verification
framework using formal methods following the Model Driven Architecture (MDA).
Based on our results, the human effort in the register verification can be
reduced significantly, from 20Person-Day (20PD) to 3PD for each configuration,
and 100\% code coverage can be achieved. During the project execution, eleven
new design bugs were found with the proposed verification framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in DVCon US 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedLogic-AQA: Enhancing Medical Question Answering with Abstractive
  Models Focusing on Logical Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aizan Zafar, Kshitij Mishra, Asif Ekbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Medical question-answering (QA) tasks, the need for effective systems is
pivotal in delivering accurate responses to intricate medical queries. However,
existing approaches often struggle to grasp the intricate logical structures
and relationships inherent in medical contexts, thus limiting their capacity to
furnish precise and nuanced answers. In this work, we address this gap by
proposing a novel Abstractive QA system MedLogic-AQA that harnesses First Order
Logic (FOL) based rules extracted from both context and questions to generate
well-grounded answers. Through initial experimentation, we identified six
pertinent first-order logical rules, which were then used to train a
Logic-Understanding (LU) model capable of generating logical triples for a
given context, question, and answer. These logic triples are then integrated
into the training of MedLogic-AQA, enabling effective and coherent reasoning
during answer generation. This distinctive fusion of logical reasoning with
abstractive QA equips our system to produce answers that are logically sound,
relevant, and engaging. Evaluation with respect to both automated and
human-based demonstrates the robustness of MedLogic-AQA against strong
baselines. Through empirical assessments and case studies, we validate the
efficacy of MedLogic-AQA in elevating the quality and comprehensiveness of
answers in terms of reasoning as well as informativeness
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polynomial Lawvere Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Bacci, Radu Mardare, Prakash Panangaden, Gordon Plotkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Polynomial Lawvere logic PL, a logic defined over the Lawvere
quantale of extended positive reals with sum as tensor, to which we add
multiplication, thereby obtaining a semiring structure. PL is designed for
complex quantitative reasoning, allowing judgements that express inequalities
between polynomials on the extended positive reals. We introduce a deduction
system and demonstrate its expressiveness by deriving a classical result from
probability theory relating the Kantorovich and the total variation distances.
Although the deductive system is not complete in general, we achieve
completeness for finitely axiomatizable theories. The proof of completeness
relies on the Krivine-Stengle Positivstellensatz (a variant of Hilbert's
Nullstellensatz). Additionally, we provide new complexity results, both for PL
and its affine fragment AL, regarding two decision problems: satisfiability of
a set of judgements and semantical consequence from a set of judgements. The
former is NP-complete in AL and in PSPACE for PL; the latter is co-NP complete
in PL and in PSPACE for PL.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-19T00:00:00Z">2024-10-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLARE: Faithful Logic-Aided Reasoning and Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Question Answering (QA) and Reasoning approaches based on Large
Language Models (LLMs) commonly use prompting techniques, such as
Chain-of-Thought (CoT), assuming the resulting generation will have a more
granular exploration and reasoning over the question space and scope. However,
such methods struggle with generating outputs that are faithful to the
intermediate chain of reasoning produced by the model. On the other end of the
spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to
combine LLMs with external symbolic solvers. While such approaches boast a high
degree of faithfulness, they usually require a model trained for code
generation and struggle with tasks that are ambiguous or hard to formalise
strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided
$\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel
interpretable approach for traversing the problem space using task
decompositions. We use the LLM to plan a solution, soft-formalise the query
into facts and predicates using a logic programming code and simulate that code
execution using an exhaustive multi-hop search over the defined space. Our
method allows us to compute the faithfulness of the reasoning process w.r.t.
the generated code and analyse the steps of the multi-hop search without
relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$
out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model
faithfulness positively correlates with overall performance and further
demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors
sufficient for and leading to the correct answer with optimal reasoning during
the multi-hop search.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pipeline Gradient-based Model Training on Analog In-memory Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxian Wu, Quan Xiao, Tayfun Gokmen, Hsinyu Tsai, Kaoutar El Maghraoui, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to accelerate the training of large deep neural models (DNN) in an
energy-efficient way, an analog in-memory computing (AIMC) accelerator emerges
as a solution with immense potential. In AIMC accelerators, trainable weights
are kept in memory without the need to move from memory to processors during
the training, reducing a bunch of overhead. However, although the in-memory
feature enables efficient computation, it also constrains the use of data
parallelism since copying weights from one AIMC to another is expensive. To
enable parallel training using AIMC, we propose synchronous and asynchronous
pipeline parallelism for AIMC accelerators inspired by the pipeline in digital
domains. This paper provides a theoretical convergence guarantee for both
synchronous and asynchronous pipelines in terms of both sampling and clock
cycle complexity, which is non-trivial since the physical characteristic of
AIMC accelerators leads to analog updates that suffer from asymmetric bias. The
simulations of training DNN on real datasets verify the efficiency of pipeline
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System <span class="chip">ASPLOS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minseok Seo, Xuan Truong Nguyen, Seok Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, Jongsoon Won, Haerang Choi, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, Sangheon Lee, Yongseok Choi, Wooseok Byun, Seungcheol Baek, Hyuk-Jae Lee, John Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating end-to-end inference of transformer-based large language models
(LLMs) is a critical component of AI services in datacenters. However, diverse
compute characteristics of end-to-end LLM inference present challenges as
previously proposed accelerators only address certain operations or stages
(e.g., self-attention, generation stage, etc.). To address the unique
challenges of accelerating end-to-end inference, we propose IANUS -- Integrated
Accelerator based on NPU-PIM Unified Memory System. IANUS is a domain-specific
system architecture that combines a Neural Processing Unit (NPU) with a
Processing-in-Memory (PIM) to leverage both the NPU's high computation
throughput and the PIM's high effective memory bandwidth. In particular, IANUS
employs a unified main memory system where the PIM memory is used both for PIM
operations and for NPU's main memory. The unified main memory system ensures
that memory capacity is efficiently utilized and the movement of shared data
between NPU and PIM is minimized. However, it introduces new challenges since
normal memory accesses and PIM computations cannot be performed simultaneously.
Thus, we propose novel PIM Access Scheduling that manages normal memory
accesses and PIM computations through workload mapping and scheduling across
the PIM and the NPU. Our detailed simulation evaluations show that IANUS
improves the performance of GPT-2 by 6.2$\times$ and 3.2$\times$, on average,
compared to the NVIDIA A100 GPU and the state-of-the-art accelerator. As a
proof-of-concept, we develop a prototype of IANUS with a commercial PIM, NPU,
and an FPGA-based PIM controller to demonstrate the feasibility of IANUS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version of the paper accepted to ASPLOS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Sunk Carbon Fallacy: Rethinking Carbon Footprint Metrics for
  Effective Carbon-Aware Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noman Bashir, Varun Gohil, Anagha Belavadi, Mohammad Shahrad, David Irwin, Elsa Olivetti, Christina Delimitrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in computing demand and its corresponding energy
consumption have focused attention on computing's impact on the climate and
sustainability. Prior work proposes metrics that quantify computing's carbon
footprint across several lifecycle phases, including its supply chain,
operation, and end-of-life. Industry uses these metrics to optimize the carbon
footprint of manufacturing hardware and running computing applications.
Unfortunately, prior work on optimizing datacenters' carbon footprint often
succumbs to the \emph{sunk cost fallacy} by considering embodied carbon
emissions (a sunk cost) when making operational decisions (i.e., job scheduling
and placement), which leads to operational decisions that do not always reduce
the total carbon footprint.
  In this paper, we evaluate carbon-aware job scheduling and placement on a
given set of servers for a number of carbon accounting metrics. Our analysis
reveals state-of-the-art carbon accounting metrics that include embodied carbon
emissions when making operational decisions can actually increase the total
carbon footprint of executing a set of jobs. We study the factors that affect
the added carbon cost of such suboptimal decision-making. We then use a
real-world case study from a datacenter to demonstrate how the sunk carbon
fallacy manifests itself in practice. Finally, we discuss the implications of
our findings in better guiding effective carbon-aware scheduling in on-premise
and cloud datacenters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safer Heuristics With XPlain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pantea Karimi, Solal Pirelli, Siva Kesava Reddy Kakarla, Ryan Beckett, Santiago Segarra, Beibin Li, Pooria Namyar, Behnaz Arzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many problems that cloud operators solve are computationally expensive, and
operators often use heuristic algorithms (that are faster and scale better than
optimal) to solve them more efficiently. Heuristic analyzers enable operators
to find when and by how much their heuristics underperform. However, these
tools do not provide enough detail for operators to mitigate the heuristic's
impact in practice: they only discover a single input instance that causes the
heuristic to underperform (and not the full set), and they do not explain why.
  We propose XPlain, a tool that extends these analyzers and helps operators
understand when and why their heuristics underperform. We present promising
initial results that show such an extension is viable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Weather Forecasting: A CNN-LSTM Hybrid Model for
  Predicting Historical Temperature Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Gong, Yuchen Zhang, Fei Wang, Chi-Han Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As global climate change intensifies, accurate weather forecasting has become
increasingly important, affecting agriculture, energy management, environmental
protection, and daily life. This study introduces a hybrid model combining
Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks
to predict historical temperature data. CNNs are utilized for spatial feature
extraction, while LSTMs handle temporal dependencies, resulting in
significantly improved prediction accuracy and stability. By using Mean
Absolute Error (MAE) as the loss function, the model demonstrates excellent
performance in processing complex meteorological data, addressing challenges
such as missing data and high-dimensionality. The results show a strong
alignment between the prediction curve and test data, validating the model's
potential in climate prediction. This study offers valuable insights for fields
such as agriculture, energy management, and urban planning, and lays the
groundwork for future applications in weather forecasting under the context of
global climate change.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fast AI Surrogate for Coastal Ocean Circulation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelin Xu, Jie Ren, Yupu Zhang, Jose Maria Gonzalez Ondina, Maitane Olabarrieta, Tingsong Xiao, Wenchong He, Zibo Liu, Shigang Chen, Kaleb Smith, Zhe Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nearly 900 million people live in low-lying coastal zones around the world
and bear the brunt of impacts from more frequent and severe hurricanes and
storm surges. Oceanographers simulate ocean current circulation along the
coasts to develop early warning systems that save lives and prevent loss and
damage to property from coastal hazards. Traditionally, such simulations are
conducted using coastal ocean circulation models such as the Regional Ocean
Modeling System (ROMS), which usually runs on an HPC cluster with multiple CPU
cores. However, the process is time-consuming and energy expensive. While
coarse-grained ROMS simulations offer faster alternatives, they sacrifice
detail and accuracy, particularly in complex coastal environments. Recent
advances in deep learning and GPU architecture have enabled the development of
faster AI (neural network) surrogates. This paper introduces an AI surrogate
based on a 4D Swin Transformer to simulate coastal tidal wave propagation in an
estuary for both hindcast and forecast (up to 12 days). Our approach not only
accelerates simulations but also incorporates a physics-based constraint to
detect and correct inaccurate results, ensuring reliability while minimizing
manual intervention. We develop a fully GPU-accelerated workflow, optimizing
the model training and inference pipeline on NVIDIA DGX-2 A100 GPUs. Our
experiments demonstrate that our AI surrogate reduces the time cost of 12-day
forecasting of traditional ROMS simulations from 9,908 seconds (on 512 CPU
cores) to 22 seconds (on one A100 GPU), achieving over 450$\times$ speedup
while maintaining high-quality simulation results. This work contributes to
oceanographic modeling by offering a fast, accurate, and physically consistent
alternative to traditional simulation models, particularly for real-time
forecasting in rapid disaster response.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Workflows Community Summit 2024: Future Trends and Challenges in
  Scientific Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Ferreira da Silva, Deborah Bard, Kyle Chard, Shaun de Witt, Ian T. Foster, Tom Gibbs, Carole Goble, William Godoy, Johan Gustafsson, Utz-Uwe Haus, Stephen Hudson, Shantenu Jha, Laila Los, Drew Paine, Frédéric Suter, Logan Ward, Sean Wilkinson, Marcos Amaris, Yadu Babuji, Jonathan Bader, Riccardo Balin, Daniel Balouek, Sarah Beecroft, Khalid Belhajjame, Rajat Bhattarai, Wes Brewer, Paul Brunk, Silvina Caino-Lores, Henri Casanova, Daniela Cassol, Jared Coleman, Taina Coleman, Iacopo Colonnelli, Anderson Andrei Da Silva, Daniel de Oliveira, Pascal Elahi, Nour Elfaramawy, Wael Elwasif, Brian Etz, Thomas Fahringer, Wesley Ferreira, Rosa Filgueira, Jacob Fosso Tande, Luiz Gadelha, Andy Gallo, Daniel Garijo, Yiannis Georgiou, Philipp Gritsch, Patricia Grubel, Amal Gueroudji, Quentin Guilloteau, Carlo Hamalainen, Rolando Hong Enriquez, Lauren Huet, Kevin Hunter Kesling, Paula Iborra, Shiva Jahangiri, Jan Janssen, Joe Jordan, Sehrish Kanwal, Liliane Kunstmann, Fabian Lehmann, Ulf Leser, Chen Li, Peini Liu, Jakob Luettgau, Richard Lupat, Jose M. Fernandez, Ketan Maheshwari, Tanu Malik, Jack Marquez, Motohiko Matsuda, Doriana Medic, Somayeh Mohammadi, Alberto Mulone, John-Luke Navarro, Kin Wai Ng, Klaus Noelp, Bruno P. Kinoshita, Ryan Prout, Michael R. Crusoe, Sashko Ristov, Stefan Robila, Daniel Rosendo, Billy Rowell, Jedrzej Rybicki, Hector Sanchez, Nishant Saurabh, Sumit Kumar Saurav, Tom Scogland, Dinindu Senanayake, Woong Shin, Raul Sirvent, Tyler Skluzacek, Barry Sly-Delgado, Stian Soiland-Reyes, Abel Souza, Renan Souza, Domenico Talia, Nathan Tallent, Lauritz Thamsen, Mikhail Titov, Benjamin Tovar, Karan Vahi, Eric Vardar-Irrgang, Edite Vartina, Yuandou Wang, Merridee Wouters, Qi Yu, Ziad Al Bkhetan, Mahnoor Zulfiqar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Workflows Community Summit gathered 111 participants from 18 countries to
discuss emerging trends and challenges in scientific workflows, focusing on six
key areas: time-sensitive workflows, AI-HPC convergence, multi-facility
workflows, heterogeneous HPC environments, user experience, and FAIR
computational workflows. The integration of AI and exascale computing has
revolutionized scientific workflows, enabling higher-fidelity models and
complex, time-sensitive processes, while introducing challenges in managing
heterogeneous environments and multi-facility data dependencies. The rise of
large language models is driving computational demands to zettaflop scales,
necessitating modular, adaptable systems and cloud-service models to optimize
resource utilization and ensure reproducibility. Multi-facility workflows
present challenges in data movement, curation, and overcoming institutional
silos, while diverse hardware architectures require integrating workflow
considerations into early system design and developing standardized resource
management tools. The summit emphasized improving user experience in workflow
systems and ensuring FAIR workflows to enhance collaboration and accelerate
scientific discovery. Key recommendations include developing standardized
metrics for time-sensitive workflows, creating frameworks for cloud-HPC
integration, implementing distributed-by-design workflow modeling, establishing
multi-facility authentication protocols, and accelerating AI integration in HPC
workflow management. The summit also called for comprehensive workflow
benchmarks, workflow-specific UX principles, and a FAIR workflow maturity
model, highlighting the need for continued collaboration in addressing the
complex challenges posed by the convergence of AI, HPC, and multi-facility
research environments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">LLM</span> Support for Generating IEC 61131-3 Graphic Language
  Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Zhang, Mario de Sousa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities demonstrated by Large Language Models (LLMs) inspire
researchers to integrate them into industrial production and automation. In the
field of Programmable Logic Controller (PLC) programming, previous researchers
have focused on using LLMs to generate Structured Text (ST) language, and
created automatic programming workflows based on it. The IEC 61131 graphic
programming languages, which still has the most users, have however been
overlooked.
  In this paper we explore using LLMs to generate graphic languages in ASCII
art to provide assistance to engineers. Our series of experiments indicate
that, contrary to what researchers usually think, it is possible to generate a
correct Sequential Function Chart (SFC) for simple requirements when LLM is
provided with several examples. On the other hand, generating a Ladder Diagram
(LD) automatically remains a challenge even for very simple use cases. The
automatic conversion between LD and SFC without extra information also fails
when using prompt engineering alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HPVM-HDC: A <span class="highlight-title">Heterogeneous</span> Programming System for Hyperdimensional
  Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Russel Arbore, Xavier Routh, Abdul Rafae Noor, Akash Kothari, Haichao Yang, Weihong Xu, Sumukh Pinge, Minxuan Zhou, Vikram Adve, Tajana Rosing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperdimensional Computing (HDC), a technique inspired by cognitive models of
computation, has garnered significant interest in recent years. For example,
HDC has been proposed as a more efficient and robust alternative basis for
machine learning. The highly parallel nature of HDC algorithms makes them
well-suited for execution on several hardware architectures, including CPUs,
GPUs, FPGAs, ASIC-based and Resistive RAM-based accelerators. Traditionally,
these diverse architectures are programmed using different languages and
programming models, making heterogeneous programming for HDC prohibitively
difficult. To make matters worse, currently no compiler framework that enables
heterogeneous compilation of HDC programs and generates efficient code for a
wide variety of hardware targets exists. We propose an end-to-end heterogeneous
programming system for HDC: a novel programming language, HDC++, that enables
programmers to write programs using a unified programming model, including a
set of high-level, HDC-specific, abstractions to ease programmability; and a
heterogeneous compilation framework, HPVM-HDC, that provides an intermediate
representation that reflects the parallel character of HDC algorithms and
enables compilation of HDC++ programs to a wide array of hardware targets,
including a custom HD Digital ASIC and an HD Resistive RAM accelerator.
HPVM-HDC can perform HD specific optimizations, which we demonstrate by
implementing two domain specific optimizations. Our evaluation shows that
HPVM-HDC generates performance competitive code, compared with baseline HD
applications. Additionally, HPVM-HDC efficiently targets an HD Digital ASIC and
an HD ReRAM accelerator simulator, achieving a geomean 1.28x and 2.15x speed-up
over our compiled GPU implementations, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Distribution</span> Semantics for Probabilistic Term Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germán Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic programming is becoming increasingly popular thanks to its
ability to specify problems with a certain degree of uncertainty. In this work,
we focus on term rewriting, a well-known computational formalism. In
particular, we consider systems that combine traditional rewriting rules with
probabilities. Then, we define a distribution semantics for such systems that
can be used to model the probability of reducing a term to some value. We also
show how to compute a set of "explanations" for a given reduction, which can be
used to compute its probability. Finally, we illustrate our approach with
several examples and outline a couple of extensions that may prove useful to
improve the expressive power of probabilistic rewrite systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance Profiling <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Sunk Carbon Fallacy: Rethinking Carbon Footprint Metrics for
  Effective Carbon-Aware Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noman Bashir, Varun Gohil, Anagha Belavadi, Mohammad Shahrad, David Irwin, Elsa Olivetti, Christina Delimitrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in computing demand and its corresponding energy
consumption have focused attention on computing's impact on the climate and
sustainability. Prior work proposes metrics that quantify computing's carbon
footprint across several lifecycle phases, including its supply chain,
operation, and end-of-life. Industry uses these metrics to optimize the carbon
footprint of manufacturing hardware and running computing applications.
Unfortunately, prior work on optimizing datacenters' carbon footprint often
succumbs to the \emph{sunk cost fallacy} by considering embodied carbon
emissions (a sunk cost) when making operational decisions (i.e., job scheduling
and placement), which leads to operational decisions that do not always reduce
the total carbon footprint.
  In this paper, we evaluate carbon-aware job scheduling and placement on a
given set of servers for a number of carbon accounting metrics. Our analysis
reveals state-of-the-art carbon accounting metrics that include embodied carbon
emissions when making operational decisions can actually increase the total
carbon footprint of executing a set of jobs. We study the factors that affect
the added carbon cost of such suboptimal decision-making. We then use a
real-world case study from a datacenter to demonstrate how the sunk carbon
fallacy manifests itself in practice. Finally, we discuss the implications of
our findings in better guiding effective carbon-aware scheduling in on-premise
and cloud datacenters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safer Heuristics With XPlain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pantea Karimi, Solal Pirelli, Siva Kesava Reddy Kakarla, Ryan Beckett, Santiago Segarra, Beibin Li, Pooria Namyar, Behnaz Arzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many problems that cloud operators solve are computationally expensive, and
operators often use heuristic algorithms (that are faster and scale better than
optimal) to solve them more efficiently. Heuristic analyzers enable operators
to find when and by how much their heuristics underperform. However, these
tools do not provide enough detail for operators to mitigate the heuristic's
impact in practice: they only discover a single input instance that causes the
heuristic to underperform (and not the full set), and they do not explain why.
  We propose XPlain, a tool that extends these analyzers and helps operators
understand when and why their heuristics underperform. We present promising
initial results that show such an extension is viable.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Computational Separation Between Quantum No-cloning and
  No-telegraphing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01858v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01858v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Nehoran, Mark Zhandry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two of the fundamental no-go theorems of quantum information are the
no-cloning theorem (that it is impossible to make copies of general quantum
states) and the no-teleportation theorem (the prohibition on telegraphing, or
sending quantum states over classical channels without pre-shared
entanglement). They are known to be equivalent, in the sense that a collection
of quantum states is telegraphable if and only if it is clonable.
  Our main result suggests that this is not the case when computational
efficiency is considered. We give a collection of quantum states and quantum
oracles relative to which these states are efficiently clonable but not
efficiently telegraphable. Given that the opposite scenario is impossible
(states that can be telegraphed can always trivially be cloned), this gives the
most complete quantum oracle separation possible between these two important
no-go properties.
  We additionally study the complexity class clonableQMA, a subset of QMA whose
witnesses are efficiently clonable. As a consequence of our main result, we
give a quantum oracle separation between clonableQMA and the class QCMA, whose
witnesses are restricted to classical strings. We also propose a candidate
oracle-free promise problem separating these classes. We finally demonstrate an
application of clonable-but-not-telegraphable states to cryptography, by
showing how such states can be used to protect against key exfiltration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages; Changed "No-teleportation" to "No-telegraphing" in the
  title & abstract, since this terminology is less ambiguous; added
  applications to complexity theory and cryptography, including the definition
  of $\mathsf{clonableQMA}$, a quantum oracle separation between
  $\mathsf{clonableQMA}$ and $\mathsf{QCMA}$, and a definition and construction
  of parallelizable but non-exfiltratable encryption</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-18T00:00:00Z">2024-10-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-diseases detection with memristive system on chip 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Daniel W. Yang, Zerui Liu, Evan Yan, Heming Sun, Ning Ge, Miao Hu, Wei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents the first implementation of multilayer neural networks on
a memristor/CMOS integrated system on chip (SoC) to simultaneously detect
multiple diseases. To overcome limitations in medical data, generative AI
techniques are used to enhance the dataset, improving the classifier's
robustness and diversity. The system achieves notable performance with low
latency, high accuracy (91.82%), and energy efficiency, facilitated by
end-to-end execution on a memristor-based SoC with ten 256x256 crossbar arrays
and an integrated on-chip processor. This research showcases the transformative
potential of memristive in-memory computing hardware in accelerating machine
learning applications for medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Arbitrary Precision Acceleration for Large Language Models on
  GPU Tensor Cores <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been widely applied but face challenges in
efficient inference. While quantization methods reduce computational demands,
ultra-low bit quantization with arbitrary precision is hindered by limited GPU
Tensor Core support and inefficient memory management, leading to suboptimal
acceleration. To address these challenges, we propose a comprehensive
acceleration scheme for arbitrary precision LLMs. At its core, we introduce a
novel bipolar-INT data format that facilitates parallel computing and supports
symmetric quantization, effectively reducing data redundancy. Building on this,
we implement an arbitrary precision matrix multiplication scheme that
decomposes and recovers matrices at the bit level, enabling flexible precision
while maximizing GPU Tensor Core utilization. Furthermore, we develop an
efficient matrix preprocessing method that optimizes data layout for subsequent
computations. Finally, we design a data recovery-oriented memory management
system that strategically utilizes fast shared memory, significantly enhancing
kernel execution speed and minimizing memory access latency. Experimental
results demonstrate our approach's effectiveness, with up to 2.4\times speedup
in matrix multiplication compared to NVIDIA's CUTLASS. When integrated into
LLMs, we achieve up to 6.7\times inference acceleration. These improvements
significantly enhance LLM inference efficiency, enabling broader and more
responsive applications of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ASP-DAC 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A subquadratic certification scheme for P5-free graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Bousquet, Sébastien Zeitoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In local certification, vertices of a $n$-vertex graph perform a local
verification to check if a given property is satisfied by the graph. This
verification is performed thanks to certificates, which are pieces of
information that are given to the vertices. In this work, we focus on the local
certification of $P_5$-freeness, and we prove a $O(n^{3/2})$ upper bound on the
size of the certificates, which is (to our knowledge) the first subquadratic
upper bound for this property.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient charge-preserving excited state preparation with variational
  quantum algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohim Chandani, Kazuki Ikeda, Zhong-Bo Kang, Dmitri E. Kharzeev, Alexander McCaskey, Andrea Palermo, C. R. Ramakrishnan, Pooja Rao, Ranjani G. Sundaram, Kwangmin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the spectrum and wave functions of excited states of a system is
crucial in quantum physics and chemistry. Low-depth quantum algorithms, such as
the Variational Quantum Eigensolver (VQE) and its variants, can be used to
determine the ground-state energy. However, current approaches to computing
excited states require numerous controlled unitaries, making the application of
the original Variational Quantum Deflation (VQD) algorithm to problems in
chemistry or physics suboptimal. In this study, we introduce a
charge-preserving VQD (CPVQD) algorithm, designed to incorporate symmetry and
the corresponding conserved charge into the VQD framework. This results in
dimension reduction, significantly enhancing the efficiency of excited-state
computations. We present benchmark results with GPU-accelerated simulations
using systems up to 24 qubits, showcasing applications in high-energy physics,
nuclear physics, and quantum chemistry. This work is performed on NERSC's
Perlmutter system using NVIDIA's open-source platform for accelerated quantum
supercomputing - CUDA-Q.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> TF-DDRL: A <span class="highlight-title">Transformer</span>-enhanced Distributed DRL Technique for Scheduling
  IoT Applications in Edge and Cloud Computing Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi<span class="highlight-author">yu Wang</span>, Mohammad Goudarzi, Rajkumar Buyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous increase of IoT applications, their effective scheduling
in edge and cloud computing has become a critical challenge. The inherent
dynamism and stochastic characteristics of edge and cloud computing, along with
IoT applications, necessitate solutions that are highly adaptive. Currently,
several centralized Deep Reinforcement Learning (DRL) techniques are adapted to
address the scheduling problem. However, they require a large amount of
experience and training time to reach a suitable solution. Moreover, many IoT
applications contain multiple interdependent tasks, imposing additional
constraints on the scheduling problem. To overcome these challenges, we propose
a Transformer-enhanced Distributed DRL scheduling technique, called TF-DDRL, to
adaptively schedule heterogeneous IoT applications. This technique follows the
Actor-Critic architecture, scales efficiently to multiple distributed servers,
and employs an off-policy correction method to stabilize the training process.
In addition, Prioritized Experience Replay (PER) and Transformer techniques are
introduced to reduce exploration costs and capture long-term dependencies for
faster convergence. Extensive results of practical experiments show that
TF-DDRL, compared to its counterparts, significantly reduces response time,
energy consumption, monetary cost, and weighted cost by up to 60%, 51%, 56%,
and 58%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TiMePReSt: Time and Memory Efficient Pipeline Parallel DNN Training with
  Removed Staleness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankita Dutta, Nabendu Chaki, Rajat K. De
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNN training is extremely time-consuming, necessitating efficient
multi-accelerator parallelization,where a single iteration of training is split
over the accelerators.Current approaches often use intra-batch
parallelization.Combining inter-batch pipeline parallelism with intra-batch
parallelism is a common approach to further improve parallel training
throughput.Here, we develop a system, called TiMePReSt, that adds both of them,
but in a way which helps to better overlap computation and communication within
a mini-batch, and limits the amount of communication.The traditional pipeline
parallel training maintains similar working principle as conventional
training.Thus, it suffers from high GPU memory usage during training to
maintain consistent version of weights in forward and backward passes of a
mini-batch.Here, it has been shown experimentally that violating consistency of
weight versions does not necessarily reduce prediction capability of a
parallely trained DNN.TiMePReSt helps to overcome GPU memory overhead and
achieve zero degree of staleness of weights, but not effecting prediction
capability.Existing techniques often become costly in terms of training
time.Thus, TiMePReSt introduces a variant of intra-batch parallelism that
parallelizes the forward pass of each mini-batch by decomposing it into smaller
micro-batches.Synchronization between backward and forward passes are performed
in a novel way reduce training time in TiMePReSt.The chances of occurring
multiple sequence problem and its relation with version difference have been
observed in TiMePReSt.A mathematical relationship between the number of
micro-batches and worker machines has been formulated.A mathematical expression
of version difference has also been devised so that the version difference for
different combination of these two can be computed mathematically without
preparing diagrams for all the combinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Writing of Nested Data in Columnar Formats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Hahnfeld, Jakob Blomer, Thorsten Kollegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Energy Physics (HEP) experiments, for example at the Large Hadron
Collider (LHC) at CERN, store data at exabyte scale in sets of files. They use
a binary columnar data format by the ROOT framework, that also transparently
compresses the data. In this format, cells are not necessarily atomic but they
may contain nested collections of variable size. The fact that row and block
sizes are not known upfront makes it challenging to implement efficient
parallel writing. In particular, the data cannot be organized in a regular grid
where it is possible to precompute indices and offsets for independent writing.
In this paper, we propose a scalable approach to efficient multithreaded
writing of nested data in columnar format into a single file. Our approach
removes the bottleneck of a single writer while staying fully compatible with
the compressed, columnar, variably row-sized data representation. We discuss
our design choices and the implementation of scalable parallel writing for
ROOT's RNTuple format. An evaluation of our approach shows perfect scalability
only limited by storage bandwidth for a synthetic benchmark. Finally we
evaluate the benefits for a real-world application of dataset skimming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review (when applicable) or any
  post-submission improvements or corrections. The Version of Record of this
  contribution is published in "Euro-Par 2024: Parallel Processing", and is
  available online at https://doi.org/10.1007/978-3-031-69766-1_2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal, Non-pipelined Reduce-scatter and Allreduce Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesper Larsson Träff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reduce-scatter collective operation in which $p$ processors in a network
of processors collectively reduce $p$ input vectors into a result vector that
is partitioned over the processors is important both in its own right and as
building block for other collective operations. We present a surprisingly
simple, but non-trivial algorithm for solving this problem optimally in
$\lceil\log_2 p\rceil$ communication rounds with each process sending,
receiving and reducing exactly $p-1$ blocks of vector elements. We combine this
with a similarly simple allgather algorithm to get a likewise optimal algorithm
for the allreduce collective operation where the result vector is replicated on
all processors. The communication pattern is a simple, $\lceil\log_2
p\rceil$-regular, circulant graph also used elsewhere. The algorithms assume
the binary reduction operator to be commutative and we discuss this assumption.
The algorithms can readily be implemented and used for the collective
operations MPI_Reduce_scatter_block, MPI_Reduce_scatter and MPI_Allreduce as
specified in the MPI standard. The communication pattern can likewise be used
for all-to-all communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Communication and Computation Efficient Fully First-order Method for
  Decentralized Bilevel Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Wen, Chengchang Liu, Ahmed Abdelmoniem, Yipeng Zhou, Yuedong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization, crucial for hyperparameter tuning, meta-learning and
reinforcement learning, remains less explored in the decentralized learning
paradigm, such as decentralized federated learning (DFL). Typically,
decentralized bilevel methods rely on both gradients and Hessian matrices to
approximate hypergradients of upper-level models. However, acquiring and
sharing the second-order oracle is compute and communication intensive. % and
sharing this information incurs heavy communication overhead. To overcome these
challenges, this paper introduces a fully first-order decentralized method for
decentralized Bilevel optimization, $\text{C}^2$DFB which is both compute- and
communicate-efficient. In $\text{C}^2$DFB, each learning node optimizes a
min-min-max problem to approximate hypergradient by exclusively using gradients
information. To reduce the traffic load at the inner-loop of solving the
lower-level problem, $\text{C}^2$DFB incorporates a lightweight communication
protocol for efficiently transmitting compressed residuals of local parameters.
% during the inner loops. Rigorous theoretical analysis ensures its convergence
% of the algorithm, indicating a first-order oracle calls of
$\tilde{\mathcal{O}}(\epsilon^{-4})$. Experiments on hyperparameter tuning and
hyper-representation tasks validate the superiority of $\text{C}^2$DFB across
various typologies and heterogeneous data distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slipstream: Ebb-and-Flow Consensus on a DAG with Fast Confirmation for
  UTXO Transactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Polyanskii, Sebastian Muller, Mayank Raikwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Slipstream, a Byzantine Fault Tolerance (BFT) protocol
where nodes concurrently propose blocks to be added to a Directed Acyclic Graph
(DAG) and aim to agree on block ordering. Slipstream offers two types of block
orderings: an optimistic ordering, which is live and secure in a sleepy model
under up to 50% Byzantine nodes, and a final ordering, which is a prefix of the
optimistic ordering and ensures safety and liveness in an eventual lock-step
synchronous model under up to 33% Byzantine nodes. Additionally, Slipstream
integrates a payment system that allows for fast UTXO transaction confirmation
independently of block ordering. Transactions are confirmed in three rounds
during synchrony, and unconfirmed double spends are resolved in a novel way
using the DAG structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 3 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Parameter Tuning for a Structure-Based Virtual Screening HPC
  Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Guindani, Davide Gadioli, Roberto Rocco, Danilo Ardagna, Gianluca Palermo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual screening applications are highly parameterized to optimize the
balance between quality and execution performance. While output quality is
critical, the entire screening process must be completed within a reasonable
time. In fact, a slight reduction in output accuracy may be acceptable when
dealing with large datasets. Finding the optimal quality-throughput trade-off
depends on the specific HPC system used and should be re-evaluated with each
new deployment or significant code update. This paper presents two parallel
autotuning techniques for constrained optimization in distributed
High-Performance Computing (HPC) environments. These techniques extend
sequential Bayesian Optimization (BO) with two parallel asynchronous
approaches, and they integrate predictions from Machine Learning (ML) models to
help comply with constraints. Our target application is LiGen, a real-world
virtual screening software for drug discovery. The proposed methods address two
relevant challenges: efficient exploration of the parameter space and
performance measurement using domain-specific metrics and procedures. We
conduct an experimental campaign comparing the two methods with a popular
state-of-the-art autotuner. Results show that our methods find configurations
that are, on average, up to 35-42% better than the ones found by the autotuner
and the default expert-picked LiGen configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the Journal of Parallel and Distributed Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DistRL: An Asynchronous Distributed Reinforcement Learning Framework for
  On-Device Control Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device control agents, especially on mobile devices, are responsible for
operating mobile devices to fulfill users' requests, enabling seamless and
intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)
into these agents enhances their ability to understand and execute complex
commands, thereby improving user experience. However, fine-tuning MLLMs for
on-device control presents significant challenges due to limited data
availability and inefficient online training processes. This paper introduces
DistRL, a novel framework designed to enhance the efficiency of online RL
fine-tuning for mobile device control agents. DistRL employs centralized
training and decentralized data acquisition to ensure efficient fine-tuning in
the context of dynamic online interactions. Additionally, the framework is
backed by our tailor-made RL algorithm, which effectively balances exploration
with the prioritized utilization of collected data to ensure stable and robust
training. Our experiments show that, on average, DistRL delivers a 3X
improvement in training efficiency and enables training data collection 2.4X
faster than the leading synchronous multi-machine methods. Notably, after
training, DistRL achieves a 20% relative improvement in success rate compared
to state-of-the-art methods on general Android tasks from an open benchmark,
significantly outperforming existing approaches while maintaining the same
training time. These results validate DistRL as a scalable and efficient
solution, offering substantial improvements in both training efficiency and
agent performance for real-world, in-the-wild device control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper and Appendix, 24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedECA: A Federated External Control Arm Method for Causal Inference
  with Time-To-Event Data in Distributed Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16984v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16984v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Taïeb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, Jérome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, Félix Balazard, Mathieu Andreux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  External control arms (ECA) can inform the early clinical development of
experimental drugs and provide efficacy evidence for regulatory approval.
However, the main challenge in implementing ECA lies in accessing real-world or
historical clinical trials data. Indeed, regulations protecting patients'
rights by strictly controlling data processing make pooling data from multiple
sources in a central server often difficult. To address these limitations, we
develop a new method, 'FedECA' that leverages federated learning (FL) to enable
inverse probability of treatment weighting (IPTW) for time-to-event outcomes on
separate cohorts without needing to pool data. To showcase the potential of
FedECA, we apply it in different settings of increasing complexity culminating
with a real-world use-case in which FedECA provides evidence for a differential
effect between two drugs that would have otherwise gone unnoticed. By sharing
our code, we hope FedECA will foster the creation of federated research
networks and thus accelerate drug development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available at: https://github.com/owkin/fedeca, bug in SMD
  computation present in v1 and v2 has been fixed, many experiments on real
  data have been added + fix in YODA experiments using imputed data instead of
  raw data as well as typos and affiliations fix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Liger Kernel: Efficient Triton Kernels for <span class="highlight-title">LLM</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) efficiently at scale presents a
formidable challenge, driven by their ever-increasing computational demands and
the need for enhanced performance. In this work, we introduce Liger-Kernel, an
open-sourced set of Triton kernels developed specifically for LLM training.
With kernel optimization techniques like kernel operation fusing and input
chunking, our kernels achieve on average a 20% increase in training throughput
and a 60% reduction in GPU memory usage for popular LLMs compared to
HuggingFace implementations. In addition, Liger-Kernel is designed with
modularity, accessibility, and adaptability in mind, catering to both casual
and expert users. Comprehensive benchmarks and integration tests are built in
to ensure compatibility, performance, correctness, and convergence across
diverse computing environments and model architectures.
  The source code is available under a permissive license at:
github.com/linkedin/Liger-Kernel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Latency and Fast Atomic Snapshot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Paulo Bezerra, Luciano Freitas, Petr Kuznetsov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The original goal of this paper was a novel, fast atomic-snapshot protocol
for asynchronous message-passing systems. In the process of defining what fast
means exactly, we faced a number of interesting issues that arise when
conventional time metrics are applied to asynchronous implementations. We
discovered some gaps in latency claims made in earlier work on snapshot
algorithms, which hampers their comparative time-complexity analysis. We then
came up with a new unifying time-complexity analysis that captures the latency
of an operation in an asynchronous, long-lived implementation, which allowed us
to formally grasp latency improvements of our solution with respect to the
state-of-the-art protocols: optimal latency in fault-free runs without
contention, short constant latency in fault-free runs with contention, the
worst-case latency proportional to the number of failures, and constant, close
to optimal amortized latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Failure Transparency in Stateful Dataflow Systems (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksey Veresov, Jonas Spenger, Paris Carbone, Philipp Haller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Failure transparency enables users to reason about distributed systems at a
higher level of abstraction, where complex failure-handling logic is hidden.
This is especially true for stateful dataflow systems, which are the backbone
of many cloud applications. In particular, this paper focuses on proving
failure transparency in Apache Flink, a popular stateful dataflow system. Even
though failure transparency is a critical aspect of Apache Flink, to date it
has not been formally proven. Showing that the failure transparency mechanism
is correct, however, is challenging due to the complexity of the mechanism
itself. Nevertheless, this complexity can be effectively hidden behind a
failure transparent programming interface. To show that Apache Flink is failure
transparent, we model it in small-step operational semantics. Next, we provide
a novel definition of failure transparency based on observational
explainability, a concept which relates executions according to their
observations. Finally, we provide a formal proof of failure transparency for
the implementation model; i.e., we prove that the failure-free model correctly
abstracts from the failure-related details of the implementation model. We also
show liveness of the implementation model under a fair execution assumption.
These results are a first step towards a verified stack for stateful dataflow
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, 44 pages including references and appendix with
  proofs, technical report, ECOOP 2024; updated with DOI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEdge: Benchmarking Federated Machine Learning Applications in Edge
  Computing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herbert Woisetschläger, Alexander Isenko, Ruben Mayer, Shiqiang Wang, Hans-Arno Jacobsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has become a viable technique for realizing
privacy-enhancing distributed deep learning on the network edge. Heterogeneous
hardware, unreliable client devices, and energy constraints often characterize
edge computing systems. In this paper, we propose FLEdge, which complements
existing FL benchmarks by enabling a systematic evaluation of client
capabilities. We focus on computational and communication bottlenecks, client
behavior, and data security implications. Our experiments with models varying
from 14K to 80M trainable parameters are carried out on dedicated hardware with
emulated network characteristics and client behavior. We find that
state-of-the-art embedded hardware has significant memory bottlenecks, leading
to 4x longer processing times than on modern data center GPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for publication at the ACM/IFIP Middleware Conference
  2024. Please cite the published version via
  https://doi.org/10.1145/3652892.3700751</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication-Efficient Distributed Deep Learning via Federated Dynamic
  Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Theologitis, Georgios Frangias, Georgios Anestis, Vasilis Samoladas, Antonios Deligiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the ever-growing volume and decentralized nature of data, coupled
with the need to harness this data and generate knowledge from it, has led to
the extensive use of distributed deep learning (DDL) techniques for training.
These techniques rely on local training that is performed at the distributed
nodes based on locally collected data, followed by a periodic synchronization
process that combines these models to create a global model. However, frequent
synchronization of DL models, encompassing millions to many billions of
parameters, creates a communication bottleneck, severely hindering scalability.
Worse yet, DDL algorithms typically waste valuable bandwidth, and make
themselves less practical in bandwidth-constrained federated settings, by
relying on overly simplistic, periodic, and rigid synchronization schedules.
These drawbacks also have a direct impact on the time required for the training
process, necessitating excessive time for data communication. To address these
shortcomings, we propose Federated Dynamic Averaging (FDA), a
communication-efficient DDL strategy that dynamically triggers synchronization
based on the value of the model variance. In essence, the costly
synchronization step is triggered only if the local models, which are
initialized from a common global model after each synchronization, have
significantly diverged. This decision is facilitated by the communication of a
small local state from each distributed node/worker. Through extensive
experiments across a wide range of learning tasks we demonstrate that FDA
reduces communication cost by orders of magnitude, compared to both traditional
and cutting-edge communication-efficient algorithms. Additionally, we show that
FDA maintains robust performance across diverse data heterogeneity settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as research paper at EDBT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Opinion Dynamics for No-Regret Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08670v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08670v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lazarsfeld, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a cooperative multi-agent bandit setting in the distributed GOSSIP
model: in every round, each of $n$ agents chooses an action from a common set,
observes the action's corresponding reward, and subsequently exchanges
information with a single randomly chosen neighbor, which may inform its choice
in the next round. We introduce and analyze families of memoryless and
time-independent protocols for this setting, inspired by opinion dynamics that
are well-studied for other algorithmic tasks in the GOSSIP model. For
stationary reward settings, we prove for the first time that these simple
protocols exhibit best-of-both-worlds behavior, simultaneously obtaining
constant cumulative regret scaling like $R(T)/T = \widetilde O(1/T)$, and also
reaching consensus on the highest-mean action within $\widetilde O(\sqrt{n})$
rounds. We obtain these results by showing a new connection between the global
evolution of these decentralized protocols and a class of zero-sum
multiplicative weights update} processes. Using this connection, we establish a
general framework for analyzing the population-level regret and other
properties of our protocols. Finally, we show our protocols are also
surprisingly robust to adversarial rewards, and in this regime we obtain
sublinear regret scaling like $R(T)/T = \widetilde O(1/\sqrt{T})$ as long as
the number of rounds does not grow too fast as a function of $n$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Federated Learning Framework over <span class="highlight-title">Heterogeneous</span> LEO Satellite
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fisher Information-based Efficient Curriculum Federated Learning with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Liu, Jiaxiang Ren, Ruoming Jin, Zijie Zhang, Yang Zhou, Patrick Valduriez, Dejing Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a promising paradigm to collaboratively train models with decentralized
data, Federated Learning (FL) can be exploited to fine-tune Large Language
Models (LLMs). While LLMs correspond to huge size, the scale of the training
data significantly increases, which leads to tremendous amounts of computation
and communication costs. The training data is generally non-Independent and
Identically Distributed (non-IID), which requires adaptive data processing
within each device. Although Low Rank Adaptation (LoRA) can significantly
reduce the scale of parameters to update in the fine-tuning process, it still
takes unaffordable time to transfer the low-rank parameters of all the layers
in LLMs. In this paper, we propose a Fisher Information-based Efficient
Curriculum Federated Learning framework (FibecFed) with two novel methods,
i.e., adaptive federated curriculum learning and efficient sparse parameter
update. First, we propose a fisher information-based method to adaptively
sample data within each device to improve the effectiveness of the FL
fine-tuning process. Second, we dynamically select the proper layers for global
aggregation and sparse parameters for local update with LoRA so as to improve
the efficiency of the FL fine-tuning process. Extensive experimental results
based on 10 datasets demonstrate that FibecFed yields excellent performance (up
to 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%
faster) compared with 17 baseline approaches).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 14 tables, to appear in EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual-INR: Communication Efficient On-Device Learning Using Implicit
  Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqiu Chen, Xuebin Yao, Pradeep Subedi, Cong Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing is a distributed computing paradigm that collects and
processes data at or near the source of data generation. The on-device learning
at edge relies on device-to-device wireless communication to facilitate
real-time data sharing and collaborative decision-making among multiple
devices. This significantly improves the adaptability of the edge computing
system to the changing environments. However, as the scale of the edge
computing system is getting larger, communication among devices is becoming the
bottleneck because of the limited bandwidth of wireless communication leads to
large data transfer latency. To reduce the amount of device-to-device data
transmission and accelerate on-device learning, in this paper, we propose
Residual-INR, a fog computing-based communication-efficient on-device learning
framework by utilizing implicit neural representation (INR) to compress
images/videos into neural network weights. Residual-INR enhances data transfer
efficiency by collecting JPEG images from edge devices, compressing them into
INR format at the fog node, and redistributing them for on-device learning. By
using a smaller INR for full image encoding and a separate object INR for
high-quality object region reconstruction through residual encoding, our
technique can reduce the encoding redundancy while maintaining the object
quality. Residual-INR is a promising solution for edge on-device learning
because it reduces data transmission by up to 5.16 x across a network of 10
edge devices. It also facilitates CPU-free accelerated on-device learning,
achieving up to 2.9 x speedup without sacrificing accuracy. Our code is
available at: https://github.com/sharclab/Residual-INR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICCAD 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Connectors and <span class="highlight-title">Heterogeneous</span> Bisimulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Nora, Jurriaan Rot, Lutz Schröder, Paul Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While behavioural equivalences among systems of the same type, such as
Park/Milner bisimilarity of labelled transition systems, are an established
notion, a systematic treatment of relationships between systems of different
type is currently missing. We provide such a treatment in the framework of
universal coalgebra, in which the type of a system (nondeterministic,
probabilistic, weighted, game-based etc.) is abstracted as a set functor: We
introduce relational connectors among set functors, which induce notions of
heterogeneous (bi)simulation among coalgebras of the respective types. We give
a number of constructions on relational connectors. In particular, we identify
composition and converse operations on relational connectors; we construct
corresponding identity relational connectors, showing that the latter
generalize the standard Barr extension of weak-pullback-preserving functors;
and we introduce a Kantorovich construction in which relational connectors are
induced from relations between modalities. For Kantorovich relational
connectors, one has a notion of dual-purpose modal logic interpreted over both
system types, and we prove a corresponding Hennessy-Milner-type theorem stating
that generalized (bi)similarity coincides with theory inclusion on
finitely-branching systems. We apply these results to a number of example
scenarios involving labelled transition systems with different label alphabets,
probabilistic systems, and input/output conformances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identity-Preserving Lax Extensions and Where to Find Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Goncharov, Dirk Hofmaan, Pedro Nora, Lutz Schröder, Paul Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generic notions of bisimulation for various types of systems
(nondeterministic, probabilistic, weighted etc.) rely on identity-preserving
(normal) lax extensions of the functor encapsulating the system type, in the
paradigm of universal coalgebra. It is known that preservation of weak
pullbacks is a sufficient condition for a functor to admit a normal lax
extension (the Barr extension, which in fact is then even strict); in the
converse direction, nothing is currently known about necessary (weak) pullback
preservation conditions for the existence of normal lax extensions. In the
present work, we narrow this gap by showing on the one hand that functors
admitting a normal lax extension preserve 1/4-iso pullbacks, i.e. pullbacks in
which at least one of the projections is an isomorphism. On the other hand, we
give sufficient conditions, showing that a functor admits a normal lax
extension if it weakly preserves either 1/4-iso pullbacks and 4/4-epi pullbacks
(i.e. pullbacks in which all morphisms are epic) or inverse images. We apply
these criteria to concrete examples, in particular to functors modelling
neighbourhood systems and weighted systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Model Checker for Natural Strategic Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Aruta, Vadim Malvone, Aniello Murano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last two decades, Alternating-time Temporal Logic (ATL) has been
proved to be very useful in modeling strategic reasoning for Multi-Agent
Systems (MAS). However, this logic struggles to capture the bounded rationality
inherent in human decision-making processes. To overcome these limitations,
Natural Alternating-time Temporal Logic (NatATL) has been recently introduced.
As an extension of ATL, NatATL incorporates bounded memory constraints into
agents' strategies, which allows to resemble human cognitive limitations. In
this paper, we present a model checker tool for NatATL specifications - both
for memoryless strategies and strategies with recall - integrated into VITAMIN,
an open-source model checker designed specifically for MAS verification. By
embedding NatATL into VITAMIN, we transform theoretical advancements into a
practical verification framework, enabling comprehensive analysis and
validation of strategic reasoning in complex multi-agent environments. Our
novel tool paves the way for applications in areas such as explainable AI and
human-in-the-loop systems, highlighting NatATL's substantial potential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tableaux for Automated Reasoning in Dependently-Typed Higher-Order Logic
  (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Niederhauser, Chad E. Brown, Cezary Kaliszyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependent type theory gives an expressive type system facilitating succinct
formalizations of mathematical concepts. In practice, it is mainly used for
interactive theorem proving with intensional type theories, with PVS being a
notable exception. In this paper, we present native rules for automated
reasoning in a dependently-typed version (DHOL) of classical higher-order logic
(HOL). DHOL has an extensional type theory with an undecidable type checking
problem which contains theorem proving. We implemented the inference rules as
well as an automatic type checking mode in Lash, a fork of Satallax, the
leading tableaux-based prover for HOL. Our method is sound and complete with
respect to provability in DHOL. Completeness is guaranteed by the incorporation
of a sound and complete translation from DHOL to HOL recently proposed by
Rothgang et al. While this translation can already be used as a preprocessing
step to any HOL prover, to achieve better performance, our system directly
works in DHOL. Moreover, experimental results show that the DHOL version of
Lash can outperform all major HOL provers executed on the translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>extended version with appendix of corresponding IJCAR 2024 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Explanations for Neuro-Symbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushmita Paul, Jinqiang Yu, Jip J. Dekker, Alexey Ignatiev, Peter J. Stuckey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the practical success of Artificial Intelligence (AI), current neural
AI algorithms face two significant issues. First, the decisions made by neural
architectures are often prone to bias and brittleness. Second, when a chain of
reasoning is required, neural systems often perform poorly. Neuro-symbolic
artificial intelligence is a promising approach that tackles these (and other)
weaknesses by combining the power of neural perception and symbolic reasoning.
Meanwhile, the success of AI has made it critical to understand its behaviour,
leading to the development of explainable artificial intelligence (XAI). While
neuro-symbolic AI systems have important advantages over purely neural AI, we
still need to explain their actions, which are obscured by the interactions of
the neural and symbolic components. To address the issue, this paper proposes a
formal approach to explaining the decisions of neuro-symbolic systems. The
approach hinges on the use of formal abductive explanations and on solving the
neuro-symbolic explainability problem hierarchically. Namely, it first computes
a formal explanation for the symbolic component of the system, which serves to
identify a subset of the individual parts of neural information that needs to
be explained. This is followed by explaining only those individual neural
inputs, independently of each other, which facilitates succinctness of
hierarchical formal explanations and helps to increase the overall performance
of the approach. Experimental results for a few complex reasoning tasks
demonstrate practical efficiency of the proposed approach, in comparison to
purely neural systems, from the perspective of explanation size, explanation
time, training time, model sizes, and the quality of explanations reported.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural temporal logic for mechanized program verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleftherios Ioannidis, Yannick Zakowski, Steve Zdancewic, Sebastian Angel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanized verification of liveness properties for programs with effects,
nondeterminism, and nontermination is difficult. Existing temporal reasoning
frameworks operate on the level of models (traces, automata) not executable
code, creating a verification gap and losing the benefits of modularity and
composition enjoyed by structural program logics. Reasoning about infinite
traces and automata requires complex (co-)inductive proof techniques and
familiarity with proof assistant mechanics (e.g., guardedness checker). We
propose a structural approach to the verification of temporal properties with a
new temporal logic that we call ictl. Using ictl, we internalize complex
(co-)inductive proof techniques to structural lemmas and reasoning about
variants and invariants. We show that it is possible to perform mechanized
proofs of general temporal properties, while working in a high-level of
abstraction. We demonstrate the benefits of ictl by giving mechanized proofs of
safety and liveness properties for programs with queues, secure memory, and
distributed consensus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Satisfied: An end-to-end framework for SAT generation and
  prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher R. Serrano, Jonathan Gallagher, Kenji Yamada, Alexei Kopylov, Michael A. Warren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The boolean satisfiability (SAT) problem asks whether there exists an
assignment of boolean values to the variables of an arbitrary boolean formula
making the formula evaluate to True. It is well-known that all NP-problems can
be coded as SAT problems and therefore SAT is important both practically and
theoretically. From both of these perspectives, better understanding the
patterns and structure implicit in SAT data is of significant value. In this
paper, we describe several advances that we believe will help open the door to
such understanding: we introduce hardware accelerated algorithms for fast SAT
problem generation, a geometric SAT encoding that enables the use of
transformer architectures typically applied to vision tasks, and a simple yet
effective technique we term head slicing for reducing sequence length
representation inside transformer architectures. These advances allow us to
scale our approach to SAT problems with thousands of variables and tens of
thousands of clauses. We validate our architecture, termed Satisfiability
Transformer (SaT), on the SAT prediction task with data from the SAT
Competition (SATComp) 2022 problem sets. Prior related work either leveraged a
pure machine learning approach, but could not handle SATComp-sized problems, or
was hybrid in the sense of integrating a machine learning component in a
standard SAT solving tool. Our pure machine learning approach achieves
prediction accuracies comparable to recent work, but on problems that are an
order of magnitude larger than previously demonstrated. A fundamental aspect of
our work concerns the very nature of SAT data and its suitability for training
machine learning models. We both describe experimental results that probe the
landscape of where SAT data can be successfully used for learning and position
these results within the broader context of complexity and learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeanAgent: Lifelong Learning for Formal Theorem Proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06209v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06209v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been successful in mathematical reasoning
tasks such as formal theorem proving when integrated with interactive proof
assistants like Lean. Existing approaches involve training or fine-tuning an
LLM on a specific dataset to perform well on particular domains, such as
undergraduate-level mathematics. These methods struggle with generalizability
to advanced mathematics. A fundamental limitation is that these approaches
operate on static domains, failing to capture how mathematicians often work
across multiple domains and projects simultaneously or cyclically. We present
LeanAgent, a novel lifelong learning framework for theorem proving that
continuously generalizes to and improves on ever-expanding mathematical
knowledge without forgetting previously learned knowledge. LeanAgent introduces
several key innovations, including a curriculum learning strategy that
optimizes the learning trajectory in terms of mathematical difficulty, a
dynamic database for efficient management of evolving mathematical knowledge,
and progressive training to balance stability and plasticity. LeanAgent
successfully proves 162 theorems previously unproved by humans across 23
diverse Lean repositories, many from advanced mathematics. It performs
significantly better than the static LLM baseline, proving challenging theorems
in domains like abstract algebra and algebraic topology while showcasing a
clear progression of learning from basic concepts to advanced topics. In
addition, we analyze LeanAgent's superior performance on key lifelong learning
metrics. LeanAgent achieves exceptional scores in stability and backward
transfer, where learning new tasks improves performance on previously learned
tasks. This emphasizes LeanAgent's continuous generalizability and improvement,
explaining its superior theorem-proving performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algebraic Reasoning over Relational Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08445v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08445v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Jurka, Stefan Milius, Henning Urbat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many important computational structures involve an intricate interplay
between algebraic features (given by operations on the underlying set) and
relational features (taking account of notions such as order or distance). This
paper investigates algebras over relational structures axiomatized by an
infinitary Horn theory, which subsume, for example, partial algebras, various
incarnations of ordered algebras, quantitative algebras introduced by Mardare,
Panangaden, and Plotkin, and their recent extension to generalized metric
spaces and lifted algebraic signatures by Mio, Sarkis, and Vignudelli. To this
end, we develop the notion of clustered equation, which is inspired by Mardare
et al.'s basic conditional equations in the theory of quantitative algebras, at
the level of generality of arbitrary relational structures, and we prove that
it is equivalent to an abstract categorical form of equation earlier introduced
by Milius and Urbat. Our main results are a family of Birkhoff-type variety
theorems (classifying the expressive power of clustered equations) and an
exactness theorem (classifying abstract equations by a congruence property).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How hard can it be? Quantifying MITRE attack campaigns with attack trees
  and cATM logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano M. Nicoletti, Milan Lopuhaä-Zwakenberg, Mariëlle Stoelinga, Fabio Massacci, Carlos E. Budde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of cyber threats grows more complex by the day. Advanced
Persistent Threats carry out systematic attack campaigns against which
cybersecurity practitioners must defend. Examples of such organized attacks are
operations Dream Job, Wocao, WannaCry or the SolarWinds Compromise. To evaluate
which risks are most threatening, and which campaigns to prioritize against
when defending, cybersecurity experts must be equipped with the right toolbox.
In particular, they must be able to (a) obtain likelihood values for each
attack campaign recorded in the wild and (b) reliably and transparently
operationalize these values to carry out quantitative comparisons among
campaigns. This will allow security experts to perform quantitatively-informed
decision making that is transparent and accountable. In this paper we construct
such a framework by: (1) quantifying the likelihood of attack campaigns via
data-driven procedures on the MITRE knowledge base and (2) introducing a
methodology for automatic modelling of MITRE intelligence data: this is
complete in the sense that it captures any attack campaign via template attack
tree models. (3) We further propose a computational framework to carry out this
comparisons based on the cATM formal logic, and implement this into an
open-source Python tool. Finally, we validate our approach by quantifying the
likelihood of all MITRE campaigns, and comparing the likelihood of the Wocao
and Dream Job MITRE campaigns -- generated with our proposed approach --
against "ad hoc" traditionally-built attack tree models, demonstrating how our
methodology is substantially lighter in modelling effort, and still capable of
capturing all the quantitative relevant data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-deterministic, probabilistic, and quantum effects through the lens
  of event structures (Technical report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vítor Fernandes, Marc de Visme, Benoît Valiron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider event structures and their probabilistic and
quantum extensions as originally defined by Winskel. If these structures have
already been part of sophisticated computational models, they have rarely been
directly studied as an immediate model of execution traces of programs. This
paper offers such an analysis. We propose a simple imperative operational
framework and show how to derive soundness and adequacy results with event
structures considered as a semantics. We show how event structures naturally
handle non-deterministic, probabilistic and quantum effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructive Interpolation and Concept-Based Beth Definability for
  Description Logics via Sequents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15840v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15840v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim S. Lyon, Jonas Karge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a constructive method applicable to a large number of
description logics (DLs) for establishing the concept-based Beth definability
property (CBP) based on sequent systems. Using the highly expressive DL RIQ as
a case study, we introduce novel sequent calculi for RIQ-ontologies and show
how certain interpolants can be computed from sequent calculus proofs, which
permit the extraction of explicit definitions of implicitly definable concepts.
To the best of our knowledge, this is the first sequent-based approach to
computing interpolants and definitions within the context of DLs, as well as
the first proof that RIQ enjoys the CBP. Moreover, due to the modularity of our
sequent systems, our results hold for restrictions of RIQ, and are applicable
to other DLs by suitable modifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Apartness relations between propositions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zoltan A. Kocsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We classify all apartness relations definable in propositional logics
extending intuitionistic logic using Heyting algebra semantics. We show that
every Heyting algebra which contains a non-trivial apartness term satisfies the
weak law of excluded middle, and every Heyting algebra which contains a tight
apartness term is in fact a Boolean algebra. This answers a question of E.
Rijke regarding the correct notion of apartness for propositions, and yields a
short classification of apartness terms that can occur in a Heyting algebra. We
also show that Martin-L\"of Type Theory is not able to construct non-trivial
apartness relations between propositions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final revision, to appear in Mathematical Logic Quarterly; 19 pages,
  1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-Aware Computing By Partial Quantifier Elimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05928v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05928v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugene Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By structure-aware computing (SAC) we mean computing that is formula-specific
i.e., takes into account the structure of the formula at hand. Virtually all
efficient algorithms of hardware verification employ some form of SAC. We
relate SAC to $\mathit{partial}$ $\mathit{quantifier}$ $\mathit{elimination}$
(PQE). The latter is a generalization of regular quantifier elimination where
one can take a $\mathit{part}$ of the formula out of the scope of quantifiers.
The objective of this paper is to emphasize the significance of studying PQE
for enhancing the $\mathit{existing}$ methods of SAC and creating
$\mathit{new}$ ones. First, we show that interpolation (that can be viewed as
an instance of SAC) is a special case of PQE. Then we describe application of
SAC by PQE to three different problems of hardware verification: property
generation, equivalence checking and model checking. Besides, we discuss using
SAC by PQE for SAT solving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Solitary Directives to Interactive Encouragement! <span class="highlight-title">LLM</span> Secure Code
  Generation by Natural Language Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shigang Liu, Bushra Sabir, Seung Ick Jang, Yuval Kansal, Yansong Gao, Kristen Moore, Alsharif Abuadbba, Surya Nepal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable potential in code
generation, making them increasingly important in the field. However, the
security issues of generated code have not been fully addressed, and the
usability of LLMs in code generation still requires further exploration.
  This work introduces SecCode, a framework that leverages an innovative
interactive encouragement prompting (EP) technique for secure code generation
with \textit{only NL} prompts. This approach ensures that the prompts can be
easily shared and understood by general users. SecCode functions through three
stages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection
and Fixing, utilising our proposed encouragement prompting; 3) Vulnerability
Cross-Checking and Code Security Refinement. These stages are executed in
multiple interactive iterations to progressively enhance security. By using
both proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source
LLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated
on three benchmark datasets, extensive experimental results show that our
proposed SecCode greatly outperforms compared baselines, generating secure code
with a high vulnerability correction rate. For example, SecCode exhibits a high
fix success rate of over 76\% after running 5 automated EP interactive
iterations and over 89\% after running 10 automated EP interactive iterations.
To the best of our knowledge, this work is the first to formulate secure code
generation with NL prompts only. We have open-sourced our code and encourage
the community to focus on secure code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural temporal logic for mechanized program verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleftherios Ioannidis, Yannick Zakowski, Steve Zdancewic, Sebastian Angel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanized verification of liveness properties for programs with effects,
nondeterminism, and nontermination is difficult. Existing temporal reasoning
frameworks operate on the level of models (traces, automata) not executable
code, creating a verification gap and losing the benefits of modularity and
composition enjoyed by structural program logics. Reasoning about infinite
traces and automata requires complex (co-)inductive proof techniques and
familiarity with proof assistant mechanics (e.g., guardedness checker). We
propose a structural approach to the verification of temporal properties with a
new temporal logic that we call ictl. Using ictl, we internalize complex
(co-)inductive proof techniques to structural lemmas and reasoning about
variants and invariants. We show that it is possible to perform mechanized
proofs of general temporal properties, while working in a high-level of
abstraction. We demonstrate the benefits of ictl by giving mechanized proofs of
safety and liveness properties for programs with queues, secure memory, and
distributed consensus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automated Verification of <span class="highlight-title">LLM</span>-Synthesized C Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasita Mukherjee, Benjamin Delaware
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present \synver{}, a novel synthesis and verification framework for C
programs, that deploys a Large Language Model (LLM) to search for a candidate
program that satisfies the given specification. Our key idea is to impose
syntactic and semantic biases on programs generated by LLMs, such that the
synthesized program is more amenable to automated verification. Based on this
idea, we propose a novel specification-verification tool, built on top of
Verified Software Toolchain, that help automate the process. Our experiments on
a diverse set of benchmarks drawn from the deductive program synthesis
community, shows that this approach is scalable and extensible. The benchmarks
constitute of specifications comprising of basic coding examples, Separation
Logic based assertions, and API specifications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Quantized Large Language Models for Code Generation on
  Low-Resource Language Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enkhbold Nyamsuren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Democratization of AI is an important topic within the broader topic of the
digital divide. This issue is relevant to LLMs, which are becoming popular as
AI co-pilots but suffer from a lack of accessibility due to high computational
demand. In this study, we evaluate whether quantization is a viable approach
toward enabling LLMs on generic consumer devices. The study assesses the
performance of five quantized code LLMs in Lua code generation tasks. To
evaluate the impact of quantization, the models with 7B parameters were tested
on a consumer laptop at 2-, 4-, and 8-bit integer precisions and compared to
non-quantized code LLMs with 1.3, 2, and 3 billion parameters. Lua is chosen as
a low-level resource language to avoid models' biases related to high-resource
languages. The results suggest that the models quantized at the 4-bit integer
precision offer the best trade-off between performance and model size. These
models can be comfortably deployed on an average laptop without a dedicated
GPU. The performance significantly drops at the 2-bit integer precision. The
models at 8-bit integer precision require more inference time that does not
effectively translate to better performance. The 4-bit models with 7 billion
parameters also considerably outperform non-quantized models with lower
parameter numbers despite having comparable model sizes with respect to storage
and memory demand. While quantization indeed increases the accessibility of
smaller LLMs with 7 billion parameters, these LLMs demonstrate overall low
performance (less than 50\%) on high-precision and low-resource tasks such as
Lua code generation. While accessibility is improved, usability is still not at
the practical level comparable to foundational LLMs such as GPT-4o or Llama 3.1
405B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Failure Transparency in Stateful Dataflow Systems (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksey Veresov, Jonas Spenger, Paris Carbone, Philipp Haller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Failure transparency enables users to reason about distributed systems at a
higher level of abstraction, where complex failure-handling logic is hidden.
This is especially true for stateful dataflow systems, which are the backbone
of many cloud applications. In particular, this paper focuses on proving
failure transparency in Apache Flink, a popular stateful dataflow system. Even
though failure transparency is a critical aspect of Apache Flink, to date it
has not been formally proven. Showing that the failure transparency mechanism
is correct, however, is challenging due to the complexity of the mechanism
itself. Nevertheless, this complexity can be effectively hidden behind a
failure transparent programming interface. To show that Apache Flink is failure
transparent, we model it in small-step operational semantics. Next, we provide
a novel definition of failure transparency based on observational
explainability, a concept which relates executions according to their
observations. Finally, we provide a formal proof of failure transparency for
the implementation model; i.e., we prove that the failure-free model correctly
abstracts from the failure-related details of the implementation model. We also
show liveness of the implementation model under a fair execution assumption.
These results are a first step towards a verified stack for stateful dataflow
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, 44 pages including references and appendix with
  proofs, technical report, ECOOP 2024; updated with DOI</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operation Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing over FP/EDF Execution Times: Known Results and Open Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Bini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many use cases the execution time of tasks is unknown and can be chosen by
the designer to increase or decrease the application features depending on the
availability of processing capacity. If the application has real-time
constraints, such as deadlines, then the necessary and sufficient
schedulability test must allow the execution times to be left unspecified. By
doing so, the designer can then perform optimization of the execution times by
picking the schedulable values that minimize any given cost.
  In this paper, we review existing results on the formulation of both the
Fixed Priority and Earliest Deadline First exact schedulability constraints.
The reviewed formulations are expressed by a combination of linear constraints,
which enables then optimization routines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at OPERA 2024 (https://opera24.di.unito.it/)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum LDPC Codes with Transversal Non-Clifford Gates via Products of
  Algebraic Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Golowich, Ting-Chun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For every integer $r\geq 2$ and every $\epsilon>0$, we construct an explicit
infinite family of quantum LDPC codes supporting a transversal $C^{r-1}Z$ gate
with length $N$, dimension $K\geq N^{1-\epsilon}$, distance $D\geq
N^{1/r}/\operatorname{poly}(\log N)$, and stabilizer weight
$w\leq\operatorname{poly}(\log N)$. The previous state of the art construction
(in most parameter regimes) was the $r$-dimensional color code, which has only
constant dimension $K=O(1)$, and otherwise has the same parameters up to
polylogarithmic factors. Our construction provides the first known codes with
low-weight stabilizers that are capable of magic state distillation with
arbitrarily small yield parameter $\gamma=\log(N/K)/\log(D)>0$.
  A classical analogue of transversal $C^{r-1}Z$ gates is given by the
multiplication property, which requires component-wise products of classical
codewords to belong to another similar code. As a byproduct of our techniques,
we also obtain a new construction of classical locally testable codes with such
a multiplication property.
  We construct our codes as products of chain complexes associated to classical
LDPC codes, which in turn we obtain by imposing local Reed-Solomon codes on a
specific spectral expander that we construct. We prove that our codes support
the desired transversal $C^{r-1}Z$ gates by using the multiplication property
to combine local circuits based on the topological structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transversal non-Clifford gates for quantum LDPC codes on sheaves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Chun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major goal in quantum computing is to build a fault-tolerant quantum
computer. One approach involves quantum low-density parity-check (qLDPC) codes
that support transversal non-Clifford gates. In this work, we provide a large
family of such codes. The key insight is to interpret the logical operators of
qLDPC codes as geometric surfaces and use the intersection number of these
surfaces to define the non-Clifford operation. At a more abstract level, this
construction is based on defining the cup product on the chain complex induced
from a sheaf.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hardness results for decoding the surface code with Pauli noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10331v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10331v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Fischer, Akimasa Miyake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real quantum computers will be subject to complicated, qubit-dependent noise,
instead of simple noise such as depolarizing noise with the same strength for
all qubits. We can do quantum error correction more effectively if our decoding
algorithms take into account this prior information about the specific noise
present. This motivates us to consider the complexity of surface code decoding
where the input to the decoding problem is not only the syndrome-measurement
results, but also a noise model in the form of probabilities of single-qubit
Pauli errors for every qubit.
  In this setting, we show that quantum maximum likelihood decoding (QMLD) and
degenerate quantum maximum likelihood decoding (DQMLD) for the surface code are
NP-hard and #P-hard, respectively. We reduce directly from SAT for QMLD, and
from #SAT for DQMLD, by showing how to transform a boolean formula into a
qubit-dependent Pauli noise model and set of syndromes that encode the
satisfiability properties of the formula. We also give hardness of
approximation results for QMLD and DQMLD. These are worst-case hardness results
that do not contradict the empirical fact that many efficient surface code
decoders are correct in the average case (i.e., for most sets of syndromes and
for most reasonable noise models). These hardness results are nicely analogous
with the known hardness results for QMLD and DQMLD for arbitrary stabilizer
codes with independent $X$ and $Z$ noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 21 figures. 29 pages, 13 figures in main text. Published
  version to appear in Quantum</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Formal Languages and Automata Theory <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Verification and Refinement of Language Models for
  Safety-Constrained Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Yang, William Ward, Zichao Hu, Joydeep Biswas, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pre-trained language models can generate executable plans (e.g.,
programmatic policies) for solving robot tasks, the generated plans may violate
task-relevant logical specifications due to the models' black-box nature. A
significant gap remains between the language models' outputs and verifiable
executions of plans. We develop a method to generate executable plans and
formally verify them against task-relevant safety specifications. Given a
high-level task description in natural language, the proposed method queries a
language model to generate plans in the form of executable robot programs. It
then converts the generated plan into an automaton-based representation,
allowing formal verification of the automaton against the specifications. We
prove that given a set of verified plans, the composition of these plans also
satisfies the safety specifications. This proof ensures the safety of complex,
multi-component plans, obviating the computation complexity of verifying the
composed plan. We then propose an automated fine-tuning process that refines
the language model to generate specification-compliant plans without the need
for human labeling. The empirical results show a 30 percent improvement in the
probability of generating plans that meet task specifications after
fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oblivious Monitoring for Discrete-Time STL via Fully Homomorphic
  Encryption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Waga, Kotaro Matsuoka, Takashi Suwa, Naoki Matsumoto, Ryotaro Banno, Song Bian, Kohei Suenaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When monitoring a cyber-physical system (CPS) from a remote server, keeping
the monitored data secret is crucial, particularly when they contain sensitive
information, e.g., biological or location data. Recently, Banno et al. (CAV'22)
proposed a protocol for online LTL monitoring that keeps data concealed from
the server using Fully Homomorphic Encryption (FHE). We build on this protocol
to allow arithmetic operations over encrypted values, e.g., to compute a safety
measurement combining distance, velocity, and so forth. Overall, our protocol
enables oblivious online monitoring of discrete-time real-valued signals
against signal temporal logic (STL) formulas. Our protocol combines two FHE
schemes, CKKS and TFHE, leveraging their respective strengths. We employ CKKS
to evaluate arithmetic predicates in STL formulas while utilizing TFHE to
process them using a DFA derived from the STL formula. We conducted case
studies on monitoring blood glucose levels and vehicles' behavior against the
Responsibility-Sensitive Safety (RSS) rules. Our results suggest the practical
relevance of our protocol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RV'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-10-17T00:00:00Z">2024-10-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Hardware Architecturea <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 1.2 mm$^2$ 416 mW 1.44 Mmat/s 64$\times$16 Matrix Preprocessing ASIC
  for Massive MIMO in 22FDX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darja Nonaca, Christoph Studer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive multiuser (MU) multiple-input multiple-output (MIMO) enables
concurrent transmission of multiple users to a multi-antenna basestation (BS).
To detect the users' data using linear equalization, the BS must perform
preprocessing, which requires, among other tasks, the inversion of a matrix
whose dimension equals the number of user data streams. Explicit inversion of
large matrices is notoriously difficult to implement due to high complexity,
stringent data dependencies that lead to high latency, and high numerical
precision requirements. We propose a novel preprocessing architecture based on
the block-LDL matrix factorization, which improves parallelism and, hence,
reduces latency. We demonstrate the effectiveness of our architecture through
(i) massive MU-MIMO system simulations with mmWave channel vectors and (ii)
measurements of a 22FDX ASIC, which is, to our knowledge, the first fabricated
preprocessing engine for massive MU-MIMO with 64 BS antennas and 16
single-antenna users. Our ASIC reaches a clock frequency of 870 MHz while
consuming 416 mW. At its peak throughput, the ASIC preprocesses 1.44 M
64$\times$16 matrices per second at a latency of only 0.7 $\mu$s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the IEEE European Solid-State Electronics Research
  Conference (ESSERC) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three-Input Ciphertext Multiplication for Homomorphic Encryption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajjad Akherati, Yok Jye Tang, Xinmiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Homomorphic encryption (HE) allows computations to be directly carried out on
ciphertexts and is essential to privacy-preserving computing, such as neural
network inference, medical diagnosis, and financial data analysis. Only
addition and 2-input multiplication are defined over ciphertexts in popular HE
schemes. However, many HE applications involve non-linear functions and they
need to be approximated using high-order polynomials to maintain precision. To
reduce the complexity of these computations, this paper proposes 3-input
ciphertext multiplication. One extra evaluation key is introduced to carry out
the relinearization step of ciphertext multiplication, and new formulas are
proposed to combine computations and share intermediate results. Compared to
using two consecutive 2- input multiplications, computing the product of three
ciphertexts utilizing the proposed scheme leads to almost a half of the
latency, 29% smaller silicon area, and lower noise without scarifying the
throughput.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shavette: Low Power Neural Network Acceleration via Algorithm-level
  Error Detection and Undervolting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikael Rinkinen, Lauri Koskinen, Olli Silven, Mehdi Safarpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reduced voltage operation is an effective technique for substantial energy
efficiency improvement in digital circuits. This brief introduces a simple
approach for enabling reduced voltage operation of Deep Neural Network (DNN)
accelerators by mere software modifications. Conventional approaches for
enabling reduced voltage operation e.g., Timing Error Detection (TED) systems,
incur significant development costs and overheads, while not being applicable
to the off-the-shelf components. Contrary to those, the solution proposed in
this paper relies on algorithm-based error detection, and hence, is implemented
with low development costs, does not require any circuit modifications, and is
even applicable to commodity devices. By showcasing the solution through
experimenting on popular DNNs, i.e., LeNet and VGG16, on a GPU platform, we
demonstrate 18% to 25% energy saving with no accuracy loss of the models and
negligible throughput compromise (< 3.9%), considering the overheads from
integration of the error detection schemes into the DNN. The integration of
presented algorithmic solution into the design is simpler when compared
conventional TED based techniques that require extensive circuit-level
modifications, cell library characterizations or special support from the
design tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trinity: A General Purpose FHE Accelerator <span class="chip">MICRO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianglong Deng, Shengyu Fan, Zhicheng Hu, Zhuoyu Tian, Zihao Yang, Jiangrui Yu, Dingyuan Cao, Dan Meng, Rui Hou, Meng Li, Qian Lou, Mingzhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the first multi-modal FHE accelerator based on a
unified architecture, which efficiently supports CKKS, TFHE, and their
conversion scheme within a single accelerator. To achieve this goal, we first
analyze the theoretical foundations of the aforementioned schemes and highlight
their composition from a finite number of arithmetic kernels. Then, we
investigate the challenges for efficiently supporting these kernels within a
unified architecture, which include 1) concurrent support for NTT and FFT, 2)
maintaining high hardware utilization across various polynomial lengths, and 3)
ensuring consistent performance across diverse arithmetic kernels. To tackle
these challenges, we propose a novel FHE accelerator named Trinity, which
incorporates algorithm optimizations, hardware component reuse, and dynamic
workload scheduling to enhance the acceleration of CKKS, TFHE, and their
conversion scheme. By adaptive select the proper allocation of components for
NTT and MAC, Trinity maintains high utilization across NTTs with various
polynomial lengths and imbalanced arithmetic workloads. The experiment results
show that, for the pure CKKS and TFHE workloads, the performance of our Trinity
outperforms the state-of-the-art accelerator for CKKS (SHARP) and TFHE
(Morphling) by 1.49x and 4.23x, respectively. Moreover, Trinity achieves 919.3x
performance improvement for the FHE-conversion scheme over the CPU-based
implementation. Notably, despite the performance improvement, the hardware
overhead of Trinity is only 85% of the summed circuit areas of SHARP and
Morphling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared in MICRO 2024. The first ASIC-based FHE accelerator
  which supports both CKKS, TFHE and their conversions. Provide new SOTA
  performance record for CKKS, TFHE and conversion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Sullivan, Alex Manley, Mohammad Alian, Heechul Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern commercial-off-the-shelf (COTS) multicore processors have advanced
memory hierarchies that enhance memory-level parallelism (MLP), which is
crucial for high performance. To support high MLP, shared last-level caches
(LLCs) are divided into multiple banks, allowing parallel access. However,
uneven distribution of cache requests from the cores, especially when requests
from multiple cores are concentrated on a single bank, can result in
significant contention affecting all cores that access the cache. Such cache
bank contention can even be maliciously induced -- known as cache bank-aware
denial-of-service (DoS) attacks -- in order to jeopardize the system's timing
predictability.
  In this paper, we propose a per-bank bandwidth regulation approach for
multi-banked shared LLC based multicore real-time systems. By regulating
bandwidth on a per-bank basis, the approach aims to prevent unnecessary
throttling of cache accesses to non-contended banks, thus improving overall
performance (throughput) without compromising isolation benefits of throttling.
We implement our approach on a RISC-V system-on-chip (SoC) platform using
FireSim and evaluate extensively using both synthetic and real-world workloads.
Our evaluation results show that the proposed per-bank regulation approach
effectively protects real-time tasks from co-running cache bank-aware DoS
attacks, and offers up to a 3.66$\times$ performance improvement for the
throttled benign best-effort tasks compared to prior bank-oblivious bandwidth
throttling approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RTSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Managing DRAM: A Low-Cost Framework for Enabling Autonomous and
  Efficient in-DRAM Operations <span class="chip">MICRO 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.13358v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.13358v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Hassan, Ataberk Olgun, A. Giray Yaglikci, Haocong Luo, Onur Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The memory controller is in charge of managing DRAM maintenance operations
(e.g., refresh, RowHammer protection, memory scrubbing) to reliably operate
modern DRAM chips. Implementing new maintenance operations often necessitates
modifications in the DRAM interface, memory controller, and potentially other
system components. Such modifications are only possible with a new DRAM
standard, which takes a long time to develop, likely leading to slow progress
in the adoption of new architectural techniques in DRAM chips.
  We propose a new low-cost DRAM architecture, Self-Managing DRAM (SMD), that
enables autonomous in-DRAM maintenance operations by transferring the
responsibility for controlling maintenance operations from the memory
controller to the SMD chip. To enable autonomous maintenance operations, we
make a single modification to the DRAM interface, such that an SMD chip rejects
memory controller accesses to DRAM regions under maintenance, while allowing
memory accesses to others. Thus, SMD enables 1) implementing new in-DRAM
maintenance mechanisms (or modifying existing ones) with no further changes in
the DRAM interface or other system components, and 2) overlapping the latency
of a maintenance operation in one DRAM region with the latency of accessing
data in another.
  We evaluate SMD and show that it 1) can be implemented without adding new
pins to the DDRx interface with low latency and area overhead, 2) achieves 4.1%
average speedup across 20 four-core memory-intensive workloads over a
DDR4-based system/DRAM co-design technique that intelligently parallelizes
maintenance operations with memory accesses, and 3) guarantees forward progress
for rejected memory accesses. We believe and hope SMD can enable innovations in
DRAM architecture to rapidly come to fruition. We open source all SMD source
code and data at https://github.com/CMU-SAFARI/SelfManagingDRAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of MICRO 2024 paper titled "Self-Managing DRAM: A
  Low-Cost Framework for Enabling Autonomous and Efficient DRAM Maintenance
  Operations''</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cost-Efficient <span class="highlight-title">FPGA</span> Implementation of Tiny <span class="highlight-title">Transformer</span> Model using
  Neural ODE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02721v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02721v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ikumi Okubo, Keisuke Sugiura, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has been adopted to image recognition tasks and shown to
outperform CNNs and RNNs while it suffers from high training cost and
computational complexity. To address these issues, a hybrid approach has become
a recent research trend, which replaces a part of ResNet with an MHSA
(Multi-Head Self-Attention). In this paper, we propose a lightweight hybrid
model which uses Neural ODE (Ordinary Differential Equation) as a backbone
instead of ResNet so that we can increase the number of iterations of building
blocks while reusing the same parameters, mitigating the increase in parameter
size per iteration. The proposed model is deployed on a modest-sized FPGA
device for edge computing. The model is further quantized by QAT (Quantization
Aware Training) scheme to reduce FPGA resource utilization while suppressing
the accuracy loss. The quantized model achieves 79.68% top-1 accuracy for STL10
dataset that contains 96$\times$96 pixel images. The weights of the feature
extraction network are stored on-chip to minimize the memory transfer overhead,
allowing faster inference. By eliminating the overhead of memory transfers,
inference can be executed seamlessly, leading to accelerated inference. The
proposed FPGA implementation accelerates the backbone and MHSA parts by
34.01$\times$, and achieves an overall 9.85$\times$ speedup when taking into
account the software pre- and post-processing. The FPGA acceleration leads to
7.10$\times$ better energy efficiency compared to the ARM Cortex-A53 CPU. The
proposed lightweight Transformer model is demonstrated on Xilinx ZCU104 board
for the image recognition of 96$\times$96 pixel images in this paper and can be
applied to different image sizes by modifying the pre-processing layer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGim : An Efficient Graph Neural Network Library for Real
  Processing-In-Memory Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML library that accelerates GNNs on
real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively. We
extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using
emerging GNN models, and demonstrate that it outperforms its state-of-the-art
CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource
utilization than CPU and GPU systems. Our work provides useful recommendations
for software, system and hardware designers. PyGim is publicly available at
https://github.com/CMU-SAFARI/PyGim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-aided explanations of EDA synthesis errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Qiu, Benjamin Tan, Hammond Pearce
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training new engineers in digital design is a challenge, particularly when it
comes to teaching the complex electronic design automation (EDA) tooling used
in this domain. Learners will typically deploy designs in the Verilog and VHDL
hardware description languages to Field Programmable Gate Arrays (FPGAs) from
Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains
(Quartus Prime and Vivado, respectively). These tools are complex and difficult
to use -- yet, as they are the tools used in industry, they are an essential
first step in this space. In this work, we examine how recent advances in
artificial intelligence may be leveraged to address aspects of this challenge.
Specifically, we investigate if Large Language Models (LLMs), which have
demonstrated text comprehension and question-answering capabilities, can be
used to generate novice-friendly explanations of compile-time synthesis error
messages from Quartus Prime and Vivado. To perform this study we generate 936
error message explanations using three OpenAI LLMs over 21 different buggy code
samples. These are then graded for relevance and correctness, and we find that
in approximately 71% of cases the LLMs give correct & complete explanations
suitable for novice learners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. Accepted in IEEE LLM Aided Design Workshop
  (LAD'2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advocate -- Trustworthy Evidence in Cloud Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Werner, Sepideh Masoudi, Fernando Castillo, Fabian Piper, Jonathan Heiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of cloud-native applications, characterized by dynamic,
interconnected services, presents significant challenges for maintaining
trustworthy and auditable systems, especially in sensitive contexts, such as
finance or healthcare. Traditional methods of verification and certification
are often inadequate due to the fast-past and dynamic development practices
common in cloud computing. This paper introduces Advocate, a novel agent-based
system designed to generate verifiable evidence of cloud-native application
operations. By integrating with existing infrastructure tools, such as
Kubernetes and distributed tracing systems, Advocate captures, authenticates,
and stores evidence trails in a tamper-resistant manner. This approach not only
supports the auditing process but also allows for privacy-preserving evidence
aggregation. Advocate's extensible architecture facilitates its deployment in
diverse environments, enabling the verification and adherence to policies and
enhance trust in cloud services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version of the paper at 6th Conference on Blockchain
  Research & Applications for Innovative Networks and Services (BRAINS'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Formal Verification of Federated Learning Orchestration
  Protocols on Satellites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miroslav Popovic, Marko Popovic, Miodrag Djukic, Ilija Basicevic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Python Testbed for Federated Learning Algorithms (PTB-FLA) is a simple FL
framework targeting smart Internet of Things in edge systems that provides both
generic centralized and decentralized FL algorithms, which implement the
corresponding FL orchestration protocols that were formally verified using the
process algebra CSP. This approach is appropriate for systems with stationary
nodes but cannot be applied to systems with moving nodes. In this paper, we use
celestial mechanics to model spacecraft movement, and timed automata (TA) to
formalize and verify the centralized FL orchestration protocol, in two phases.
In the first phase, we created a conventional TA model to prove traditional
properties, namely deadlock freeness and termination. In the second phase, we
created a stochastic TA model to prove timing correctness and to estimate
termination probability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures, submitted to a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Malleus: Straggler-Resilient Hybrid Parallel Training of Large-scale
  Models via Malleable Data and Model Parallelization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Li, Fangcheng Fu, Hao Ge, Sheng Lin, Xuan<span class="highlight-author">yu Wang</span>, Jiawen Niu, Yujie Wang, Hailin Zhang, Xiaonan Nie, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the scale of models and training data continues to grow, there is an
expanding reliance on more GPUs to train large-scale models, which inevitably
increases the likelihood of encountering dynamic stragglers that some devices
lag behind in performance occasionally. However, hybrid parallel training, one
of the de facto paradigms to train large models, is typically sensitive to the
stragglers.
  This paper presents Malleus, a straggler-resilient hybrid parallel training
framework for large-scale models. Malleus captures the dynamic straggler issues
at the nuanced, per-GPU granularity during training. Once a shift in the GPU
ability is detected, Malleus adaptively adjusts the parallelization of GPU
devices, pipeline stages, model layers, and training data through a novel
planning algorithm, accommodating the dynamic stragglers in real time. In
addition, Malleus seamlessly and efficiently migrates the model states to
fulfill the adjusted parallelization plan on the fly, without sacrificing the
stability of the training tasks. Empirical results on large language models
with up to 110B parameters show that Malleus consistently outperforms existing
parallel training frameworks under various straggler situations, delivering on
average 2.63-5.28 times of efficiency improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming Memory Constraints in Quantum Circuit Simulation with a
  High-Fidelity Compression Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zhang, Bo Fang, Fanjiang Ye, Yida Gu, Nathan Tallent, Guangming Tan, Dingwen Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-state quantum circuit simulation requires exponentially increased memory
size to store the state vector as the number of qubits scales, presenting
significant limitations in classical computing systems. Our paper introduces
BMQSim, a novel state vector quantum simulation framework that employs lossy
compression to address the memory constraints on graphics processing unit (GPU)
machines. BMQSim effectively tackles four major challenges for state-vector
simulation with compression: frequent compression/decompression, high memory
movement overhead, lack of dedicated error control, and unpredictable memory
space requirements. Our work proposes an innovative strategy of circuit
partitioning to significantly reduce the frequency of compression occurrences.
We introduce a pipeline that seamlessly integrates compression with data
movement while concealing its overhead. Additionally, BMQSim incorporates the
first GPU-based lossy compression technique with point-wise error control.
Furthermore, BMQSim features a two-level memory management system, ensuring
efficient and stable execution. Our evaluations demonstrate that BMQSim can
simulate the same circuit with over 10 times less memory usage on average,
achieving fidelity over 0.99 and maintaining comparable simulation time to
other state-of-the-art simulators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Spanning Centrality with Random Bouquets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gökhan Göktürk, Kamer Kaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spanning Centrality is a measure used in network analysis to determine the
importance of an edge in a graph based on its contribution to the connectivity
of the entire network. Specifically, it quantifies how critical an edge is in
terms of the number of spanning trees that include that edge. The current
state-of-the-art for All Edges Spanning Centrality~(AESC), which computes the
exact centrality values for all the edges, has a time complexity of
$\mathcal{O}(mn^{3/2})$ for $n$ vertices and $m$ edges. This makes the
computation infeasible even for moderately sized graphs. Instead, there exist
approximation algorithms which process a large number of random walks to
estimate edge centralities. However, even the approximation algorithms can be
computationally overwhelming, especially if the approximation error bound is
small. In this work, we propose a novel, hash-based sampling method and a
vectorized algorithm which greatly improves the execution time by clustering
random walks into {\it Bouquets}. On synthetic random walk benchmarks, {\it
Bouquets} performs $7.8\times$ faster compared to naive, traditional
random-walk generation. We also show that the proposed technique is scalable by
employing it within a state-of-the-art AESC approximation algorithm, {\sc
TGT+}. The experiments show that using Bouquets yields more than $100\times$
speed-up via parallelization with 16 threads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiFuseR: A Distributed Sketch-based Influence Maximization Algorithm for
  GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gökhan Göktürk, Kamer Kaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence Maximization (IM) aims to find a given number of "seed" vertices
that can effectively maximize the expected spread under a given diffusion
model. Due to the NP-Hardness of finding an optimal seed set, approximation
algorithms are often used for IM. However, these algorithms require a large
number of simulations to find good seed sets. In this work, we propose DiFuseR,
a blazing-fast, high-quality IM algorithm that can run on multiple GPUs in a
distributed setting. DiFuseR is designed to increase GPU utilization, reduce
inter-node communication, and minimize overlapping data/computation among the
nodes. Based on the experiments with various graphs, containing some of the
largest networks available, and diffusion settings, the proposed approach is
found to be 3.2x and 12x faster on average on a single GPU and 8 GPUs,
respectively. It can achieve up to 8x and 233.7x speedup on the same hardware
settings. Furthermore, thanks to its smart load-balancing mechanism, on 8 GPUs,
it is on average 5.6x faster compared to its single-GPU performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Your DRAM and SSD for Sustainable and Accessible <span class="highlight-title">LLM</span>
  Inference with Mixed-Precision and Multi-level Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Peng, Zhang Cao, Huaizhi Qu, Zhengyu Zhang, Chang Guo, Yanyong Zhang, Zhichao Zhang, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated remarkable
capabilities, their massive parameter counts and associated extensive computing
make LLMs' deployment the main part of carbon emission from nowadays AI
applications. Compared to modern GPUs like H$100$, it would be significantly
carbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as
shown in Figure~\ref{fig:tisser}, M$40$ only has one third carbon emission of
H$100$'s) for LLM servings. However, the limited High Bandwidth Memory (HBM)
available on such GPU often cannot support the loading of LLMs due to the
gigantic model size and intermediate activation data, making their serving
challenging. For instance, a LLaMA2 model with $70$B parameters typically
requires $128$GB for inference, which substantially surpasses $24$GB HBM in a
$3090$ GPU and remains infeasible even considering the additional $64$GB DRAM.
To address this challenge, this paper proposes a mixed-precision with a model
modularization algorithm to enable LLM inference on outdated hardware with
resource constraints. (The precision denotes the numerical precision like FP16,
INT8, INT4) and multi-level caching (M2Cache).)
  Specifically, our M2Cache first modulizes neurons in LLM and creates their
importance ranking. Then, it adopts a dynamic sparse mixed-precision
quantization mechanism in weight space to reduce computational demands and
communication overhead at each decoding step. It collectively lowers the
operational carbon emissions associated with LLM inference. Moreover, M2Cache
introduces a three-level cache management system with HBM, DRAM, and SSDs that
complements the dynamic sparse mixed-precision inference. To enhance
communication efficiency, M2Cache maintains a neuron-level mixed-precision LRU
cache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Python Applications with Dask and ProxyStore <span class="chip">SC24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Gregory Pauloski, Klaudiusz Rydzy, Valerie Hayot-Sasson, Ian Foster, Kyle Chard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications are increasingly written as dynamic workflows underpinned by an
execution framework that manages asynchronous computations across distributed
hardware. However, execution frameworks typically offer one-size-fits-all
solutions for data flow management, which can restrict performance and
scalability. ProxyStore, a middleware layer that optimizes data flow via an
advanced pass-by-reference paradigm, has shown to be an effective mechanism for
addressing these limitations. Here, we investigate integrating ProxyStore with
Dask Distributed, one of the most popular libraries for distributed computing
in Python, with the goal of supporting scalable and portable scientific
workflows. Dask provides an easy-to-use and flexible framework, but is less
optimized for scaling certain data-intensive workflows. We investigate these
limitations and detail the technical contributions necessary to develop a
robust solution for distributed applications and demonstrate improved
performance on synthetic benchmarks and real applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented as a demo at the SC24 Workshop on High Performance
  Python for Science at Scale (HPPSS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ t-READi: <span class="highlight-title">Transformer</span>-Powered Robust and Efficient Multimodal Inference
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hu, Yuhang Qian, Tianyue Zheng, Ang Li, Zhe Chen, Yue Gao, Xiuzhen Cheng, Jun Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by
autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust
perception become imperative. However, existing fusion methods often make two
assumptions rarely holding in practice: i) similar data distributions for all
inputs and ii) constant availability for all sensors. Because, for example,
lidars have various resolutions and failures of radars may occur, such
variability often results in significant performance degradation in fusion. To
this end, we present tREADi, an adaptive inference system that accommodates the
variability of multimodal sensory data and thus enables robust and efficient
perception. t-READi identifies variation-sensitive yet structure-specific model
parameters; it then adapts only these parameters while keeping the rest intact.
t-READi also leverages a cross-modality contrastive learning method to
compensate for the loss from missing modalities. Both functions are implemented
to maintain compatibility with existing multimodal deep fusion methods. The
extensive experiments evidently demonstrate that compared with the status quo
approaches, t-READi not only improves the average inference accuracy by more
than 6% but also reduces the inference latency by almost 15x with the cost of
only 5% extra memory overhead in the worst case under realistic data and modal
variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mahi-Mahi: Low-Latency Asynchronous BFT DAG-Based Consensus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Jovanovic, Lefteris Kokoris Kogias, Bryan Kumara, Alberto Sonnino, Pasindu Tennage, Igor Zablotchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Mahi-Mahi, the first asynchronous BFT consensus protocol that
achieves sub-second latency in the WAN while processing over 100,000
transactions per second. We accomplish this remarkable performance by building
Mahi-Mahi on an uncertified structured Directed Acyclic Graph (DAG). By
forgoing explicit certification, we significantly reduce the number of messages
required to commit and minimize CPU overhead associated with certificate
verification. Mahi-Mahi introduces a novel commit rule that allows committing
multiple blocks in each DAG round, while ensuring liveness in the presence of
an asynchronous adversary. Mahi-Mahi can be parametrized to either attempt to
commit within 5 message delays, maximizing the probability of commitment under
a continuously active asynchronous adversary, or within 4 message delays, which
reduces latency under a more moderate and realistic asynchronous adversary. We
demonstrate the safety and liveness of Mahi-Mahi in a Byzantine context.
Subsequently, we evaluate Mahi-Mahi in a geo-replicated setting and compare its
performance against state-of-the-art asynchronous consensus protocols,
showcasing Mahi-Mahi's significantly lower latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POSEIDON : Efficient Function Placement at the Edge using Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakhar Jain, Prakhar Singhal, Divyansh Pandey, Giovanni Quatrocchi, Karthik Vaidhyanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge computing allows for reduced latency and operational costs compared to
centralized cloud systems. In this context, serverless functions are emerging
as a lightweight and effective paradigm for managing computational tasks on
edge infrastructures. However, the placement of such functions in constrained
edge nodes remains an open challenge. On one hand, it is key to minimize
network delays and optimize resource consumption; on the other hand, decisions
must be made in a timely manner due to the highly dynamic nature of edge
environments.
  In this paper, we propose POSEIDON, a solution based on Deep Reinforcement
Learning for the efficient placement of functions at the edge. POSEIDON
leverages Proximal Policy Optimization (PPO) to place functions across a
distributed network of nodes under highly dynamic workloads. A comprehensive
empirical evaluation demonstrates that POSEIDON significantly reduces execution
time, network delay, and resource consumption compared to state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted at ICSOC'24 (International Conference on
  Service-Oriented Computing)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGim : An Efficient Graph Neural Network Library for Real
  Processing-In-Memory Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML library that accelerates GNNs on
real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively. We
extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using
emerging GNN models, and demonstrate that it outperforms its state-of-the-art
CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource
utilization than CPU and GPU systems. Our work provides useful recommendations
for software, system and hardware designers. PyGim is publicly available at
https://github.com/CMU-SAFARI/PyGim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PrE-Text: Training Language Models on Private Federated Data in the Age
  of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02958v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02958v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, Daniel Lazar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device training is currently the most common approach for training machine
learning (ML) models on private, distributed user data. Despite this, on-device
training has several drawbacks: (1) most user devices are too small to train
large models on-device, (2) on-device training is communication- and
computation-intensive, and (3) on-device training can be difficult to debug and
deploy. To address these problems, we propose Private Evolution-Text
(PrE-Text), a method for generating differentially private (DP) synthetic
textual data. First, we show that across multiple datasets, training small
models (models that fit on user devices) with PrE-Text synthetic data
outperforms small models trained on-device under practical privacy regimes
($\epsilon=1.29$, $\epsilon=7.58$). We achieve these results while using
9$\times$ fewer rounds, 6$\times$ less client computation per round, and
100$\times$ less communication per round. Second, finetuning large models on
PrE-Text's DP synthetic data improves large language model (LLM) performance on
private data across the same range of privacy budgets. Altogether, these
results suggest that training on DP synthetic data can be a better option than
training a model on-device on private distributed data. Code is available at
https://github.com/houcharlie/PrE-Text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 (Oral). Latest revision corrects a discussion on concurrent
  work arXiv:2403.01749. We described their work as reliant on using
  closed-sourced models when in reality they also evaluate and use open source
  models. This has been corrected in this version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Complexity <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On estimating the trace of quantum state powers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupan Liu, Qisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the computational complexity of estimating the trace of
quantum state powers $\text{tr}(\rho^q)$ for an $n$-qubit mixed quantum state
$\rho$, given its state-preparation circuit of size $\text{poly}(n)$. This
quantity is closely related to and often interchangeable with the Tsallis
entropy $\text{S}_q(\rho) = \frac{1-\text{tr}(\rho^q)}{q-1}$, where $q = 1$
corresponds to the von Neumann entropy. For any non-integer $q \geq 1 +
\Omega(1)$, we provide a quantum estimator for $\text{S}_q(\rho)$ with time
complexity $\text{poly}(n)$, exponentially improving the prior best results of
$\exp(n)$ due to Acharya, Issa, Shende, and Wagner (ISIT 2019), Wang, Guan,
Liu, Zhang, and Ying (TIT 2024), and Wang, Zhang, and Li (TIT 2024), and Wang
and Zhang (ESA 2024). Our speedup is achieved by introducing efficiently
computable uniform approximations of positive power functions into quantum
singular value transformation.
  Our quantum algorithm reveals a sharp phase transition between the case of
$q=1$ and constant $q>1$ in the computational complexity of the Quantum
$q$-Tsallis Entropy Difference Problem (TsallisQED$_q$), particularly deciding
whether the difference $\text{S}_q(\rho_0) - \text{S}_q(\rho_1)$ is at least
$0.001$ or at most $-0.001$:
  - For any $1+\Omega(1) \leq q \leq 2$, TsallisQED$_q$ is
$\mathsf{BQP}$-complete, which implies that Purity Estimation is also
$\mathsf{BQP}$-complete.
  - For any $1 \leq q \leq 1 + \frac{1}{n-1}$, TsallisQED$_q$ is
$\mathsf{QSZK}$-hard, leading to hardness of approximating von Neumann entropy
because $\text{S}_q(\rho) \leq \text{S}(\rho)$, as long as $\mathsf{BQP}
\subsetneq \mathsf{QSZK}$.
  The hardness results are derived from reductions based on new inequalities
for the quantum $q$-Jensen-(Shannon-)Tsallis divergence with $1\leq q \leq 2$,
which are of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 3 tables, 3 algorithms. To appear in SODA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quasi-quantum states and the quasi-quantum PCP theorem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Arad, Miklos Santha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $k$-local quasi-quantum states: a superset of the regular
quantum states, defined by relaxing the positivity constraint. We show that a
$k$-local quasi-quantum state on $n$ qubits can be 1-1 mapped to a distribution
of assignments over $n$ variables with an alphabet of size $4$, which is
subject to non-linear constraints over its $k$-local marginals. Therefore,
solving the $k$-local Hamiltonian over the quasi-quantum states is equivalent
to optimizing a distribution of assignment over a classical $k$-local CSP. We
show that this optimization problem is essentially classical by proving it is
NP-complete. Crucially, just as ordinary quantum states, these distributions
lack a simple tensor-product structure and are therefore not determined
straightforwardly by their local marginals. Consequently, our classical
optimization problem shares some unique aspects of Hamiltonian complexity: it
lacks an easy search-to-decision reduction, and it is not clear that its 1D
version can be solved with dynamical programming (i.e., it could remain
NP-hard).
  Our main result is a PCP theorem for the $k$-local Hamiltonian over the
quasi-quantum states in the form of a hardness-of-approximation result. The
proof suggests the existence of a subtle promise-gap amplification procedure in
a model that shares many similarities with the quantum local Hamiltonian
problem, thereby providing insights on the quantum PCP conjecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 18 figures, comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive and oblivious statistical adversaries are equivalent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Blanc, Gregory Valiant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We resolve a fundamental question about the ability to perform a statistical
task, such as learning, when an adversary corrupts the sample. Such adversaries
are specified by the types of corruption they can make and their level of
knowledge about the sample. The latter distinguishes between sample-adaptive
adversaries which know the contents of the sample when choosing the corruption,
and sample-oblivious adversaries, which do not. We prove that for all types of
corruptions, sample-adaptive and sample-oblivious adversaries are
\emph{equivalent} up to polynomial factors in the sample size. This resolves
the main open question introduced by \cite{BLMT22} and further explored in
\cite{CHLLN23}.
  Specifically, consider any algorithm $A$ that solves a statistical task even
when a sample-oblivious adversary corrupts its input. We show that there is an
algorithm $A'$ that solves the same task when the corresponding sample-adaptive
adversary corrupts its input. The construction of $A'$ is simple and maintains
the computational efficiency of $A$: It requests a polynomially larger sample
than $A$ uses and then runs $A$ on a uniformly random subsample.
  One of our main technical tools is a new structural result relating two
distributions defined on sunflowers which may be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Rate Multivariate Polynomial Evaluation Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swastik Kopparty, Mrinal Kumar, Harry Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical Reed-Muller codes over a finite field $\mathbb{F}_q$ are based
on evaluations of $m$-variate polynomials of degree at most $d$ over a product
set $U^m$, for some $d$ less than $|U|$. Because of their good distance
properties, as well as the ubiquity and expressive power of polynomials, these
codes have played an influential role in coding theory and complexity theory.
This is especially so in the setting of $U$ being ${\mathbb{F}}_q$ where they
possess deep locality properties. However, these Reed-Muller codes have a
significant limitation in terms of the rate achievable -- the rate cannot be
more than $\frac{1}{m{!}} = \exp(-m \log m)$.
  In this work, we give the first constructions of multivariate polynomial
evaluation codes which overcome the rate limitation -- concretely, we give
explicit evaluation domains $S \subseteq \mathbb{F}_q^m$ on which evaluating
$m$-variate polynomials of degree at most $d$ gives a good code. For $m= O(1)$,
these new codes have relative distance $\Omega(1)$ and rate $1 - \epsilon$ for
any $\epsilon > 0$. In fact, we give two quite different constructions, and for
both we develop efficient decoding algorithms for these codes that can decode
from half the minimum distance.
  The first of these codes is based on evaluating multivariate polynomials on
simplex-like sets whereas the second construction is more algebraic, and
surprisingly (to us), has some strong locality properties, specifically, we
show that they are locally testable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract shortened due to arxiv's space constraints</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Social Choice: Parameterized Complexity and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiehua Chena, Christian Hatschka, Sofia Simola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We survey two key problems-Multi-Winner Determination and Hedonic Games in
Computational Social Choice, with a special focus on their parameterized
complexity, and propose some research challenges in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Computer Science Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum computational complexity of matrix functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Cifuentes, Samson Wang, Thais L. Silva, Mario Berta, Leandro Aolita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the dividing line between classical and quantum computational
power in estimating properties of matrix functions. More precisely, we study
the computational complexity of two primitive problems: given a function $f$
and a Hermitian matrix $A$, compute a matrix element of $f(A)$ or compute a
local measurement on $f(A)|0\rangle^{\otimes n}$, with $|0\rangle^{\otimes n}$
an $n$-qubit reference state vector, in both cases up to additive approximation
error. We consider four functions -- monomials, Chebyshev polynomials, the time
evolution function, and the inverse function -- and probe the complexity across
a broad landscape covering different problem input regimes. Namely, we consider
two types of matrix inputs (sparse and Pauli access), matrix properties (norm,
sparsity), the approximation error, and function-specific parameters.
  We identify BQP-complete forms of both problems for each function and then
toggle the problem parameters to easier regimes to see where hardness remains,
or where the problem becomes classically easy. As part of our results we make
concrete a hierarchy of hardness across the functions; in parameter regimes
where we have classically efficient algorithms for monomials, all three other
functions remain robustly BQP-hard, or hard under usual computational
complexity assumptions. In identifying classically easy regimes, among others,
we show that for any polynomial of degree $\mathrm{poly}(n)$ both problems can
be efficiently classically simulated when $A$ has $O(\log n)$ non-zero
coefficients in the Pauli basis. This contrasts with the fact that the problems
are BQP-complete in the sparse access model even for constant row sparsity,
whereas the stated Pauli access efficiently constructs sparse access with row
sparsity $O(\log n)$. Our work provides a catalog of efficient quantum and
classical algorithms for fundamental linear-algebra tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11+28 pages, 1 table, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Commuting Local Hamiltonians Beyond 2D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10495v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10495v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Bostanci, Yeongwoo Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commuting local Hamiltonians provide a testing ground for studying many of
the most interesting open questions in quantum information theory, including
the quantum PCP conjecture and the existence of area laws. Although they are a
simplified model of quantum computation, the status of the commuting local
Hamiltonian problem remains largely unknown. A number of works have shown that
increasingly expressive families of commuting local Hamiltonians admit
completely classical verifiers. Despite intense work, the largest class of
commuting local Hamiltonians we can place in NP are those on a square lattice,
where each lattice site is a qutrit. Even worse, many of the techniques used to
analyze these problems rely heavily on the geometry of the square lattice and
the properties of the numbers 2 and 3 as local dimensions. In this work, we
present a new technique to analyze the complexity of various families of
commuting local Hamiltonians: guided reductions. Intuitively, these are a
generalization of typical reduction where the prover provides a guide so that
the verifier can construct a simpler Hamiltonian. The core of our reduction is
a new rounding technique based on a combination of Jordan's Lemma and the
Structure Lemma. Our rounding technique is much more flexible than previous
work, and allows us to show that a larger family of commuting local
Hamiltonians is in NP, albiet with the restriction that all terms are rank-1.
Specifically, we prove the following two results:
  1. Commuting local Hamiltonians in 2D that are rank-1 are contained in NP,
independent of the qudit dimension. Note that this family of commuting local
Hamiltonians has no restriction on the local dimension or the locality.
  2. We prove that rank-1, 3D commuting Hamiltonians with qudits on edges are
in NP. To our knowledge this is the first time a family of 3D commuting local
Hamiltonians has been contained in NP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 12 figures. v2: Fixed transparencies in figures. v3: Fixed
  date in title page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lift-and-Project Integrality Gaps for Santa Claus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Bamas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is devoted to the study of the MaxMinDegree Arborescence (MMDA)
problem in layered directed graphs of depth $\ell\le O(\log n/\log \log n)$,
which is an important special case of the Santa Claus problem. Obtaining a
polylogarithmic approximation for MMDA in polynomial time is of high interest
as it is a necessary condition to improve upon the well-known 2-approximation
for makespan scheduling on unrelated machines by Lenstra, Shmoys, and Tardos
[FOCS'87]. The only way we have to solve the MMDA problem within a
polylogarithmic factor is via an elegant recursive rounding of the
$(\ell-1)^{th}$ level of the Sherali-Adams hierarchy, which needs time
$n^{O(\ell)}$ to solve. However, it remains plausible that one could obtain a
polylogarithmic approximation in polynomial time by using the same rounding
with only $1$ round of the Sherali-Adams hierarchy. As a main result, we rule
out this possibility by constructing an MMDA instance of depth $3$ for which an
integrality gap of $n^{\Omega(1)}$ survives $1$ round of the Sherali-Adams
hierarchy. This result is tight since it is known that after only $2$ rounds
the gap is at most polylogarithmic on depth-3 graphs. Second, we show that our
instance can be ``lifted'' via a simple trick to MMDA instances of any depth
$\ell\in \Omega(1)\cap o(\log n/\log \log n)$ (the whole range of interest),
for which we conjecture that an integrality gap of $n^{\Omega(1/\ell)}$
survives $\Omega(\ell)$ rounds of Sherali-Adams. We show a number of
intermediate results towards this conjecture, which also suggest that our
construction is a significant challenge to the techniques used so far for Santa
Claus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added figures and comments. To appear in SODA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying lower bounds for algebraic machines, semantically 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.06787v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.06787v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Seiller, Luc Pellissier, Ulysse Léchine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new abstract method for proving lower bounds in
computational complexity. Based on the notion of topological and measurable
entropy for dynamical systems, it is shown to generalise three previous lower
bounds results from the literature in algebraic complexity. We use it to prove
that maxflow, a Ptime complete problem, is not computable in polylogarithmic
time on parallel random access machines (prams) working with real numbers. This
improves, albeit slightly, on a result of Mulmuley since the class of machines
considered extends the class "prams without bit operations", making more
precise the relationship between Mulmuley's result and similar lower bounds on
real prams.
  More importantly, we show our method captures previous lower bounds results
from the literature, thus providing a unifying framework for "topological"
proofs of lower bounds: Steele and Yao's lower bounds for algebraic decision
trees, Ben-Or's lower bounds for algebraic computation trees, Cucker's proof
that NC is not equal to Ptime in the real case, and Mulmuley's lower bounds for
"prams without bit operations".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in Information and Computation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ActionReasoningBench: Reasoning about Actions with and without
  Ramification Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divij Handa, Pavel Dolin, Shrinidhi Kumbhar, Tran Cao Son, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning about Actions and Change (RAC) has historically played a pivotal
role in solving foundational AI problems, such as the frame problem. It has
driven advancements in AI fields, such as non-monotonic and commonsense
reasoning. RAC remains crucial for AI systems that operate in dynamic
environments, engage in interactive scenarios, or rely on commonsense
reasoning. Despite substantial advances made by Large Language Models (LLMs) in
various AI domains, their performance in RAC remains underexplored. To address
this gap, we introduce a new diagnostic benchmark, ActionReasoningBench, which
encompasses 8 domains and includes questions for up to 19 action sequences.
This benchmark rigorously evaluates LLMs across six key RAC dimensions: Fluent
Tracking, State Tracking, Action Executability, Effects of Actions, Numerical
RAC, and Composite Questions. LLMs demonstrate average accuracy rates of
73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are
frequently discussed in RAC literature. However, the performance on the latter
two dimensions, which introduce complex and novel reasoning questions, the
average performance of LLMs is lowered to 33.16% and 51.19%, respectively,
reflecting a 17.9% performance decline. We also introduce new ramification
constraints to capture the indirect effects of actions, providing deeper
insights into RAC challenges. Our evaluation of state-of-the-art LLMs,
including both open-source and commercial models, reveals challenges across all
RAC dimensions, particularly in handling ramifications, with GPT-4o failing to
solve any question and o1-preview achieving a score of only 18.4%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Rank-Ramsey Problem and the Log-Rank Conjecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gal Beniamini, Nati Linial, Adi Shraibman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A graph is called Rank-Ramsey if (i) Its clique number is small, and (ii) The
adjacency matrix of its complement has small rank. We initiate a systematic
study of such graphs. Our main motivation is that their constructions, as well
as proofs of their non-existence, are intimately related to the famous log-rank
conjecture from the field of communication complexity. These investigations
also open interesting new avenues in Ramsey theory.
  We construct two families of Rank-Ramsey graphs exhibiting polynomial
separation between order and complement rank. Graphs in the first family have
bounded clique number (as low as $41$). These are subgraphs of certain strong
products, whose building blocks are derived from triangle-free strongly-regular
graphs. Graphs in the second family are obtained by applying Boolean functions
to Erd\H{o}s-R\'enyi graphs. Their clique number is logarithmic, but their
complement rank is far smaller than in the first family, about
$\mathcal{O}(n^{2/3})$. A key component of this construction is our
matrix-theoretic view of lifts.
  We also consider lower bounds on the Rank-Ramsey numbers, and determine them
in the range where the complement rank is $5$ or less. We consider connections
between said numbers and other graph parameters, and find that the two best
known explicit constructions of triangle-free Ramsey graphs turn out to be far
from Rank-Ramsey.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Formal Languages and Automata Theory <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing measures of weak-MSO definable sets of trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damian Niwiński, Marcin Przybyłko, Michał Skrzypczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  his work addresses the problem of computing measures of recognisable sets of
infinite trees. An algorithm is provided to compute the probability measure of
a tree language recognisable by a weak alternating automaton, or equivalently
definable in weak monadic second-order logic. The measure is the uniform
coin-flipping measure or more generally it is generated by a~branching
stochastic process. The class of tree languages in consideration, although
smaller than all regular tree languages, comprises in particular the languages
definable in the alternation-free mu-calculus or in temporal logic CTL. Thus,
the new algorithm may enhance the toolbox of probabilistic model checking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An ArXiv version of a paper from ICALP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A complete formalization of Fermat's Last Theorem for regular primes in
  Lean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Brasca, Christopher Birkbeck, Eric Rodriguez Boidi, Alex Best, Ruben van De Velde, Andrew Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formalize a complete proof of the regular case of Fermat's Last Theorem in
the Lean4 theorem prover. Our formalization includes a proof of Kummer's lemma,
that is the main obstruction to Fermat's Last Theorem for regular primes.
Rather than following the modern proof of Kummer's lemma via class field
theory, we prove it by using Hilbert's Theorems 90-94 in a way that is more
amenable to formalization.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Logic in Computer Science <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Axiomatization of Compact Initial Value Problems: Open Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Platzer, Long Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proves the completeness of an axiomatization for initial value
problems (IVPs) with compact initial conditions and compact time horizons for
bounded open safety, open liveness and existence properties. Completeness
systematically reduces the proofs of these properties to a complete
axiomatization for differential equation invariants. This result unifies
symbolic logic and numerical analysis by a computable procedure that generates
symbolic proofs with differential invariants for rigorous error bounds of
numerical solutions to polynomial initial value problems. The procedure is
modular and works for all polynomial IVPs with rational coefficients and
initial conditions and symbolic parameters constrained to compact sets.
Furthermore, this paper discusses generalizations to IVPs with initial
conditions/symbolic parameters that are not necessarily constrained to compact
sets, achieved through the derivation of fully symbolic axioms/proof-rules
based on the axiomatization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formalizing Hyperspaces and Operations on Subsets of Polish spaces over
  Abstract Exact Real Numbers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Konečný, Sewon Park, Holger Thies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on our prior work on axiomatization of exact real computation by
formalizing nondeterministic first-order partial computations over real and
complex numbers in a constructive dependent type theory, we present a framework
for certified computation on hyperspaces of subsets by formalizing various
higher-order data types and operations. We first define open, closed, compact
and overt subsets for generic spaces in an abstract topological way that allows
short and elegant proofs with computational content coinciding with standard
definitions in computable analysis and constructive mathematics. From these
proofs we can extract programs for testing inclusion, overlapping of sets, et
cetera. To enhance the efficiency of the extracted programs, we then focus on
Polish spaces, where we give more efficient encodings based on metric
properties of the space. As various computational properties depend on the
continuity of the encoding functions, we introduce a nondeterministic version
of a continuity principle which is natural in our formalization and valid under
the standard type-2 realizability interpretation. Using this principle we
further derive the computational equivalence between the generic and the metric
encodings. Our theory is fully implemented in the Coq proof assistant. From
proofs in this Coq formalization, we can extract certified programs for
error-free operations on subsets. As an application, we provide a function that
constructs fractals in Euclidean space, such as the Sierpinski triangle, from
iterated function systems using the limit operation. The resulting programs can
be used to draw such fractals up to any desired resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Quantum Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoît Valiron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis (Habilitation \`a diriger des recherches) presents some of my
research contributions since my Ph.D defense in 2008. I have had the chance to
participate in the development of quantum programming languages since their
early developments: the presentation aims to present my point of view on the
evolution of the subject, my contributions, and the current research trends in
the community. The target audience is a graduate student interested in pointers
to the field of quantum programming languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>127 pages. French "Habilitation \`a diriger des recherche" (HDR),
  presented on September 24, 2024 at Universit\'e Paris Saclay. On Hal
  repository: tel-04740855</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Golyadkin's Torment: Doppelgängers and Adversarial Vulnerability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George I. Kamberov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many machine learning (ML) classifiers are claimed to outperform humans, but
they still make mistakes that humans do not. The most notorious examples of
such mistakes are adversarial visual metamers. This paper aims to define and
investigate the phenomenon of adversarial Doppelgangers (AD), which includes
adversarial visual metamers, and to compare the performance and robustness of
ML classifiers to human performance.
  We find that AD are inputs that are close to each other with respect to a
perceptual metric defined in this paper. AD are qualitatively different from
the usual adversarial examples. The vast majority of classifiers are vulnerable
to AD and robustness-accuracy trade-offs may not improve them. Some
classification problems may not admit any AD robust classifiers because the
underlying classes are ambiguous. We provide criteria that can be used to
determine whether a classification problem is well defined or not; describe the
structure and attributes of an AD-robust classifier; introduce and explore the
notions of conceptual entropy and regions of conceptual ambiguity for
classifiers that are vulnerable to AD attacks, along with methods to bound the
AD fooling rate of an attack. We define the notion of classifiers that exhibit
hypersensitive behavior, that is, classifiers whose only mistakes are
adversarial Doppelgangers. Improving the AD robustness of hyper-sensitive
classifiers is equivalent to improving accuracy. We identify conditions
guaranteeing that all classifiers with sufficiently high accuracy are
hyper-sensitive.
  Our findings are aimed at significant improvements in the reliability and
security of machine learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A complete formalization of Fermat's Last Theorem for regular primes in
  Lean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Brasca, Christopher Birkbeck, Eric Rodriguez Boidi, Alex Best, Ruben van De Velde, Andrew Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formalize a complete proof of the regular case of Fermat's Last Theorem in
the Lean4 theorem prover. Our formalization includes a proof of Kummer's lemma,
that is the main obstruction to Fermat's Last Theorem for regular primes.
Rather than following the modern proof of Kummer's lemma via class field
theory, we prove it by using Hilbert's Theorems 90-94 in a way that is more
amenable to formalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization-baed similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10096v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10096v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Antić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and exploiting similarities between seemingly distant objects is
without doubt an important human ability. This paper develops \textit{from the
ground up} an abstract algebraic and qualitative notion of similarity based on
the observation that sets of generalizations encode important properties of
elements. We show that similarity defined in this way has appealing
mathematical properties. As we construct our notion of similarity from first
principles using only elementary concepts of universal algebra, to convince the
reader of its plausibility, we show that it can model fundamental relations
occurring in mathematics and be naturally embedded into first-order logic via
model-theoretic types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Formalizing equivalences without tears 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11501v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11501v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom de Jong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This expository note describes two convenient techniques in the context of
homotopy type theory for proving and formalizing that a given map is an
equivalence. The first technique decomposes the map as a series of basic
equivalences, while the second refines this approach using the 3-for-2 property
of equivalences. The techniques are illustrated by proving a basic result in
synthetic homotopy theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: Minor and funding changes only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying lower bounds for algebraic machines, semantically 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1811.06787v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1811.06787v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Seiller, Luc Pellissier, Ulysse Léchine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new abstract method for proving lower bounds in
computational complexity. Based on the notion of topological and measurable
entropy for dynamical systems, it is shown to generalise three previous lower
bounds results from the literature in algebraic complexity. We use it to prove
that maxflow, a Ptime complete problem, is not computable in polylogarithmic
time on parallel random access machines (prams) working with real numbers. This
improves, albeit slightly, on a result of Mulmuley since the class of machines
considered extends the class "prams without bit operations", making more
precise the relationship between Mulmuley's result and similar lower bounds on
real prams.
  More importantly, we show our method captures previous lower bounds results
from the literature, thus providing a unifying framework for "topological"
proofs of lower bounds: Steele and Yao's lower bounds for algebraic decision
trees, Ben-Or's lower bounds for algebraic computation trees, Cucker's proof
that NC is not equal to Ptime in the real case, and Mulmuley's lower bounds for
"prams without bit operations".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in Information and Computation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Programming and Languages <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Axiomatization of Compact Initial Value Problems: Open Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Platzer, Long Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article proves the completeness of an axiomatization for initial value
problems (IVPs) with compact initial conditions and compact time horizons for
bounded open safety, open liveness and existence properties. Completeness
systematically reduces the proofs of these properties to a complete
axiomatization for differential equation invariants. This result unifies
symbolic logic and numerical analysis by a computable procedure that generates
symbolic proofs with differential invariants for rigorous error bounds of
numerical solutions to polynomial initial value problems. The procedure is
modular and works for all polynomial IVPs with rational coefficients and
initial conditions and symbolic parameters constrained to compact sets.
Furthermore, this paper discusses generalizations to IVPs with initial
conditions/symbolic parameters that are not necessarily constrained to compact
sets, achieved through the derivation of fully symbolic axioms/proof-rules
based on the axiomatization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Large Language Models and Reinforcement Learning for
  Non-Linear Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoav Alon, Cristina David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) were shown to struggle with long-term planning,
which may be caused by the limited way in which they explore the space of
possible solutions. We propose an architecture where a Reinforcement Learning
(RL) Agent guides an LLM's space exploration: (1) the Agent has access to
domain-specific information, and can therefore make decisions about the quality
of candidate solutions based on specific and relevant metrics, which were not
explicitly considered by the LLM's training objective; (2) the LLM can focus on
generating immediate next steps, without the need for long-term planning. We
allow non-linear reasoning by exploring alternative paths and backtracking. We
evaluate this architecture on the program equivalence task, and compare it
against Chain of Thought (CoT) and Tree of Thoughts (ToT). We assess both the
downstream task, denoting the binary classification, and the intermediate
reasoning steps. Our approach compares positively against CoT and ToT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EOSpython Version 0.0.11: A Framework for Scenario Generation and a
  Solution System for the Agile Earth Observation Satellite Scheduling Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Elkjær Vasegaard, Andreas Kühne Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EOSpython is a PyPI published Python package that encompass everything within
a centralized earth observation satellite scheduling system in terms of
customer database setup, scenario generation, pre-processing, problem setup,
scheduling solution approach, decision maker preference integration, and
visualization. The package is tailored to easily configure internal parameters
and contribute with other solution approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Quantum Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoît Valiron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis (Habilitation \`a diriger des recherches) presents some of my
research contributions since my Ph.D defense in 2008. I have had the chance to
participate in the development of quantum programming languages since their
early developments: the presentation aims to present my point of view on the
evolution of the subject, my contributions, and the current research trends in
the community. The target audience is a graduate student interested in pointers
to the field of quantum programming languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>127 pages. French "Habilitation \`a diriger des recherche" (HDR),
  presented on September 24, 2024 at Universit\'e Paris Saclay. On Hal
  repository: tel-04740855</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Testing for Semantic Regular Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Huang, Matin Amini, Alexis Le Glaunec, Konstantinos Mamouras, Mukund Raghothaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SMORE (Chen et al., 2023) recently proposed the concept of semantic regular
expressions that extend the classical formalism with a primitive to query
external oracles such as databases and large language models (LLMs). Such
patterns can be used to identify lines of text containing references to
semantic concepts such as cities, celebrities, political entities, etc. The
focus in their paper was on automatically synthesizing semantic regular
expressions from positive and negative examples. In this paper, we study the
membership testing problem:
  First, We present a two-pass NFA-based algorithm to determine whether a
string $w$ matches a semantic regular expression (SemRE) $r$ in $O(|r|^2 |w|^2
+ |r| |w|^3)$ time, assuming the oracle responds to each query in unit time. In
common situations, where oracle queries are not nested, we show that this
procedure runs in $O(|r|^2 |w|^2)$ time. Experiments with a prototype
implementation of this algorithm validate our theoretical analysis, and show
that the procedure massively outperforms a dynamic programming-based baseline,
and incurs a $\approx 2 \times$ overhead over the time needed for interaction
with the oracle.
  Next, We establish connections between SemRE membership testing and the
triangle finding problem from graph theory, which suggest that developing
algorithms which are simultaneously practical and asymptotically faster might
be challenging. Furthermore, algorithms for classical regular expressions
primarily aim to optimize their time and memory consumption. In contrast, an
important consideration in our setting is to minimize the cost of invoking the
oracle. We demonstrate an $\Omega(|w|^2)$ lower bound on the number of oracle
queries necessary to make this determination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target-Aware Implementation of Real Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brett Saiki, Jackson Brough, Jonas Regehr, Jesús Ponce, Varun Pradeep, Aditya Akhileshwaran, Zachary Tatlock, Pavel Panchekha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New low-precision accelerators, vector instruction sets, and library
functions make maximizing accuracy and performance of numerical code
increasingly challenging. Two lines of work$\unicode{x2013}$traditional
compilers and numerical compilers$\unicode{x2013}$attack this problem from
opposite directions. Traditional compiler backends optimize for specific target
environments but are limited in their ability to balance performance and
accuracy. Numerical compilers trade off accuracy and performance, or even
improve both, but ignore the target environment. We join aspects of both to
produce Chassis, a target-aware numerical compiler.
  Chassis compiles mathematical expressions to operators from a target
description, which lists the real expressions each operator approximates and
estimates its cost and accuracy. Chassis then uses an iterative improvement
loop to optimize for speed and accuracy. Specifically, a new instruction
selection modulo equivalence algorithm efficiently searches for faster
target-specific programs, while a new cost-opportunity heuristic supports
iterative improvement. We demonstrate Chassis' capabilities on 9 different
targets, including hardware ISAs, math libraries, and programming languages.
Chassis achieves significantly better accuracy and performance trade-offs than
both Clang (by 8.9x) or Herbie (by up to 3.5x) by leveraging low-precision
accelerators, accuracy-optimized numerical helper functions, and library
subcomponents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scenario-Based Proofs for Concurrent Objects [Extended Version] 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Enea, Eric Koskinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concurrent objects form the foundation of many applications that exploit
multicore architectures and their importance has lead to informal correctness
arguments, as well as formal proof systems. Correctness arguments (as found in
the distributed computing literature) give intuitive descriptions of a few
canonical executions or "scenarios" often each with only a few threads, yet it
remains unknown as to whether these intuitive arguments have a formal grounding
and extend to arbitrary interleavings over unboundedly many threads.
  We present a novel proof technique for concurrent objects, based around
identifying a small set of scenarios (representative, canonical interleavings),
formalized as the commutativity quotient of a concurrent object. We next give
an expression language for defining abstractions of the quotient in the form of
regular or context-free languages that enable simple proofs of linearizability.
These quotient expressions organize unbounded interleavings into a form more
amenable to reasoning and make explicit the relationship between
implementation-level contention/interference and ADT-level transitions.
  We evaluate our work on numerous non-trivial concurrent objects from the
literature (including the Michael-Scott queue, Elimination stack, SLS
reservation queue, RDCSS and Herlihy-Wing queue). We show that quotients
capture the diverse features/complexities of these algorithms, can be used even
when linearization points are not straight-forward, correspond to original
authors' correctness arguments, and provide some new scenario-based arguments.
Finally, we show that discovery of some object's quotients reduces to
two-thread reasoning and give an implementation that can derive candidate
quotients expressions from source code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Logic Bugs in Spatial Database Engines via Affine Equivalent
  Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Deng, Qiuyang Mang, Chengyu Zhang, Manuel Rigger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial Database Management Systems (SDBMSs) aim to store, manipulate, and
retrieve spatial data. SDBMSs are employed in various modern applications, such
as geographic information systems, computer-aided design tools, and
location-based services. However, the presence of logic bugs in SDBMSs can lead
to incorrect results, substantially undermining the reliability of these
applications. Detecting logic bugs in SDBMSs is challenging due to the lack of
ground truth for identifying incorrect results. In this paper, we propose an
automated geometry-aware generator to generate high-quality SQL statements for
SDBMSs and a novel concept named Affine Equivalent Inputs (AEI) to validate the
results of SDBMSs. We implemented them as a tool named Spatter (Spatial DBMSs
Tester) for finding logic bugs in four popular SDBMSs: PostGIS, DuckDB Spatial,
MySQL, and SQL Server. Our testing campaign detected 34 previously unknown and
unique bugs in these SDBMS, of which 30 have been confirmed, and 18 have been
already fixed. Our testing efforts have been well appreciated by the
developers. Experimental results demonstrate that the geometry-aware generator
significantly outperforms a naive random-shape generator in detecting unique
bugs, and AEI can identify 14 logic bugs in SDBMSs that were overlooked by
previous methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-aided explanations of EDA synthesis errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Qiu, Benjamin Tan, Hammond Pearce
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training new engineers in digital design is a challenge, particularly when it
comes to teaching the complex electronic design automation (EDA) tooling used
in this domain. Learners will typically deploy designs in the Verilog and VHDL
hardware description languages to Field Programmable Gate Arrays (FPGAs) from
Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains
(Quartus Prime and Vivado, respectively). These tools are complex and difficult
to use -- yet, as they are the tools used in industry, they are an essential
first step in this space. In this work, we examine how recent advances in
artificial intelligence may be leveraged to address aspects of this challenge.
Specifically, we investigate if Large Language Models (LLMs), which have
demonstrated text comprehension and question-answering capabilities, can be
used to generate novice-friendly explanations of compile-time synthesis error
messages from Quartus Prime and Vivado. To perform this study we generate 936
error message explanations using three OpenAI LLMs over 21 different buggy code
samples. These are then graded for relevance and correctness, and we find that
in approximately 71% of cases the LLMs give correct & complete explanations
suitable for novice learners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. Accepted in IEEE LLM Aided Design Workshop
  (LAD'2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Performance Profiling <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Spanning Centrality with Random Bouquets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gökhan Göktürk, Kamer Kaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spanning Centrality is a measure used in network analysis to determine the
importance of an edge in a graph based on its contribution to the connectivity
of the entire network. Specifically, it quantifies how critical an edge is in
terms of the number of spanning trees that include that edge. The current
state-of-the-art for All Edges Spanning Centrality~(AESC), which computes the
exact centrality values for all the edges, has a time complexity of
$\mathcal{O}(mn^{3/2})$ for $n$ vertices and $m$ edges. This makes the
computation infeasible even for moderately sized graphs. Instead, there exist
approximation algorithms which process a large number of random walks to
estimate edge centralities. However, even the approximation algorithms can be
computationally overwhelming, especially if the approximation error bound is
small. In this work, we propose a novel, hash-based sampling method and a
vectorized algorithm which greatly improves the execution time by clustering
random walks into {\it Bouquets}. On synthetic random walk benchmarks, {\it
Bouquets} performs $7.8\times$ faster compared to naive, traditional
random-walk generation. We also show that the proposed technique is scalable by
employing it within a state-of-the-art AESC approximation algorithm, {\sc
TGT+}. The experiments show that using Bouquets yields more than $100\times$
speed-up via parallelization with 16 threads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiFuseR: A Distributed Sketch-based Influence Maximization Algorithm for
  GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gökhan Göktürk, Kamer Kaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influence Maximization (IM) aims to find a given number of "seed" vertices
that can effectively maximize the expected spread under a given diffusion
model. Due to the NP-Hardness of finding an optimal seed set, approximation
algorithms are often used for IM. However, these algorithms require a large
number of simulations to find good seed sets. In this work, we propose DiFuseR,
a blazing-fast, high-quality IM algorithm that can run on multiple GPUs in a
distributed setting. DiFuseR is designed to increase GPU utilization, reduce
inter-node communication, and minimize overlapping data/computation among the
nodes. Based on the experiments with various graphs, containing some of the
largest networks available, and diffusion settings, the proposed approach is
found to be 3.2x and 12x faster on average on a single GPU and 8 GPUs,
respectively. It can achieve up to 8x and 233.7x speedup on the same hardware
settings. Furthermore, thanks to its smart load-balancing mechanism, on 8 GPUs,
it is on average 5.6x faster compared to its single-GPU performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGim : An Efficient Graph Neural Network Library for Real
  Processing-In-Memory Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML library that accelerates GNNs on
real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively. We
extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using
emerging GNN models, and demonstrate that it outperforms its state-of-the-art
CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource
utilization than CPU and GPU systems. Our work provides useful recommendations
for software, system and hardware designers. PyGim is publicly available at
https://github.com/CMU-SAFARI/PyGim.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-10-25T00:40:22.172455022Z">
            2024-10-25 00:40:22 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
